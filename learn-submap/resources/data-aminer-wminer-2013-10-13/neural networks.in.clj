#topic_maps.core.TopicMap{:topic-graph #graphs.core.Digraph{:nodes #{{:id 763505, :title "Signal processing", :type :wiki-api.core/category} {:id 693702, :title "Functional programming", :type :wiki-api.core/category} {:id 690672, :title "Abstract algebra", :type :wiki-api.core/category} {:id 574759, :title "Feed forward (control)", :type :wiki-api.core/article} {:id 690803, :title "Calculus", :type :wiki-api.core/category} {:id 1034006, :title "Particle physics", :type :wiki-api.core/category} {:id 18039672, :title "Uncertainty of numbers", :type :wiki-api.core/category} {:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} {:id 34760990, :title "Nonlinear systems", :type :wiki-api.core/category} {:id 24059390, :title "Markov models", :type :wiki-api.core/category} {:id 24403323, :title "Formal systems", :type :wiki-api.core/category} {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} {:id 14787852, :title "Media technology", :type :wiki-api.core/category} {:id 957793, :title "Cognitive science", :type :wiki-api.core/category} {:id 744360, :title "Networks", :type :wiki-api.core/category} {:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 17503782, :title "Formal sciences", :type :wiki-api.core/category} {:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category} {:id 1195726, :title "Error", :type :wiki-api.core/category} {:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} {:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} {:id 8135339, :title "Articles with example Haskell code", :type :wiki-api.core/category} {:id 7544087, :title "Norms (mathematics)", :type :wiki-api.core/category} {:id 694421, :title "Euclidean geometry", :type :wiki-api.core/category} {:id 126706, :title "Pattern recognition", :type :wiki-api.core/article} {:id 2157920, :title "Technology in society", :type :wiki-api.core/category} {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} {:id 691898, :title "Graph theory", :type :wiki-api.core/category} {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category} {:id 971740, :title "Topological spaces", :type :wiki-api.core/category} {:id 5065063, :title "Network architecture", :type :wiki-api.core/category} {:id 26304777, :title "Graph theory objects", :type :wiki-api.core/category} {:id 31195693, :title "Design of experiments", :type :wiki-api.core/category} {:id 26743152, :title "Conceptual models", :type :wiki-api.core/category} {:id 36312376, :title "Belief revision", :type :wiki-api.core/category} {:id 772545, :title "Probability distributions", :type :wiki-api.core/category} {:id 946892, :title "Econometrics", :type :wiki-api.core/category} {:id 1754736, :title "Cybernetics", :type :wiki-api.core/category} {:id 17652827, :title "Statistical dependence", :type :wiki-api.core/category} {:id 325806, :title "Graph (mathematics)", :type :wiki-api.core/article} {:id 4594748, :title "Computational problems", :type :wiki-api.core/category} {:id 717695, :title "Systems theory", :type :wiki-api.core/category} {:id 20926, :title "Supervised learning", :type :wiki-api.core/article} {:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category} {:id 5038508, :title "Information, knowledge, and uncertainty", :type :wiki-api.core/category} {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article} {:id 22406300, :title "Handwriting recognition", :type :wiki-api.core/category} {:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category} {:id 2266644, :title "Multilayer perceptron", :type :wiki-api.core/article} {:id 6539521, :title "Statistical theory", :type :wiki-api.core/category} {:id 1028771, :title "Control theory", :type :wiki-api.core/category} {:id 9697, :title "Euclidean space", :type :wiki-api.core/article} {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article} {:id 27248985, :title "Mathematical concepts", :type :wiki-api.core/category} {:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category} {:id 1008581, :title "Operations research", :type :wiki-api.core/category} {:id 734262, :title "Measurement", :type :wiki-api.core/category} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} {:id 957250, :title "Fuzzy logic", :type :wiki-api.core/category} {:id 87210, :title "Sigmoid function", :type :wiki-api.core/article} {:id 17735029, :title "Probability interpretations", :type :wiki-api.core/category} {:id 962413, :title "Image processing", :type :wiki-api.core/category} {:id 15214002, :title "Metalogic", :type :wiki-api.core/category} {:id 22166736, :title "Non-classical logic", :type :wiki-api.core/category} {:id 1532663, :title "Artificial intelligence applications", :type :wiki-api.core/category} {:id 18543448, :title "Universal approximation theorem", :type :wiki-api.core/article} {:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category} {:id 26308484, :title "Summary statistics for contingency tables", :type :wiki-api.core/category} {:id 4002768, :title "Evaluation", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 33748657, :title "Geometric measurement", :type :wiki-api.core/category} {:id 3175294, :title "Dimension", :type :wiki-api.core/category} {:id 804551, :title "Psychometrics", :type :wiki-api.core/category} {:id 10939195, :title "Systems", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category} {:id 1307911, :title "Bootstrap aggregating", :type :wiki-api.core/article} {:id 916810, :title "Spam filtering", :type :wiki-api.core/category} {:id 33420532, :title "Theorems in discrete mathematics", :type :wiki-api.core/category} {:id 242190, :title "Feature extraction", :type :wiki-api.core/article} {:id 797088, :title "Computer vision", :type :wiki-api.core/category} {:id 36477012, :title "Concepts in physics", :type :wiki-api.core/category} {:id 90500, :title "Boosting (machine learning)", :type :wiki-api.core/article} {:id 298420, :title "Maxima and minima", :type :wiki-api.core/article} {:id 17684887, :title "Statistical approximations", :type :wiki-api.core/category} {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article} {:id 693985, :title "Probability theory", :type :wiki-api.core/category} {:id 10678854, :title "Collective intelligence", :type :wiki-api.core/category} {:id 8525467, :title "Pointing-device text input", :type :wiki-api.core/category} {:id 4289067, :title "Computational neuroscience", :type :wiki-api.core/category} {:id 1706332, :title "Feedforward neural network", :type :wiki-api.core/article} {:id 699134, :title "Formal languages", :type :wiki-api.core/category} {:id 2574961, :title "Formal methods", :type :wiki-api.core/category} {:id 966983, :title "Applied mathematics", :type :wiki-api.core/category} {:id 877149, :title "Dynamical systems", :type :wiki-api.core/category} {:id 5175143, :title "Geostatistics", :type :wiki-api.core/category} {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category} {:id 693685, :title "Mathematical logic", :type :wiki-api.core/category} {:id 21753440, :title "Critical thinking", :type :wiki-api.core/category} {:id 34064788, :title "Neuroethology concepts", :type :wiki-api.core/category} {:id 714648, :title "Neuroscience", :type :wiki-api.core/category} {:id 1706303, :title "Recurrent neural network", :type :wiki-api.core/article} {:id 7297665, :title "Qualities of thought", :type :wiki-api.core/category} {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article} {:id 10974311, :title "Conditional constructs", :type :wiki-api.core/category} {:id 22691076, :title "Graphical models", :type :wiki-api.core/category} {:id 700292, :title "Scientific method", :type :wiki-api.core/category} {:id 690777, :title "Linear algebra", :type :wiki-api.core/category} {:id 6537403, :title "Biostatistics", :type :wiki-api.core/category} {:id 17193471, :title "Statistical models", :type :wiki-api.core/category} {:id 336897, :title "Function approximation", :type :wiki-api.core/article} {:id 4064652, :title "Elementary special functions", :type :wiki-api.core/category} {:id 19314112, :title "Learning in computer vision", :type :wiki-api.core/category} {:id 1010631, :title "Logic in computer science", :type :wiki-api.core/category} {:id 34846657, :title "Syntax (logic)", :type :wiki-api.core/category} {:id 691875, :title "Mathematical analysis", :type :wiki-api.core/category} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} {:id 1817228, :title "Training set", :type :wiki-api.core/article} {:id 698571, :title "Nervous system", :type :wiki-api.core/category} {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category} {:id 19655050, :title "Directed graphs", :type :wiki-api.core/category} {:id 1098276, :title "Mathematical terminology", :type :wiki-api.core/category} {:id 1055691, :title "Learning", :type :wiki-api.core/category} {:id 505717, :title "Image segmentation", :type :wiki-api.core/article} {:id 9272793, :title "Computational statistics", :type :wiki-api.core/category} {:id 204002, :title "Directed acyclic graph", :type :wiki-api.core/article} {:id 12810241, :title "Telecommunication theory", :type :wiki-api.core/category} {:id 11822062, :title "Neurobiology", :type :wiki-api.core/category} {:id 349771, :title "Artificial neuron", :type :wiki-api.core/article} {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category} {:id 32611713, :title "Emerging technologies", :type :wiki-api.core/category} {:id 25174161, :title "Open problems", :type :wiki-api.core/category} {:id 4409868, :title "American inventions", :type :wiki-api.core/category}}, :in-map {{:id 763505, :title "Signal processing", :type :wiki-api.core/category} #{{:id 14787852, :title "Media technology", :type :wiki-api.core/category} {:id 966983, :title "Applied mathematics", :type :wiki-api.core/category} {:id 12810241, :title "Telecommunication theory", :type :wiki-api.core/category}}, {:id 693702, :title "Functional programming", :type :wiki-api.core/category} #{}, {:id 690672, :title "Abstract algebra", :type :wiki-api.core/category} #{}, {:id 574759, :title "Feed forward (control)", :type :wiki-api.core/article} #{{:id 1028771, :title "Control theory", :type :wiki-api.core/category} {:id 34064788, :title "Neuroethology concepts", :type :wiki-api.core/category}}, {:id 690803, :title "Calculus", :type :wiki-api.core/category} #{{:id 691875, :title "Mathematical analysis", :type :wiki-api.core/category}}, {:id 1034006, :title "Particle physics", :type :wiki-api.core/category} #{}, {:id 18039672, :title "Uncertainty of numbers", :type :wiki-api.core/category} #{{:id 5038508, :title "Information, knowledge, and uncertainty", :type :wiki-api.core/category}}, {:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} #{{:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category} {:id 1195726, :title "Error", :type :wiki-api.core/category} {:id 31195693, :title "Design of experiments", :type :wiki-api.core/category} {:id 916810, :title "Spam filtering", :type :wiki-api.core/category}}, {:id 34760990, :title "Nonlinear systems", :type :wiki-api.core/category} #{{:id 36477012, :title "Concepts in physics", :type :wiki-api.core/category} {:id 877149, :title "Dynamical systems", :type :wiki-api.core/category}}, {:id 24059390, :title "Markov models", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 22691076, :title "Graphical models", :type :wiki-api.core/category}}, {:id 24403323, :title "Formal systems", :type :wiki-api.core/category} #{{:id 699134, :title "Formal languages", :type :wiki-api.core/category} {:id 2574961, :title "Formal methods", :type :wiki-api.core/category} {:id 693685, :title "Mathematical logic", :type :wiki-api.core/category}}, {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} #{{:id 744360, :title "Networks", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 22691076, :title "Graphical models", :type :wiki-api.core/category}}, {:id 14787852, :title "Media technology", :type :wiki-api.core/category} #{}, {:id 957793, :title "Cognitive science", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}}, {:id 744360, :title "Networks", :type :wiki-api.core/category} #{}, {:id 5206601, :title "Data mining", :type :wiki-api.core/category} #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category}}, {:id 17503782, :title "Formal sciences", :type :wiki-api.core/category} #{}, {:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category} #{{:id 946892, :title "Econometrics", :type :wiki-api.core/category} {:id 6539521, :title "Statistical theory", :type :wiki-api.core/category}}, {:id 1195726, :title "Error", :type :wiki-api.core/category} #{}, {:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} #{{:id 1010631, :title "Logic in computer science", :type :wiki-api.core/category}}, {:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} #{{:id 690672, :title "Abstract algebra", :type :wiki-api.core/category} {:id 27248985, :title "Mathematical concepts", :type :wiki-api.core/category} {:id 3175294, :title "Dimension", :type :wiki-api.core/category}}, {:id 8135339, :title "Articles with example Haskell code", :type :wiki-api.core/category} #{}, {:id 7544087, :title "Norms (mathematics)", :type :wiki-api.core/category} #{{:id 691875, :title "Mathematical analysis", :type :wiki-api.core/category}}, {:id 694421, :title "Euclidean geometry", :type :wiki-api.core/category} #{}, {:id 126706, :title "Pattern recognition", :type :wiki-api.core/article} #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 2157920, :title "Technology in society", :type :wiki-api.core/category} #{}, {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category} {:id 5175143, :title "Geostatistics", :type :wiki-api.core/category}}, {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} #{{:id 24059390, :title "Markov models", :type :wiki-api.core/category} {:id 36312376, :title "Belief revision", :type :wiki-api.core/category} {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category}}, {:id 691898, :title "Graph theory", :type :wiki-api.core/category} #{}, {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category} #{{:id 693702, :title "Functional programming", :type :wiki-api.core/category} {:id 8135339, :title "Articles with example Haskell code", :type :wiki-api.core/category} {:id 699134, :title "Formal languages", :type :wiki-api.core/category} {:id 10974311, :title "Conditional constructs", :type :wiki-api.core/category}}, {:id 971740, :title "Topological spaces", :type :wiki-api.core/category} #{}, {:id 5065063, :title "Network architecture", :type :wiki-api.core/category} #{}, {:id 26304777, :title "Graph theory objects", :type :wiki-api.core/category} #{{:id 691898, :title "Graph theory", :type :wiki-api.core/category}}, {:id 31195693, :title "Design of experiments", :type :wiki-api.core/category} #{{:id 700292, :title "Scientific method", :type :wiki-api.core/category}}, {:id 26743152, :title "Conceptual models", :type :wiki-api.core/category} #{}, {:id 36312376, :title "Belief revision", :type :wiki-api.core/category} #{{:id 796635, :title "Knowledge representation", :type :wiki-api.core/category}}, {:id 772545, :title "Probability distributions", :type :wiki-api.core/category} #{{:id 693985, :title "Probability theory", :type :wiki-api.core/category} {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}, {:id 946892, :title "Econometrics", :type :wiki-api.core/category} #{}, {:id 1754736, :title "Cybernetics", :type :wiki-api.core/category} #{{:id 957793, :title "Cognitive science", :type :wiki-api.core/category} {:id 966983, :title "Applied mathematics", :type :wiki-api.core/category}}, {:id 17652827, :title "Statistical dependence", :type :wiki-api.core/category} #{}, {:id 325806, :title "Graph (mathematics)", :type :wiki-api.core/article} #{{:id 26304777, :title "Graph theory objects", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 4594748, :title "Computational problems", :type :wiki-api.core/category} #{{:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category}}, {:id 717695, :title "Systems theory", :type :wiki-api.core/category} #{{:id 10939195, :title "Systems", :type :wiki-api.core/category}}, {:id 20926, :title "Supervised learning", :type :wiki-api.core/article} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category} #{{:id 17503782, :title "Formal sciences", :type :wiki-api.core/category} {:id 2157920, :title "Technology in society", :type :wiki-api.core/category} {:id 4289067, :title "Computational neuroscience", :type :wiki-api.core/category} {:id 32611713, :title "Emerging technologies", :type :wiki-api.core/category} {:id 25174161, :title "Open problems", :type :wiki-api.core/category}}, {:id 5038508, :title "Information, knowledge, and uncertainty", :type :wiki-api.core/category} #{}, {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} #{{:id 1008581, :title "Operations research", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 691875, :title "Mathematical analysis", :type :wiki-api.core/category}}, {:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article} #{{:id 18039672, :title "Uncertainty of numbers", :type :wiki-api.core/category} {:id 6539521, :title "Statistical theory", :type :wiki-api.core/category} {:id 26308484, :title "Summary statistics for contingency tables", :type :wiki-api.core/category} {:id 804551, :title "Psychometrics", :type :wiki-api.core/category} {:id 21753440, :title "Critical thinking", :type :wiki-api.core/category} {:id 7297665, :title "Qualities of thought", :type :wiki-api.core/category} {:id 6537403, :title "Biostatistics", :type :wiki-api.core/category}}, {:id 22406300, :title "Handwriting recognition", :type :wiki-api.core/category} #{{:id 1532663, :title "Artificial intelligence applications", :type :wiki-api.core/category} {:id 8525467, :title "Pointing-device text input", :type :wiki-api.core/category}}, {:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category} #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category} {:id 3175294, :title "Dimension", :type :wiki-api.core/category}}, {:id 2266644, :title "Multilayer perceptron", :type :wiki-api.core/article} #{{:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 6539521, :title "Statistical theory", :type :wiki-api.core/category} #{}, {:id 1028771, :title "Control theory", :type :wiki-api.core/category} #{{:id 717695, :title "Systems theory", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 9697, :title "Euclidean space", :type :wiki-api.core/article} #{{:id 7544087, :title "Norms (mathematics)", :type :wiki-api.core/category} {:id 694421, :title "Euclidean geometry", :type :wiki-api.core/category} {:id 971740, :title "Topological spaces", :type :wiki-api.core/category} {:id 690777, :title "Linear algebra", :type :wiki-api.core/category}}, {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article} #{{:id 4594748, :title "Computational problems", :type :wiki-api.core/category}}, {:id 27248985, :title "Mathematical concepts", :type :wiki-api.core/category} #{}, {:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category} #{}, {:id 1008581, :title "Operations research", :type :wiki-api.core/category} #{{:id 966983, :title "Applied mathematics", :type :wiki-api.core/category}}, {:id 734262, :title "Measurement", :type :wiki-api.core/category} #{}, {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} #{{:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} #{{:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category} {:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 957250, :title "Fuzzy logic", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category} {:id 17735029, :title "Probability interpretations", :type :wiki-api.core/category} {:id 22166736, :title "Non-classical logic", :type :wiki-api.core/category} {:id 1010631, :title "Logic in computer science", :type :wiki-api.core/category}}, {:id 87210, :title "Sigmoid function", :type :wiki-api.core/article} #{{:id 772545, :title "Probability distributions", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 4064652, :title "Elementary special functions", :type :wiki-api.core/category}}, {:id 17735029, :title "Probability interpretations", :type :wiki-api.core/category} #{{:id 6539521, :title "Statistical theory", :type :wiki-api.core/category} {:id 693985, :title "Probability theory", :type :wiki-api.core/category}}, {:id 962413, :title "Image processing", :type :wiki-api.core/category} #{{:id 763505, :title "Signal processing", :type :wiki-api.core/category}}, {:id 15214002, :title "Metalogic", :type :wiki-api.core/category} #{}, {:id 22166736, :title "Non-classical logic", :type :wiki-api.core/category} #{{:id 693685, :title "Mathematical logic", :type :wiki-api.core/category}}, {:id 1532663, :title "Artificial intelligence applications", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}}, {:id 18543448, :title "Universal approximation theorem", :type :wiki-api.core/article} #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 33420532, :title "Theorems in discrete mathematics", :type :wiki-api.core/category}}, {:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 26308484, :title "Summary statistics for contingency tables", :type :wiki-api.core/category} #{}, {:id 4002768, :title "Evaluation", :type :wiki-api.core/category} #{}, {:id 706543, :title "Machine learning", :type :wiki-api.core/category} #{{:id 1754736, :title "Cybernetics", :type :wiki-api.core/category} {:id 19314112, :title "Learning in computer vision", :type :wiki-api.core/category} {:id 1055691, :title "Learning", :type :wiki-api.core/category} {:id 9272793, :title "Computational statistics", :type :wiki-api.core/category}}, {:id 33748657, :title "Geometric measurement", :type :wiki-api.core/category} #{{:id 734262, :title "Measurement", :type :wiki-api.core/category}}, {:id 3175294, :title "Dimension", :type :wiki-api.core/category} #{{:id 33748657, :title "Geometric measurement", :type :wiki-api.core/category} {:id 36477012, :title "Concepts in physics", :type :wiki-api.core/category} {:id 1098276, :title "Mathematical terminology", :type :wiki-api.core/category}}, {:id 804551, :title "Psychometrics", :type :wiki-api.core/category} #{{:id 734262, :title "Measurement", :type :wiki-api.core/category}}, {:id 10939195, :title "Systems", :type :wiki-api.core/category} #{}, {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} #{{:id 744360, :title "Networks", :type :wiki-api.core/category} {:id 5065063, :title "Network architecture", :type :wiki-api.core/category} {:id 946892, :title "Econometrics", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}, {:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category} #{{:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category}}, {:id 1307911, :title "Bootstrap aggregating", :type :wiki-api.core/article} #{{:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category} {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category}}, {:id 916810, :title "Spam filtering", :type :wiki-api.core/category} #{}, {:id 33420532, :title "Theorems in discrete mathematics", :type :wiki-api.core/category} #{}, {:id 242190, :title "Feature extraction", :type :wiki-api.core/article} #{{:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category}}, {:id 797088, :title "Computer vision", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category} {:id 962413, :title "Image processing", :type :wiki-api.core/category}}, {:id 36477012, :title "Concepts in physics", :type :wiki-api.core/category} #{}, {:id 90500, :title "Boosting (machine learning)", :type :wiki-api.core/article} #{{:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category}}, {:id 298420, :title "Maxima and minima", :type :wiki-api.core/article} #{{:id 690803, :title "Calculus", :type :wiki-api.core/category} {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category}}, {:id 17684887, :title "Statistical approximations", :type :wiki-api.core/category} #{{:id 6539521, :title "Statistical theory", :type :wiki-api.core/category}}, {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article} #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 693985, :title "Probability theory", :type :wiki-api.core/category} #{{:id 966983, :title "Applied mathematics", :type :wiki-api.core/category}}, {:id 10678854, :title "Collective intelligence", :type :wiki-api.core/category} #{}, {:id 8525467, :title "Pointing-device text input", :type :wiki-api.core/category} #{}, {:id 4289067, :title "Computational neuroscience", :type :wiki-api.core/category} #{{:id 714648, :title "Neuroscience", :type :wiki-api.core/category}}, {:id 1706332, :title "Feedforward neural network", :type :wiki-api.core/article} #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 699134, :title "Formal languages", :type :wiki-api.core/category} #{{:id 17503782, :title "Formal sciences", :type :wiki-api.core/category} {:id 15214002, :title "Metalogic", :type :wiki-api.core/category} {:id 34846657, :title "Syntax (logic)", :type :wiki-api.core/category}}, {:id 2574961, :title "Formal methods", :type :wiki-api.core/category} #{}, {:id 966983, :title "Applied mathematics", :type :wiki-api.core/category} #{}, {:id 877149, :title "Dynamical systems", :type :wiki-api.core/category} #{{:id 717695, :title "Systems theory", :type :wiki-api.core/category} {:id 691875, :title "Mathematical analysis", :type :wiki-api.core/category}}, {:id 5175143, :title "Geostatistics", :type :wiki-api.core/category} #{}, {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 693685, :title "Mathematical logic", :type :wiki-api.core/category} #{}, {:id 21753440, :title "Critical thinking", :type :wiki-api.core/category} #{{:id 4002768, :title "Evaluation", :type :wiki-api.core/category} {:id 1055691, :title "Learning", :type :wiki-api.core/category}}, {:id 34064788, :title "Neuroethology concepts", :type :wiki-api.core/category} #{}, {:id 714648, :title "Neuroscience", :type :wiki-api.core/category} #{}, {:id 1706303, :title "Recurrent neural network", :type :wiki-api.core/article} #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 7297665, :title "Qualities of thought", :type :wiki-api.core/category} #{}, {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article} #{{:id 17652827, :title "Statistical dependence", :type :wiki-api.core/category} {:id 693985, :title "Probability theory", :type :wiki-api.core/category}}, {:id 10974311, :title "Conditional constructs", :type :wiki-api.core/category} #{}, {:id 22691076, :title "Graphical models", :type :wiki-api.core/category} #{{:id 17193471, :title "Statistical models", :type :wiki-api.core/category}}, {:id 700292, :title "Scientific method", :type :wiki-api.core/category} #{}, {:id 690777, :title "Linear algebra", :type :wiki-api.core/category} #{}, {:id 6537403, :title "Biostatistics", :type :wiki-api.core/category} #{}, {:id 17193471, :title "Statistical models", :type :wiki-api.core/category} #{}, {:id 336897, :title "Function approximation", :type :wiki-api.core/article} #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 17684887, :title "Statistical approximations", :type :wiki-api.core/category} {:id 17193471, :title "Statistical models", :type :wiki-api.core/category}}, {:id 4064652, :title "Elementary special functions", :type :wiki-api.core/category} #{}, {:id 19314112, :title "Learning in computer vision", :type :wiki-api.core/category} #{{:id 797088, :title "Computer vision", :type :wiki-api.core/category}}, {:id 1010631, :title "Logic in computer science", :type :wiki-api.core/category} #{{:id 693685, :title "Mathematical logic", :type :wiki-api.core/category}}, {:id 34846657, :title "Syntax (logic)", :type :wiki-api.core/category} #{}, {:id 691875, :title "Mathematical analysis", :type :wiki-api.core/category} #{}, {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} #{{:id 1034006, :title "Particle physics", :type :wiki-api.core/category} {:id 700292, :title "Scientific method", :type :wiki-api.core/category}}, {:id 1817228, :title "Training set", :type :wiki-api.core/article} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category}}, {:id 698571, :title "Nervous system", :type :wiki-api.core/category} #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 11822062, :title "Neurobiology", :type :wiki-api.core/category}}, {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category} #{{:id 26743152, :title "Conceptual models", :type :wiki-api.core/category} {:id 10678854, :title "Collective intelligence", :type :wiki-api.core/category} {:id 966983, :title "Applied mathematics", :type :wiki-api.core/category} {:id 1098276, :title "Mathematical terminology", :type :wiki-api.core/category} {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category}}, {:id 19655050, :title "Directed graphs", :type :wiki-api.core/category} #{}, {:id 1098276, :title "Mathematical terminology", :type :wiki-api.core/category} #{}, {:id 1055691, :title "Learning", :type :wiki-api.core/category} #{}, {:id 505717, :title "Image segmentation", :type :wiki-api.core/article} #{{:id 962413, :title "Image processing", :type :wiki-api.core/category}}, {:id 9272793, :title "Computational statistics", :type :wiki-api.core/category} #{}, {:id 204002, :title "Directed acyclic graph", :type :wiki-api.core/article} #{{:id 19655050, :title "Directed graphs", :type :wiki-api.core/category}}, {:id 12810241, :title "Telecommunication theory", :type :wiki-api.core/category} #{}, {:id 11822062, :title "Neurobiology", :type :wiki-api.core/category} #{{:id 714648, :title "Neuroscience", :type :wiki-api.core/category}}, {:id 349771, :title "Artificial neuron", :type :wiki-api.core/article} #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 4409868, :title "American inventions", :type :wiki-api.core/category}}, {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category} {:id 5038508, :title "Information, knowledge, and uncertainty", :type :wiki-api.core/category}}, {:id 32611713, :title "Emerging technologies", :type :wiki-api.core/category} #{}, {:id 25174161, :title "Open problems", :type :wiki-api.core/category} #{}, {:id 4409868, :title "American inventions", :type :wiki-api.core/category} #{}}, :out-map {{:id 763505, :title "Signal processing", :type :wiki-api.core/category} #{{:id 962413, :title "Image processing", :type :wiki-api.core/category}}, {:id 693702, :title "Functional programming", :type :wiki-api.core/category} #{{:id 2023695, :title "Pattern matching", :type :wiki-api.core/category}}, {:id 690672, :title "Abstract algebra", :type :wiki-api.core/category} #{{:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article}}, {:id 574759, :title "Feed forward (control)", :type :wiki-api.core/article} #{}, {:id 690803, :title "Calculus", :type :wiki-api.core/category} #{{:id 298420, :title "Maxima and minima", :type :wiki-api.core/article}}, {:id 1034006, :title "Particle physics", :type :wiki-api.core/category} #{{:id 14482748, :title "Data analysis", :type :wiki-api.core/category}}, {:id 18039672, :title "Uncertainty of numbers", :type :wiki-api.core/category} #{{:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article}}, {:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} #{}, {:id 34760990, :title "Nonlinear systems", :type :wiki-api.core/category} #{}, {:id 24059390, :title "Markov models", :type :wiki-api.core/category} #{{:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article}}, {:id 24403323, :title "Formal systems", :type :wiki-api.core/category} #{}, {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} #{}, {:id 14787852, :title "Media technology", :type :wiki-api.core/category} #{{:id 763505, :title "Signal processing", :type :wiki-api.core/category}}, {:id 957793, :title "Cognitive science", :type :wiki-api.core/category} #{{:id 1754736, :title "Cybernetics", :type :wiki-api.core/category}}, {:id 744360, :title "Networks", :type :wiki-api.core/category} #{{:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 5206601, :title "Data mining", :type :wiki-api.core/category} #{{:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} {:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category}}, {:id 17503782, :title "Formal sciences", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category} {:id 699134, :title "Formal languages", :type :wiki-api.core/category}}, {:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category} #{{:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article}}, {:id 1195726, :title "Error", :type :wiki-api.core/category} #{{:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article}}, {:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} #{{:id 4594748, :title "Computational problems", :type :wiki-api.core/category}}, {:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} #{}, {:id 8135339, :title "Articles with example Haskell code", :type :wiki-api.core/category} #{{:id 2023695, :title "Pattern matching", :type :wiki-api.core/category}}, {:id 7544087, :title "Norms (mathematics)", :type :wiki-api.core/category} #{{:id 9697, :title "Euclidean space", :type :wiki-api.core/article}}, {:id 694421, :title "Euclidean geometry", :type :wiki-api.core/category} #{{:id 9697, :title "Euclidean space", :type :wiki-api.core/article}}, {:id 126706, :title "Pattern recognition", :type :wiki-api.core/article} #{}, {:id 2157920, :title "Technology in society", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}}, {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} #{}, {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} #{}, {:id 691898, :title "Graph theory", :type :wiki-api.core/category} #{{:id 26304777, :title "Graph theory objects", :type :wiki-api.core/category}}, {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category} #{}, {:id 971740, :title "Topological spaces", :type :wiki-api.core/category} #{{:id 9697, :title "Euclidean space", :type :wiki-api.core/article}}, {:id 5065063, :title "Network architecture", :type :wiki-api.core/category} #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 26304777, :title "Graph theory objects", :type :wiki-api.core/category} #{{:id 325806, :title "Graph (mathematics)", :type :wiki-api.core/article}}, {:id 31195693, :title "Design of experiments", :type :wiki-api.core/category} #{{:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article}}, {:id 26743152, :title "Conceptual models", :type :wiki-api.core/category} #{{:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}, {:id 36312376, :title "Belief revision", :type :wiki-api.core/category} #{{:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article}}, {:id 772545, :title "Probability distributions", :type :wiki-api.core/category} #{{:id 87210, :title "Sigmoid function", :type :wiki-api.core/article}}, {:id 946892, :title "Econometrics", :type :wiki-api.core/category} #{{:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 1754736, :title "Cybernetics", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 17652827, :title "Statistical dependence", :type :wiki-api.core/category} #{{:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article}}, {:id 325806, :title "Graph (mathematics)", :type :wiki-api.core/article} #{}, {:id 4594748, :title "Computational problems", :type :wiki-api.core/category} #{{:id 1126536, :title "Optimization problem", :type :wiki-api.core/article}}, {:id 717695, :title "Systems theory", :type :wiki-api.core/category} #{{:id 1028771, :title "Control theory", :type :wiki-api.core/category} {:id 877149, :title "Dynamical systems", :type :wiki-api.core/category}}, {:id 20926, :title "Supervised learning", :type :wiki-api.core/article} #{}, {:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category} #{{:id 957793, :title "Cognitive science", :type :wiki-api.core/category} {:id 957250, :title "Fuzzy logic", :type :wiki-api.core/category} {:id 1532663, :title "Artificial intelligence applications", :type :wiki-api.core/category} {:id 797088, :title "Computer vision", :type :wiki-api.core/category} {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category}}, {:id 5038508, :title "Information, knowledge, and uncertainty", :type :wiki-api.core/category} #{{:id 18039672, :title "Uncertainty of numbers", :type :wiki-api.core/category} {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category}}, {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} #{{:id 298420, :title "Maxima and minima", :type :wiki-api.core/article}}, {:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article} #{}, {:id 22406300, :title "Handwriting recognition", :type :wiki-api.core/category} #{}, {:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category} #{{:id 242190, :title "Feature extraction", :type :wiki-api.core/article}}, {:id 2266644, :title "Multilayer perceptron", :type :wiki-api.core/article} #{}, {:id 6539521, :title "Statistical theory", :type :wiki-api.core/category} #{{:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category} {:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article} {:id 17735029, :title "Probability interpretations", :type :wiki-api.core/category} {:id 17684887, :title "Statistical approximations", :type :wiki-api.core/category}}, {:id 1028771, :title "Control theory", :type :wiki-api.core/category} #{{:id 574759, :title "Feed forward (control)", :type :wiki-api.core/article}}, {:id 9697, :title "Euclidean space", :type :wiki-api.core/article} #{}, {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article} #{}, {:id 27248985, :title "Mathematical concepts", :type :wiki-api.core/category} #{{:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article}}, {:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category} #{{:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} {:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category}}, {:id 1008581, :title "Operations research", :type :wiki-api.core/category} #{{:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category}}, {:id 734262, :title "Measurement", :type :wiki-api.core/category} #{{:id 33748657, :title "Geometric measurement", :type :wiki-api.core/category} {:id 804551, :title "Psychometrics", :type :wiki-api.core/category}}, {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} #{}, {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} #{}, {:id 957250, :title "Fuzzy logic", :type :wiki-api.core/category} #{}, {:id 87210, :title "Sigmoid function", :type :wiki-api.core/article} #{}, {:id 17735029, :title "Probability interpretations", :type :wiki-api.core/category} #{{:id 957250, :title "Fuzzy logic", :type :wiki-api.core/category}}, {:id 962413, :title "Image processing", :type :wiki-api.core/category} #{{:id 797088, :title "Computer vision", :type :wiki-api.core/category} {:id 505717, :title "Image segmentation", :type :wiki-api.core/article}}, {:id 15214002, :title "Metalogic", :type :wiki-api.core/category} #{{:id 699134, :title "Formal languages", :type :wiki-api.core/category}}, {:id 22166736, :title "Non-classical logic", :type :wiki-api.core/category} #{{:id 957250, :title "Fuzzy logic", :type :wiki-api.core/category}}, {:id 1532663, :title "Artificial intelligence applications", :type :wiki-api.core/category} #{{:id 22406300, :title "Handwriting recognition", :type :wiki-api.core/category}}, {:id 18543448, :title "Universal approximation theorem", :type :wiki-api.core/article} #{}, {:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category} #{{:id 2266644, :title "Multilayer perceptron", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} {:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category}}, {:id 26308484, :title "Summary statistics for contingency tables", :type :wiki-api.core/category} #{{:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article}}, {:id 4002768, :title "Evaluation", :type :wiki-api.core/category} #{{:id 21753440, :title "Critical thinking", :type :wiki-api.core/category}}, {:id 706543, :title "Machine learning", :type :wiki-api.core/category} #{{:id 24059390, :title "Markov models", :type :wiki-api.core/category} {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} {:id 20926, :title "Supervised learning", :type :wiki-api.core/article} {:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category} {:id 1817228, :title "Training set", :type :wiki-api.core/article}}, {:id 33748657, :title "Geometric measurement", :type :wiki-api.core/category} #{{:id 3175294, :title "Dimension", :type :wiki-api.core/category}}, {:id 3175294, :title "Dimension", :type :wiki-api.core/category} #{{:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} {:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category}}, {:id 804551, :title "Psychometrics", :type :wiki-api.core/category} #{{:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article}}, {:id 10939195, :title "Systems", :type :wiki-api.core/category} #{{:id 717695, :title "Systems theory", :type :wiki-api.core/category}}, {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 126706, :title "Pattern recognition", :type :wiki-api.core/article} {:id 325806, :title "Graph (mathematics)", :type :wiki-api.core/article} {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 2266644, :title "Multilayer perceptron", :type :wiki-api.core/article} {:id 1028771, :title "Control theory", :type :wiki-api.core/category} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} {:id 87210, :title "Sigmoid function", :type :wiki-api.core/article} {:id 18543448, :title "Universal approximation theorem", :type :wiki-api.core/article} {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article} {:id 1706332, :title "Feedforward neural network", :type :wiki-api.core/article} {:id 1706303, :title "Recurrent neural network", :type :wiki-api.core/article} {:id 336897, :title "Function approximation", :type :wiki-api.core/article} {:id 698571, :title "Nervous system", :type :wiki-api.core/category} {:id 349771, :title "Artificial neuron", :type :wiki-api.core/article}}, {:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category} #{{:id 1307911, :title "Bootstrap aggregating", :type :wiki-api.core/article} {:id 90500, :title "Boosting (machine learning)", :type :wiki-api.core/article}}, {:id 1307911, :title "Bootstrap aggregating", :type :wiki-api.core/article} #{}, {:id 916810, :title "Spam filtering", :type :wiki-api.core/category} #{{:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article}}, {:id 33420532, :title "Theorems in discrete mathematics", :type :wiki-api.core/category} #{{:id 18543448, :title "Universal approximation theorem", :type :wiki-api.core/article}}, {:id 242190, :title "Feature extraction", :type :wiki-api.core/article} #{}, {:id 797088, :title "Computer vision", :type :wiki-api.core/category} #{{:id 19314112, :title "Learning in computer vision", :type :wiki-api.core/category}}, {:id 36477012, :title "Concepts in physics", :type :wiki-api.core/category} #{{:id 34760990, :title "Nonlinear systems", :type :wiki-api.core/category} {:id 3175294, :title "Dimension", :type :wiki-api.core/category}}, {:id 90500, :title "Boosting (machine learning)", :type :wiki-api.core/article} #{}, {:id 298420, :title "Maxima and minima", :type :wiki-api.core/article} #{}, {:id 17684887, :title "Statistical approximations", :type :wiki-api.core/category} #{{:id 336897, :title "Function approximation", :type :wiki-api.core/article}}, {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article} #{}, {:id 693985, :title "Probability theory", :type :wiki-api.core/category} #{{:id 772545, :title "Probability distributions", :type :wiki-api.core/category} {:id 17735029, :title "Probability interpretations", :type :wiki-api.core/category} {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article}}, {:id 10678854, :title "Collective intelligence", :type :wiki-api.core/category} #{{:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}, {:id 8525467, :title "Pointing-device text input", :type :wiki-api.core/category} #{{:id 22406300, :title "Handwriting recognition", :type :wiki-api.core/category}}, {:id 4289067, :title "Computational neuroscience", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}}, {:id 1706332, :title "Feedforward neural network", :type :wiki-api.core/article} #{}, {:id 699134, :title "Formal languages", :type :wiki-api.core/category} #{{:id 24403323, :title "Formal systems", :type :wiki-api.core/category} {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category}}, {:id 2574961, :title "Formal methods", :type :wiki-api.core/category} #{{:id 24403323, :title "Formal systems", :type :wiki-api.core/category}}, {:id 966983, :title "Applied mathematics", :type :wiki-api.core/category} #{{:id 763505, :title "Signal processing", :type :wiki-api.core/category} {:id 1754736, :title "Cybernetics", :type :wiki-api.core/category} {:id 1008581, :title "Operations research", :type :wiki-api.core/category} {:id 693985, :title "Probability theory", :type :wiki-api.core/category} {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}, {:id 877149, :title "Dynamical systems", :type :wiki-api.core/category} #{{:id 34760990, :title "Nonlinear systems", :type :wiki-api.core/category}}, {:id 5175143, :title "Geostatistics", :type :wiki-api.core/category} #{{:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category}}, {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category} #{{:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} {:id 1307911, :title "Bootstrap aggregating", :type :wiki-api.core/article}}, {:id 693685, :title "Mathematical logic", :type :wiki-api.core/category} #{{:id 24403323, :title "Formal systems", :type :wiki-api.core/category} {:id 22166736, :title "Non-classical logic", :type :wiki-api.core/category} {:id 1010631, :title "Logic in computer science", :type :wiki-api.core/category}}, {:id 21753440, :title "Critical thinking", :type :wiki-api.core/category} #{{:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article}}, {:id 34064788, :title "Neuroethology concepts", :type :wiki-api.core/category} #{{:id 574759, :title "Feed forward (control)", :type :wiki-api.core/article}}, {:id 714648, :title "Neuroscience", :type :wiki-api.core/category} #{{:id 4289067, :title "Computational neuroscience", :type :wiki-api.core/category} {:id 11822062, :title "Neurobiology", :type :wiki-api.core/category}}, {:id 1706303, :title "Recurrent neural network", :type :wiki-api.core/article} #{}, {:id 7297665, :title "Qualities of thought", :type :wiki-api.core/category} #{{:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article}}, {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article} #{}, {:id 10974311, :title "Conditional constructs", :type :wiki-api.core/category} #{{:id 2023695, :title "Pattern matching", :type :wiki-api.core/category}}, {:id 22691076, :title "Graphical models", :type :wiki-api.core/category} #{{:id 24059390, :title "Markov models", :type :wiki-api.core/category} {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category}}, {:id 700292, :title "Scientific method", :type :wiki-api.core/category} #{{:id 31195693, :title "Design of experiments", :type :wiki-api.core/category} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category}}, {:id 690777, :title "Linear algebra", :type :wiki-api.core/category} #{{:id 9697, :title "Euclidean space", :type :wiki-api.core/article}}, {:id 6537403, :title "Biostatistics", :type :wiki-api.core/category} #{{:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article}}, {:id 17193471, :title "Statistical models", :type :wiki-api.core/category} #{{:id 22691076, :title "Graphical models", :type :wiki-api.core/category} {:id 336897, :title "Function approximation", :type :wiki-api.core/article}}, {:id 336897, :title "Function approximation", :type :wiki-api.core/article} #{}, {:id 4064652, :title "Elementary special functions", :type :wiki-api.core/category} #{{:id 87210, :title "Sigmoid function", :type :wiki-api.core/article}}, {:id 19314112, :title "Learning in computer vision", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 1010631, :title "Logic in computer science", :type :wiki-api.core/category} #{{:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} {:id 957250, :title "Fuzzy logic", :type :wiki-api.core/category}}, {:id 34846657, :title "Syntax (logic)", :type :wiki-api.core/category} #{{:id 699134, :title "Formal languages", :type :wiki-api.core/category}}, {:id 691875, :title "Mathematical analysis", :type :wiki-api.core/category} #{{:id 690803, :title "Calculus", :type :wiki-api.core/category} {:id 7544087, :title "Norms (mathematics)", :type :wiki-api.core/category} {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 877149, :title "Dynamical systems", :type :wiki-api.core/category}}, {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 1817228, :title "Training set", :type :wiki-api.core/article}}, {:id 1817228, :title "Training set", :type :wiki-api.core/article} #{}, {:id 698571, :title "Nervous system", :type :wiki-api.core/category} #{}, {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category} #{{:id 772545, :title "Probability distributions", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 19655050, :title "Directed graphs", :type :wiki-api.core/category} #{{:id 204002, :title "Directed acyclic graph", :type :wiki-api.core/article}}, {:id 1098276, :title "Mathematical terminology", :type :wiki-api.core/category} #{{:id 3175294, :title "Dimension", :type :wiki-api.core/category} {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}, {:id 1055691, :title "Learning", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 21753440, :title "Critical thinking", :type :wiki-api.core/category}}, {:id 505717, :title "Image segmentation", :type :wiki-api.core/article} #{}, {:id 9272793, :title "Computational statistics", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 204002, :title "Directed acyclic graph", :type :wiki-api.core/article} #{}, {:id 12810241, :title "Telecommunication theory", :type :wiki-api.core/category} #{{:id 763505, :title "Signal processing", :type :wiki-api.core/category}}, {:id 11822062, :title "Neurobiology", :type :wiki-api.core/category} #{{:id 698571, :title "Nervous system", :type :wiki-api.core/category}}, {:id 349771, :title "Artificial neuron", :type :wiki-api.core/article} #{}, {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category} #{{:id 36312376, :title "Belief revision", :type :wiki-api.core/category} {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}, {:id 32611713, :title "Emerging technologies", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}}, {:id 25174161, :title "Open problems", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}}, {:id 4409868, :title "American inventions", :type :wiki-api.core/category} #{{:id 349771, :title "Artificial neuron", :type :wiki-api.core/article}}}}, :topic-docs #graphs.core.Digraph{:nodes #{1027584 {:id 574759, :title "Feed forward (control)", :type :wiki-api.core/article} 2802560 1023425 3473921 1009281 3643233 744417 {:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} {:id 34760990, :title "Nonlinear systems", :type :wiki-api.core/category} {:id 24403323, :title "Formal systems", :type :wiki-api.core/category} 1031554 {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} 2802562 128930 {:id 5206601, :title "Data mining", :type :wiki-api.core/category} 3706403 1031939 3003171 1208131 1257379 {:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} 3265573 {:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} 1026309 {:id 126706, :title "Pattern recognition", :type :wiki-api.core/article} 3474022 {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} 3614087 3706407 {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} 3174055 228199 3757064 {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category} 973928 3753352 424 1230536 {:id 772545, :title "Probability distributions", :type :wiki-api.core/category} 3706249 3487561 3384458 1010122 3371083 875051 {:id 325806, :title "Graph (mathematics)", :type :wiki-api.core/article} 746059 1009483 3656555 808108 1031916 1279884 {:id 20926, :title "Supervised learning", :type :wiki-api.core/article} 3121357 3706253 3449421 3247757 2939757 {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article} {:id 22406300, :title "Handwriting recognition", :type :wiki-api.core/category} 3732910 1026510 {:id 2266644, :title "Multilayer perceptron", :type :wiki-api.core/article} 3005198 588815 3548495 3706287 1032847 {:id 1028771, :title "Control theory", :type :wiki-api.core/category} {:id 9697, :title "Euclidean space", :type :wiki-api.core/article} 3467280 {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} {:id 957250, :title "Fuzzy logic", :type :wiki-api.core/category} {:id 87210, :title "Sigmoid function", :type :wiki-api.core/article} 3620656 476112 3722545 12849 3537713 {:id 18543448, :title "Universal approximation theorem", :type :wiki-api.core/article} 3706258 3745362 1230546 {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 1307911, :title "Bootstrap aggregating", :type :wiki-api.core/article} 2864499 3655091 3721651 {:id 242190, :title "Feature extraction", :type :wiki-api.core/article} {:id 90500, :title "Boosting (machine learning)", :type :wiki-api.core/article} 1279955 {:id 298420, :title "Maxima and minima", :type :wiki-api.core/article} 1009908 {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article} 1279956 1173909 2878037 1022645 3387061 2866933 {:id 1706332, :title "Feedforward neural network", :type :wiki-api.core/article} {:id 877149, :title "Dynamical systems", :type :wiki-api.core/category} 3621175 1027479 3070519 3381943 3705655 3656760 {:id 1706303, :title "Recurrent neural network", :type :wiki-api.core/article} {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article} 744984 1009913 1031801 3732698 2864474 {:id 336897, :title "Function approximation", :type :wiki-api.core/article} 3461243 3706299 {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} {:id 1817228, :title "Training set", :type :wiki-api.core/article} 3644 860924 {:id 698571, :title "Nervous system", :type :wiki-api.core/category} 746236 3074940 3077116 3706365 593405 {:id 505717, :title "Image segmentation", :type :wiki-api.core/article} {:id 204002, :title "Directed acyclic graph", :type :wiki-api.core/article} {:id 349771, :title "Artificial neuron", :type :wiki-api.core/article} 3005471 2824255 832703 3546399 806847 3547103 3746783}, :in-map {1027584 #{{:id 1028771, :title "Control theory", :type :wiki-api.core/category}}, {:id 574759, :title "Feed forward (control)", :type :wiki-api.core/article} #{}, 2802560 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 1023425 #{{:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 2266644, :title "Multilayer perceptron", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} {:id 1307911, :title "Bootstrap aggregating", :type :wiki-api.core/article} {:id 90500, :title "Boosting (machine learning)", :type :wiki-api.core/article} {:id 1817228, :title "Training set", :type :wiki-api.core/article}}, 3473921 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 1009281 #{{:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} {:id 772545, :title "Probability distributions", :type :wiki-api.core/category} {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 336897, :title "Function approximation", :type :wiki-api.core/article} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} {:id 349771, :title "Artificial neuron", :type :wiki-api.core/article}}, 3643233 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 744417 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, {:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} #{}, {:id 34760990, :title "Nonlinear systems", :type :wiki-api.core/category} #{}, {:id 24403323, :title "Formal systems", :type :wiki-api.core/category} #{}, 1031554 #{{:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} {:id 505717, :title "Image segmentation", :type :wiki-api.core/article}}, {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} #{}, 2802562 #{{:id 22406300, :title "Handwriting recognition", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article}}, 128930 #{{:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} {:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 1817228, :title "Training set", :type :wiki-api.core/article}}, {:id 5206601, :title "Data mining", :type :wiki-api.core/category} #{}, 3706403 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 1031939 #{{:id 126706, :title "Pattern recognition", :type :wiki-api.core/article} {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} {:id 22406300, :title "Handwriting recognition", :type :wiki-api.core/category} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} {:id 242190, :title "Feature extraction", :type :wiki-api.core/article} {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article}}, 3003171 #{{:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category}}, 1208131 #{{:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category} {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 1257379 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 877149, :title "Dynamical systems", :type :wiki-api.core/category} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category}}, {:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} #{}, 3265573 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 1706303, :title "Recurrent neural network", :type :wiki-api.core/article}}, {:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} #{}, 1026309 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 126706, :title "Pattern recognition", :type :wiki-api.core/article} #{}, 3474022 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} #{}, 3614087 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 3706407 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} #{}, 3174055 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 228199 #{{:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category} {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 3757064 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category} #{}, 973928 #{{:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 3753352 #{{:id 34760990, :title "Nonlinear systems", :type :wiki-api.core/category} {:id 1028771, :title "Control theory", :type :wiki-api.core/category}}, 424 #{{:id 24403323, :title "Formal systems", :type :wiki-api.core/category} {:id 325806, :title "Graph (mathematics)", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 1230536 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} {:id 126706, :title "Pattern recognition", :type :wiki-api.core/article} {:id 325806, :title "Graph (mathematics)", :type :wiki-api.core/article} {:id 20926, :title "Supervised learning", :type :wiki-api.core/article} {:id 9697, :title "Euclidean space", :type :wiki-api.core/article} {:id 204002, :title "Directed acyclic graph", :type :wiki-api.core/article} {:id 349771, :title "Artificial neuron", :type :wiki-api.core/article}}, {:id 772545, :title "Probability distributions", :type :wiki-api.core/category} #{}, 3706249 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 3487561 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 3384458 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 1010122 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 3371083 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 875051 #{{:id 34760990, :title "Nonlinear systems", :type :wiki-api.core/category} {:id 126706, :title "Pattern recognition", :type :wiki-api.core/article} {:id 1028771, :title "Control theory", :type :wiki-api.core/category} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category}}, {:id 325806, :title "Graph (mathematics)", :type :wiki-api.core/article} #{}, 746059 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 1009483 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 3656555 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 808108 #{{:id 574759, :title "Feed forward (control)", :type :wiki-api.core/article} {:id 34760990, :title "Nonlinear systems", :type :wiki-api.core/category} {:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 1031916 #{{:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 877149, :title "Dynamical systems", :type :wiki-api.core/category} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category}}, 1279884 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, {:id 20926, :title "Supervised learning", :type :wiki-api.core/article} #{}, 3121357 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 3706253 #{{:id 17306305, :title "Statistical classification", :type :wiki-api.core/category}}, 3449421 #{{:id 574759, :title "Feed forward (control)", :type :wiki-api.core/article}}, 3247757 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 1706303, :title "Recurrent neural network", :type :wiki-api.core/article}}, 2939757 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} #{}, {:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article} #{}, {:id 22406300, :title "Handwriting recognition", :type :wiki-api.core/category} #{}, 3732910 #{{:id 1706303, :title "Recurrent neural network", :type :wiki-api.core/article}}, 1026510 #{{:id 1706332, :title "Feedforward neural network", :type :wiki-api.core/article}}, {:id 2266644, :title "Multilayer perceptron", :type :wiki-api.core/article} #{}, 3005198 #{{:id 126706, :title "Pattern recognition", :type :wiki-api.core/article}}, 588815 #{{:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 698571, :title "Nervous system", :type :wiki-api.core/category}}, 3548495 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 3706287 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 1032847 #{{:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} {:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 1817228, :title "Training set", :type :wiki-api.core/article}}, {:id 1028771, :title "Control theory", :type :wiki-api.core/category} #{}, {:id 9697, :title "Euclidean space", :type :wiki-api.core/article} #{}, 3467280 #{{:id 1706303, :title "Recurrent neural network", :type :wiki-api.core/article}}, {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article} #{}, {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} #{}, {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} #{}, {:id 957250, :title "Fuzzy logic", :type :wiki-api.core/category} #{}, {:id 87210, :title "Sigmoid function", :type :wiki-api.core/article} #{}, 3620656 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 476112 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category}}, 3722545 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 12849 #{{:id 24403323, :title "Formal systems", :type :wiki-api.core/category} {:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 1706332, :title "Feedforward neural network", :type :wiki-api.core/article} {:id 698571, :title "Nervous system", :type :wiki-api.core/category}}, 3537713 #{{:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, {:id 18543448, :title "Universal approximation theorem", :type :wiki-api.core/article} #{}, 3706258 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 3745362 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 1230546 #{{:id 325806, :title "Graph (mathematics)", :type :wiki-api.core/article} {:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article} {:id 9697, :title "Euclidean space", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 18543448, :title "Universal approximation theorem", :type :wiki-api.core/article} {:id 1706332, :title "Feedforward neural network", :type :wiki-api.core/article} {:id 204002, :title "Directed acyclic graph", :type :wiki-api.core/article} {:id 349771, :title "Artificial neuron", :type :wiki-api.core/article}}, {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} #{}, {:id 1307911, :title "Bootstrap aggregating", :type :wiki-api.core/article} #{}, 2864499 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 18543448, :title "Universal approximation theorem", :type :wiki-api.core/article}}, 3655091 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 3721651 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 242190, :title "Feature extraction", :type :wiki-api.core/article} #{}, {:id 90500, :title "Boosting (machine learning)", :type :wiki-api.core/article} #{}, 1279955 #{{:id 18543448, :title "Universal approximation theorem", :type :wiki-api.core/article} {:id 1706332, :title "Feedforward neural network", :type :wiki-api.core/article} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} {:id 349771, :title "Artificial neuron", :type :wiki-api.core/article}}, {:id 298420, :title "Maxima and minima", :type :wiki-api.core/article} #{}, 1009908 #{{:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 87210, :title "Sigmoid function", :type :wiki-api.core/article} {:id 349771, :title "Artificial neuron", :type :wiki-api.core/article}}, {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article} #{}, 1279956 #{{:id 126706, :title "Pattern recognition", :type :wiki-api.core/article} {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} {:id 957250, :title "Fuzzy logic", :type :wiki-api.core/category} {:id 242190, :title "Feature extraction", :type :wiki-api.core/article} {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article} {:id 336897, :title "Function approximation", :type :wiki-api.core/article} {:id 505717, :title "Image segmentation", :type :wiki-api.core/article}}, 1173909 #{{:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article}}, 2878037 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 1022645 #{{:id 34760990, :title "Nonlinear systems", :type :wiki-api.core/category} {:id 772545, :title "Probability distributions", :type :wiki-api.core/category} {:id 298420, :title "Maxima and minima", :type :wiki-api.core/article} {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article}}, 3387061 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 1706303, :title "Recurrent neural network", :type :wiki-api.core/article}}, 2866933 #{{:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} {:id 698571, :title "Nervous system", :type :wiki-api.core/category} {:id 349771, :title "Artificial neuron", :type :wiki-api.core/article}}, {:id 1706332, :title "Feedforward neural network", :type :wiki-api.core/article} #{}, {:id 877149, :title "Dynamical systems", :type :wiki-api.core/category} #{}, 3621175 #{{:id 20926, :title "Supervised learning", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 1027479 #{{:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} {:id 957250, :title "Fuzzy logic", :type :wiki-api.core/category}}, 3070519 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 3381943 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 3705655 #{{:id 698571, :title "Nervous system", :type :wiki-api.core/category}}, 3656760 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, {:id 1706303, :title "Recurrent neural network", :type :wiki-api.core/article} #{}, {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article} #{}, 744984 #{{:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 1009913 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 698571, :title "Nervous system", :type :wiki-api.core/category}}, 1031801 #{{:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category}}, 3732698 #{{:id 1706303, :title "Recurrent neural network", :type :wiki-api.core/article}}, 2864474 #{{:id 34760990, :title "Nonlinear systems", :type :wiki-api.core/category} {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} {:id 772545, :title "Probability distributions", :type :wiki-api.core/category} {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 298420, :title "Maxima and minima", :type :wiki-api.core/article} {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article}}, {:id 336897, :title "Function approximation", :type :wiki-api.core/article} #{}, 3461243 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 3706299 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} #{}, {:id 1817228, :title "Training set", :type :wiki-api.core/article} #{}, 3644 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 860924 #{{:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} {:id 298420, :title "Maxima and minima", :type :wiki-api.core/article}}, {:id 698571, :title "Nervous system", :type :wiki-api.core/category} #{}, 746236 #{{:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} {:id 1307911, :title "Bootstrap aggregating", :type :wiki-api.core/article} {:id 90500, :title "Boosting (machine learning)", :type :wiki-api.core/article}}, 3074940 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 3077116 #{{:id 87210, :title "Sigmoid function", :type :wiki-api.core/article}}, 3706365 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 593405 #{{:id 1028771, :title "Control theory", :type :wiki-api.core/category}}, {:id 505717, :title "Image segmentation", :type :wiki-api.core/article} #{}, {:id 204002, :title "Directed acyclic graph", :type :wiki-api.core/article} #{}, {:id 349771, :title "Artificial neuron", :type :wiki-api.core/article} #{}, 3005471 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, 2824255 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 832703 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 3546399 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 806847 #{{:id 574759, :title "Feed forward (control)", :type :wiki-api.core/article} {:id 126706, :title "Pattern recognition", :type :wiki-api.core/article} {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 2266644, :title "Multilayer perceptron", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 3547103 #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, 3746783 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}}, :out-map {1027584 #{}, {:id 574759, :title "Feed forward (control)", :type :wiki-api.core/article} #{808108 3449421 806847}, 2802560 #{}, 1023425 #{}, 3473921 #{}, 1009281 #{}, 3643233 #{}, 744417 #{}, {:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} #{128930 1032847}, {:id 34760990, :title "Nonlinear systems", :type :wiki-api.core/category} #{3753352 875051 808108 1022645 2864474}, {:id 24403323, :title "Formal systems", :type :wiki-api.core/category} #{424 12849}, 1031554 #{}, {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} #{1031554 2866933 2864474}, 2802562 #{}, 128930 #{}, {:id 5206601, :title "Data mining", :type :wiki-api.core/category} #{744417 1230536 1009913}, 3706403 #{}, 1031939 #{}, 3003171 #{}, 1208131 #{}, 1257379 #{}, {:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} #{1230536 1031916 588815 1009908}, 3265573 #{}, {:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} #{1009281 3537713}, 1026309 #{}, {:id 126706, :title "Pattern recognition", :type :wiki-api.core/article} #{1031939 1230536 875051 3005198 1279956 806847}, 3474022 #{}, {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} #{1031939 1279956}, 3614087 #{}, 3706407 #{}, {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} #{1208131 228199 1173909 860924}, 3174055 #{}, 228199 #{}, 3757064 #{}, {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category} #{1208131 228199}, 973928 #{}, 3753352 #{}, 424 #{}, 1230536 #{}, {:id 772545, :title "Probability distributions", :type :wiki-api.core/category} #{1009281 1022645 2864474}, 3706249 #{}, 3487561 #{}, 3384458 #{}, 1010122 #{}, 3371083 #{}, 875051 #{}, {:id 325806, :title "Graph (mathematics)", :type :wiki-api.core/article} #{424 1230536 1230546}, 746059 #{}, 1009483 #{}, 3656555 #{}, 808108 #{}, 1031916 #{}, 1279884 #{}, {:id 20926, :title "Supervised learning", :type :wiki-api.core/article} #{1230536 3621175}, 3121357 #{}, 3706253 #{}, 3449421 #{}, 3247757 #{}, 2939757 #{}, {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} #{1023425 1009281 3003171 1208131 228199 588815 744984 1031801 806847}, {:id 41932, :title "Accuracy and precision", :type :wiki-api.core/article} #{128930 973928 808108 1032847 12849 1230546 1027479 1009913}, {:id 22406300, :title "Handwriting recognition", :type :wiki-api.core/category} #{2802562 1031939}, 3732910 #{}, 1026510 #{}, {:id 2266644, :title "Multilayer perceptron", :type :wiki-api.core/article} #{1023425 806847}, 3005198 #{}, 588815 #{}, 3548495 #{}, 3706287 #{}, 1032847 #{}, {:id 1028771, :title "Control theory", :type :wiki-api.core/category} #{1027584 3753352 875051 593405}, {:id 9697, :title "Euclidean space", :type :wiki-api.core/article} #{1230536 1230546}, 3467280 #{}, {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article} #{1031554 2864474}, {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} #{1023425 3473921 1009281 744417 1031554 128930 1208131 1257379 3265573 3474022 3614087 228199 973928 424 3384458 746059 1009483 3656555 808108 1279884 3121357 3247757 2939757 588815 3548495 1032847 3620656 476112 12849 3537713 3706258 3745362 1230546 2864499 1009908 3387061 3621175 3656760 744984 1009913 2864474 3706299 3644 2824255 832703 3546399 806847 3547103}, {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} #{1023425 1031554 1031939 875051 3706253 1027479 1031801 746236}, {:id 957250, :title "Fuzzy logic", :type :wiki-api.core/category} #{1279956 1027479}, {:id 87210, :title "Sigmoid function", :type :wiki-api.core/article} #{1009908 3077116}, 3620656 #{}, 476112 #{}, 3722545 #{}, 12849 #{}, 3537713 #{}, {:id 18543448, :title "Universal approximation theorem", :type :wiki-api.core/article} #{1230546 2864499 1279955}, 3706258 #{}, 3745362 #{}, 1230546 #{}, {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} #{2802560 3643233 2802562 3706403 1026309 3706407 3174055 3757064 3706249 3487561 1010122 3371083 1031916 3706287 3722545 3655091 3721651 2878037 3070519 3381943 3461243 3074940 3706365 3005471 3746783}, {:id 1307911, :title "Bootstrap aggregating", :type :wiki-api.core/article} #{1023425 746236}, 2864499 #{}, 3655091 #{}, 3721651 #{}, {:id 242190, :title "Feature extraction", :type :wiki-api.core/article} #{1031939 1279956}, {:id 90500, :title "Boosting (machine learning)", :type :wiki-api.core/article} #{1023425 746236}, 1279955 #{}, {:id 298420, :title "Maxima and minima", :type :wiki-api.core/article} #{1022645 2864474 860924}, 1009908 #{}, {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article} #{1031939 1279956 2864474}, 1279956 #{}, 1173909 #{}, 2878037 #{}, 1022645 #{}, 3387061 #{}, 2866933 #{}, {:id 1706332, :title "Feedforward neural network", :type :wiki-api.core/article} #{1026510 12849 1230546 1279955}, {:id 877149, :title "Dynamical systems", :type :wiki-api.core/category} #{1257379 1031916}, 3621175 #{}, 1027479 #{}, 3070519 #{}, 3381943 #{}, 3705655 #{}, 3656760 #{}, {:id 1706303, :title "Recurrent neural network", :type :wiki-api.core/article} #{3265573 3247757 3732910 3467280 3387061 3732698}, {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article} #{2802562 1022645}, 744984 #{}, 1009913 #{}, 1031801 #{}, 3732698 #{}, 2864474 #{}, {:id 336897, :title "Function approximation", :type :wiki-api.core/article} #{1009281 1279956}, 3461243 #{}, 3706299 #{}, {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} #{1009281 1257379 1031916 476112 1279955}, {:id 1817228, :title "Training set", :type :wiki-api.core/article} #{1023425 128930 1032847}, 3644 #{}, 860924 #{}, {:id 698571, :title "Nervous system", :type :wiki-api.core/category} #{588815 12849 2866933 3705655 1009913}, 746236 #{}, 3074940 #{}, 3077116 #{}, 3706365 #{}, 593405 #{}, {:id 505717, :title "Image segmentation", :type :wiki-api.core/article} #{1031554 1279956}, {:id 204002, :title "Directed acyclic graph", :type :wiki-api.core/article} #{1230536 1230546}, {:id 349771, :title "Artificial neuron", :type :wiki-api.core/article} #{1009281 1230536 1230546 1279955 1009908 2866933}, 3005471 #{}, 2824255 #{}, 832703 #{}, 3546399 #{}, 806847 #{}, 3547103 #{}, 3746783 #{}}}, :doc-map {1027584 #search_api.search_api.Paper{:id 1027584, :key "journals/nn/ZhouLC04", :title "Dynamics of periodic delayed neural networks.", :abstract "This paper formulates and studies a model of periodic delayed neural networks. This model can well describe many practical architectures of delayed neural networks, which is generalization of some additive delayed neural networks such as delayed Hopfied neural networks and delayed cellular neural networks, under a time-varying environment, particularly when the network parameters and input stimuli are varied periodically with time. Without assuming the smoothness, monotonicity and boundedness of the activation functions, the two functional issues on neuronal dynamics of this periodic networks, i.e. the existence and global exponential stability of its periodic solutions, are investigated. Some explicit and conclusive results are established, which are natural extension and generalization of the corresponding results existing in the literature. Furthermore, some examples and simulations are presented to illustrate the practical nature of the new results.", :author (#search_api.search_api.Author{:id 1474522, :first-name nil, :last-name nil, :full-name "Jin Zhou"} #search_api.search_api.Author{:id 311123, :first-name nil, :last-name nil, :full-name "Zengrong Liu"} #search_api.search_api.Author{:id 588736, :first-name nil, :last-name nil, :full-name "Guanrong Chen"}), :year 2004, :venue "Neural Networks", :ncit 116, :string "Dynamics of periodic delayed neural networks.. This paper formulates and studies a model of periodic delayed neural networks. This model can well describe many practical architectures of delayed neural networks, which is generalization of some additive delayed neural networks such as delayed Hopfied neural networks and delayed cellular neural networks, under a time-varying environment, particularly when the network parameters and input stimuli are varied periodically with time. Without assuming the smoothness, monotonicity and boundedness of the activation functions, the two functional issues on neuronal dynamics of this periodic networks, i.e. the existence and global exponential stability of its periodic solutions, are investigated. Some explicit and conclusive results are established, which are natural extension and generalization of the corresponding results existing in the literature. Furthermore, some examples and simulations are presented to illustrate the practical nature of the new results.", :doc-id "Dynamics of periodic delayed neural networks. 2004  ,  ,  "}, 2802560 #search_api.search_api.Paper{:id 2802560, :key "conf/isca/Temam10", :title "The rebirth of neural networks.", :abstract "After the hype of the 1990s, where companies like Intel or Philips built commercial hardware systems based on neural networks, the approach quickly lost ground for multiple reasons: hardware neural networks were no match for software neural networks run on rapidly progressing general-purpose processors, their application scope was considered too limited, and even progress in machine-learning theory overshadowed neural networks. However, in the past few years, a remarkable convergence of trends and innovations is casting a new light on neural networks and could make them valuable components of future computing systems. Trends in technology call for architectures which can sustain a large number of defects, something neural networks are intrinsically capable of. Tends in applications, summarized in the recent RMS categorization, highlight a number of key algorithms which are eligible to neural networks implementations. At the same time, innovations in technology, such as the recent realization of a memristor, are creating the conditions for the efficient hardware implementation of neural networks. Innovations in machine learning, with the recent advent of Deep Networks, have revived interest in neural networks. Finally, recent findings in neurobiology carry even greater prospects, where detailed explanations of how complex functions, such as vision, can be implemented further open up the defect-tolerance and application potential of neural network architectures.", :author (#search_api.search_api.Author{:id 1078089, :first-name nil, :last-name nil, :full-name "Olivier Temam"}), :year 2010, :venue "ISCA", :ncit 1, :string "The rebirth of neural networks.. After the hype of the 1990s, where companies like Intel or Philips built commercial hardware systems based on neural networks, the approach quickly lost ground for multiple reasons: hardware neural networks were no match for software neural networks run on rapidly progressing general-purpose processors, their application scope was considered too limited, and even progress in machine-learning theory overshadowed neural networks. However, in the past few years, a remarkable convergence of trends and innovations is casting a new light on neural networks and could make them valuable components of future computing systems. Trends in technology call for architectures which can sustain a large number of defects, something neural networks are intrinsically capable of. Tends in applications, summarized in the recent RMS categorization, highlight a number of key algorithms which are eligible to neural networks implementations. At the same time, innovations in technology, such as the recent realization of a memristor, are creating the conditions for the efficient hardware implementation of neural networks. Innovations in machine learning, with the recent advent of Deep Networks, have revived interest in neural networks. Finally, recent findings in neurobiology carry even greater prospects, where detailed explanations of how complex functions, such as vision, can be implemented further open up the defect-tolerance and application potential of neural network architectures.", :doc-id "The rebirth of neural networks. 2010  "}, 1023425 #search_api.search_api.Paper{:id 1023425, :key "journals/neco/SchwenkB00", :title "Boosting Neural Networks.", :abstract "Boosting is a general method for improving the performance of learning algorithms. A recently proposed boosting algorithm, AdaBoost, has been applied with great success to several benchmark machine learning problems using mainly decision trees as base classifiers. In this article we investigate whether AdaBoost also works as well with neural networks, and we discuss the advantages and drawbacks of different versions of the AdaBoost algorithm. In particular, we compare training methods based on sampling the training set and weighting the cost function. The results suggest that random resampling of the training data is not the main explanation of the success of the improvements brought by AdaBoost. This is in contrast to bagging, which directly aims at reducing variance and for which random resampling is essential to obtain the reduction in generalization error. Our system achieves about 1.4% error on a data set of on-line handwritten digits from more than 200 writers. A boosted multilayer network achieved 1.5% error on the UCI letters and 8.1% error on the UCI satellite data set, which is significantly better than boosted decision trees.", :author (#search_api.search_api.Author{:id 1332307, :first-name nil, :last-name nil, :full-name "Holger Schwenk"} #search_api.search_api.Author{:id 198739, :first-name nil, :last-name nil, :full-name "Yoshua Bengio"}), :year 2000, :venue "Neural Computation", :ncit 157, :string "Boosting Neural Networks.. Boosting is a general method for improving the performance of learning algorithms. A recently proposed boosting algorithm, AdaBoost, has been applied with great success to several benchmark machine learning problems using mainly decision trees as base classifiers. In this article we investigate whether AdaBoost also works as well with neural networks, and we discuss the advantages and drawbacks of different versions of the AdaBoost algorithm. In particular, we compare training methods based on sampling the training set and weighting the cost function. The results suggest that random resampling of the training data is not the main explanation of the success of the improvements brought by AdaBoost. This is in contrast to bagging, which directly aims at reducing variance and for which random resampling is essential to obtain the reduction in generalization error. Our system achieves about 1.4% error on a data set of on-line handwritten digits from more than 200 writers. A boosted multilayer network achieved 1.5% error on the UCI letters and 8.1% error on the UCI satellite data set, which is significantly better than boosted decision trees.", :doc-id "Boosting Neural Networks. 2000  ,  "}, 3473921 #search_api.search_api.Paper{:id 3473921, :key "journals/corr/abs-1208-0318", :title "Artificial Neural Network Based", :abstract nil, :author (#search_api.search_api.Author{:id 1628806, :first-name nil, :last-name nil, :full-name "Saptarshi Das"} #search_api.search_api.Author{:id 14210221, :first-name nil, :last-name nil, :full-name "Indranil Pan"} #search_api.search_api.Author{:id 14313421, :first-name nil, :last-name nil, :full-name "Khrist Sur"} #search_api.search_api.Author{:id 1360851, :first-name nil, :last-name nil, :full-name "Shantanu Das"}), :year 2012, :venue "CoRR", :ncit 0, :string "Artificial Neural Network Based. ", :doc-id "Artificial Neural Network Based 2012  ,  ,  ,  "}, 1009281 #search_api.search_api.Paper{:id 1009281, :key "journals/ml/Barron94", :title "Approximation and Estimation Bounds for Artificial Neural Networks.", :abstract "For a common class of artificial neural networks, the mean integrated squared error between the estimated network and a target function f is shown to be bounded by O \\left ( {{C^2_f}\\over n}\\right ) + O\\left ({nd\\over N}{\\rm log}\\ N\\right), where n is the number of nodes, d is the input dimension of the function, N is the number of training observations, and Cf is the first absolute moment of the Fourier magnitude distribution of f. The two contributions to this total risk are the approximation error and the estimation error. Approximation error refers to the distance between the target function and the closest neural network function of a given architecture and estimation error refers to the distance between this ideal network function and an estimated network function. With n &tilde; Cf(N/(dlog N))1/2 nodes, the order of the bound on the mean integrated squared error is optimized to be O(Cf((d/N)log N)1/2). The bound demonstrates surprisingly favorable properties of network estimation compared to traditional series and nonparametric curve estimation techniques in the case that d is moderately large. Similar bounds are obtained when the number of nodes n is not preselected as a function of Cf (which is generally not known a priori), but rather the number of nodes is optimized from the observed data by the use of a complexity regularization or minimum description length criterion. The analysis involves Fourier techniques for the approximation error, metric entropy considerations for the estimation error, and a calculation of the index of resolvability of minimum complexity estimation of the family of networks.", :author (#search_api.search_api.Author{:id 652042, :first-name nil, :last-name nil, :full-name "Andrew R. Barron"}), :year 1994, :venue "Machine Learning", :ncit 380, :string "Approximation and Estimation Bounds for Artificial Neural Networks.. For a common class of artificial neural networks, the mean integrated squared error between the estimated network and a target function f is shown to be bounded by O \\left ( {{C^2_f}\\over n}\\right ) + O\\left ({nd\\over N}{\\rm log}\\ N\\right), where n is the number of nodes, d is the input dimension of the function, N is the number of training observations, and Cf is the first absolute moment of the Fourier magnitude distribution of f. The two contributions to this total risk are the approximation error and the estimation error. Approximation error refers to the distance between the target function and the closest neural network function of a given architecture and estimation error refers to the distance between this ideal network function and an estimated network function. With n &tilde; Cf(N/(dlog N))1/2 nodes, the order of the bound on the mean integrated squared error is optimized to be O(Cf((d/N)log N)1/2). The bound demonstrates surprisingly favorable properties of network estimation compared to traditional series and nonparametric curve estimation techniques in the case that d is moderately large. Similar bounds are obtained when the number of nodes n is not preselected as a function of Cf (which is generally not known a priori), but rather the number of nodes is optimized from the observed data by the use of a complexity regularization or minimum description length criterion. The analysis involves Fourier techniques for the approximation error, metric entropy considerations for the estimation error, and a calculation of the index of resolvability of minimum complexity estimation of the family of networks.", :doc-id "Approximation and Estimation Bounds for Artificial Neural Networks. 1994  "}, 3643233 #search_api.search_api.Paper{:id 3643233, :key "journals/jcs/Geigel13", :title "Neural network Trojan.", :abstract nil, :author (#search_api.search_api.Author{:id 14393999, :first-name nil, :last-name nil, :full-name "Arturo Geigel"}), :year 2013, :venue "Journal of Computer Security", :ncit 0, :string "Neural network Trojan.. ", :doc-id "Neural network Trojan. 2013  "}, 744417 #search_api.search_api.Paper{:id 744417, :key "journals/ai/BoutsinasV01", :title "Artificial nonmonotonic neural networks.", :abstract "In this paper, we introduce Artificial Nonmonotonic Neural Networks (ANNNs), a kind of hybrid learning systems that are capable of nonmonotonic reasoning. Nonmonotonic reasoning plays an important role in the development of artificial intelligent systems that try to mimic common sense reasoning, as exhibited by humans. On the other hand, a hybrid learning system provides an explanation capability to trained Neural Networks through acquiring symbolic knowledge of a domain, refining it using a set of classified examples along with Connectionist learning techniques and, finally, extracting comprehensible symbolic information. Artificial Nonmonotonic Neural Networks acquire knowledge represented by a multiple inheritance scheme with exceptions, such as nonmonotonic inheritance networks, and then can extract the refined knowledge in the same scheme. The key idea is to use a special cell operation during training in order to preserve the symbolic meaning of the initial inheritance scheme. Methods for knowledge initialization, knowledge refinement and knowledge extraction are introduced. We, also, prove that these methods address perfectly the constraints imposed by nonmonotonicity. Finally, performance of ANNNs is compared to other well-known hybrid systems, through extensive empirical tests. . 2001 Elsevier Science B.V. All rights reserved.", :author (#search_api.search_api.Author{:id 298090, :first-name nil, :last-name nil, :full-name "Basilis Boutsinas"} #search_api.search_api.Author{:id 1005747, :first-name nil, :last-name nil, :full-name "Michael N. Vrahatis"}), :year 2001, :venue "Artif. Intell.", :ncit 38, :string "Artificial nonmonotonic neural networks.. In this paper, we introduce Artificial Nonmonotonic Neural Networks (ANNNs), a kind of hybrid learning systems that are capable of nonmonotonic reasoning. Nonmonotonic reasoning plays an important role in the development of artificial intelligent systems that try to mimic common sense reasoning, as exhibited by humans. On the other hand, a hybrid learning system provides an explanation capability to trained Neural Networks through acquiring symbolic knowledge of a domain, refining it using a set of classified examples along with Connectionist learning techniques and, finally, extracting comprehensible symbolic information. Artificial Nonmonotonic Neural Networks acquire knowledge represented by a multiple inheritance scheme with exceptions, such as nonmonotonic inheritance networks, and then can extract the refined knowledge in the same scheme. The key idea is to use a special cell operation during training in order to preserve the symbolic meaning of the initial inheritance scheme. Methods for knowledge initialization, knowledge refinement and knowledge extraction are introduced. We, also, prove that these methods address perfectly the constraints imposed by nonmonotonicity. Finally, performance of ANNNs is compared to other well-known hybrid systems, through extensive empirical tests. . 2001 Elsevier Science B.V. All rights reserved.", :doc-id "Artificial nonmonotonic neural networks. 2001  ,  "}, 1031554 #search_api.search_api.Paper{:id 1031554, :key "journals/pami/FengWF02", :title "Combining Belief Networks and Neural Networks for Scene Segmentation.", :abstract "We are concerned with the problem of image segmentation, in which each pixel is assigned to one of a predefined finite number of labels. In Bayesian image analysis, this requires fusing together local predictions for the class labels with a prior model of label images. Following the work of, we consider the use of tree-structured belief networks (TSBNs) as prior models. The parameters in the TSBN are trained using a maximum-likelihood objective function with the EM algorithm and the resulting model is evaluated by calculating how efficiently it codes label images. A number of authors have used Gaussian mixture models to connect the label field to the image data. In this paper, we compare this approach to the scaled-likelihood method of where local predictions of pixel classification from neural networks are fused with the TSBN prior. Our results show a higher performance is obtained with the neural networks. We evaluate the classification results obtained and emphasize not only the maximum a posteriori segmentation, but also the uncertainty, as evidenced e.g., by the pixelwise posterior marginal entropies. We also investigate the use of conditional maximum-likelihood training for the TSBN and find that this gives rise to improved classification performance over the ML-trained TSBN.", :author (#search_api.search_api.Author{:id 739396, :first-name nil, :last-name nil, :full-name "Xiaojuan Feng"} #search_api.search_api.Author{:id 241116, :first-name nil, :last-name nil, :full-name "Christopher K. I. Williams"} #search_api.search_api.Author{:id 549690, :first-name nil, :last-name nil, :full-name "Stephen N. Felderhof"}), :year 2002, :venue "IEEE Trans. Pattern Anal. Mach. Intell.", :ncit 89, :string "Combining Belief Networks and Neural Networks for Scene Segmentation.. We are concerned with the problem of image segmentation, in which each pixel is assigned to one of a predefined finite number of labels. In Bayesian image analysis, this requires fusing together local predictions for the class labels with a prior model of label images. Following the work of, we consider the use of tree-structured belief networks (TSBNs) as prior models. The parameters in the TSBN are trained using a maximum-likelihood objective function with the EM algorithm and the resulting model is evaluated by calculating how efficiently it codes label images. A number of authors have used Gaussian mixture models to connect the label field to the image data. In this paper, we compare this approach to the scaled-likelihood method of where local predictions of pixel classification from neural networks are fused with the TSBN prior. Our results show a higher performance is obtained with the neural networks. We evaluate the classification results obtained and emphasize not only the maximum a posteriori segmentation, but also the uncertainty, as evidenced e.g., by the pixelwise posterior marginal entropies. We also investigate the use of conditional maximum-likelihood training for the TSBN and find that this gives rise to improved classification performance over the ML-trained TSBN.", :doc-id "Combining Belief Networks and Neural Networks for Scene Segmentation. 2002  ,  ,  "}, 2802562 #search_api.search_api.Paper{:id 2802562, :key "conf/isca/ChakradharSJC10", :title "A dynamically configurable coprocessor for convolutional neural networks.", :abstract "Convolutional neural networks (CNN) applications range from recognition and reasoning (such as handwriting recognition, facial expression recognition and video surveillance) to intelligent text applications such as semantic text analysis and natural language processing applications. Two key observations drive the design of a new architecture for CNN. First, CNN workloads exhibit a widely varying mix of three types of parallelism: parallelism within a convolution operation, intra-output parallelism where multiple input sources (features) are combined to create a single output, and inter-output parallelism where multiple, independent outputs (features) are computed simultaneously. Workloads differ significantly across different CNN applications, and across different layers of a CNN. Second, the number of processing elements in an architecture continues to scale (as per Moore's law) much faster than off-chip memory bandwidth (or pin-count) of chips. Based on these two observations, we show that for a given number of processing elements and off-chip memory bandwidth, a new CNN hardware architecture that dynamically configures the hardware on-the-fly to match the specific mix of parallelism in a given workload gives the best throughput performance. Our CNN compiler automatically translates high abstraction network specification into a parallel microprogram (a sequence of low-level VLIW instructions) that is mapped, scheduled and executed by the coprocessor. Compared to a 2.3 GHz quad-core, dual socket Intel Xeon, 1.35 GHz C870 GPU, and a 200 MHz FPGA implementation, our 120 MHz dynamically configurable architecture is 4x to 8x faster. This is the first CNN architecture to achieve real-time video stream processing (25 to 30 frames per second) on a wide range of object detection and recognition tasks.", :author (#search_api.search_api.Author{:id 21047, :first-name nil, :last-name nil, :full-name "Srimat T. Chakradhar"} #search_api.search_api.Author{:id 1118581, :first-name nil, :last-name nil, :full-name "Murugan Sankaradass"} #search_api.search_api.Author{:id 949589, :first-name nil, :last-name nil, :full-name "Venkata Jakkula"} #search_api.search_api.Author{:id 738101, :first-name nil, :last-name nil, :full-name "Srihari Cadambi"}), :year 2010, :venue "ISCA", :ncit 12, :string "A dynamically configurable coprocessor for convolutional neural networks.. Convolutional neural networks (CNN) applications range from recognition and reasoning (such as handwriting recognition, facial expression recognition and video surveillance) to intelligent text applications such as semantic text analysis and natural language processing applications. Two key observations drive the design of a new architecture for CNN. First, CNN workloads exhibit a widely varying mix of three types of parallelism: parallelism within a convolution operation, intra-output parallelism where multiple input sources (features) are combined to create a single output, and inter-output parallelism where multiple, independent outputs (features) are computed simultaneously. Workloads differ significantly across different CNN applications, and across different layers of a CNN. Second, the number of processing elements in an architecture continues to scale (as per Moore's law) much faster than off-chip memory bandwidth (or pin-count) of chips. Based on these two observations, we show that for a given number of processing elements and off-chip memory bandwidth, a new CNN hardware architecture that dynamically configures the hardware on-the-fly to match the specific mix of parallelism in a given workload gives the best throughput performance. Our CNN compiler automatically translates high abstraction network specification into a parallel microprogram (a sequence of low-level VLIW instructions) that is mapped, scheduled and executed by the coprocessor. Compared to a 2.3 GHz quad-core, dual socket Intel Xeon, 1.35 GHz C870 GPU, and a 200 MHz FPGA implementation, our 120 MHz dynamically configurable architecture is 4x to 8x faster. This is the first CNN architecture to achieve real-time video stream processing (25 to 30 frames per second) on a wide range of object detection and recognition tasks.", :doc-id "A dynamically configurable coprocessor for convolutional neural networks. 2010  ,  ,  ,  "}, 128930 #search_api.search_api.Paper{:id 128930, :key "conf/cvpr/RowleyBK96", :title "Neural Network-Based Face Detection.", :abstract "We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates.", :author (#search_api.search_api.Author{:id 522033, :first-name nil, :last-name nil, :full-name "Henry A. Rowley"} #search_api.search_api.Author{:id 460689, :first-name nil, :last-name nil, :full-name "Shumeet Baluja"} #search_api.search_api.Author{:id 367506, :first-name nil, :last-name nil, :full-name "Takeo Kanade"}), :year 1996, :venue "CVPR", :ncit 3698, :string "Neural Network-Based Face Detection.. We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates.", :doc-id "Neural Network-Based Face Detection. 1996  ,  ,  "}, 3706403 #search_api.search_api.Paper{:id 3706403, :key "journals/nn/Fukushima13a", :title "Training multi-layered neural network neocognitron.", :abstract nil, :author (#search_api.search_api.Author{:id 1050638, :first-name nil, :last-name nil, :full-name "Kunihiko Fukushima"}), :year 2013, :venue "Neural Networks", :ncit 0, :string "Training multi-layered neural network neocognitron.. ", :doc-id "Training multi-layered neural network neocognitron. 2013  "}, 1031939 #search_api.search_api.Paper{:id 1031939, :key "journals/pami/JainDM00", :title "Statistical Pattern Recognition: A Review.", :abstract "The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field.", :author (#search_api.search_api.Author{:id 324696, :first-name nil, :last-name nil, :full-name "Anil K. Jain"} #search_api.search_api.Author{:id 189517, :first-name nil, :last-name nil, :full-name "Robert P. W. Duin"} #search_api.search_api.Author{:id 447337, :first-name nil, :last-name nil, :full-name "Jianchang Mao"}), :year 2000, :venue "IEEE Trans. Pattern Anal. Mach. Intell.", :ncit 4345, :string "Statistical Pattern Recognition: A Review.. The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field.", :doc-id "Statistical Pattern Recognition: A Review. 2000  ,  ,  "}, 3003171 #search_api.search_api.Paper{:id 3003171, :key "books/daglib/0070192", :title "Neural networks for optimization and signal processing.", :abstract nil, :author (#search_api.search_api.Author{:id 442845, :first-name nil, :last-name nil, :full-name "Andrzej Cichocki"} #search_api.search_api.Author{:id 416876, :first-name nil, :last-name nil, :full-name "Rolf Unbehauen"}), :year 1993, :venue nil, :ncit 1466, :string "Neural networks for optimization and signal processing.. ", :doc-id "Neural networks for optimization and signal processing. 1993  ,  "}, 1208131 #search_api.search_api.Paper{:id 1208131, :key "conf/gecco/MiikkulainenS09", :title "Evolving neural networks.", :abstract "Neuroevolution, i.e. evolution of artificial neural networks, has recently emerged as a powerful technique for solving challenging reinforcement learning problems. Compared to traditional (e.g. value-function based) methods, neuroevolution is especially strong in domains where the state of the world is not fully known: the state can be disambiguated through recurrency, and novel situations handled through pattern matching. In this tutorial, we will review (1) neuroevolution methods that evolve fixed-topology networks, network topologies, and network construction processes, (2) ways of combining traditional neural network learning algorithms with evolutionary methods, and (3) applications of neuroevolution to game playing, robot control, resource optimization, and cognitive science.", :author (#search_api.search_api.Author{:id 255328, :first-name nil, :last-name nil, :full-name "Risto Miikkulainen"} #search_api.search_api.Author{:id 151275, :first-name nil, :last-name nil, :full-name "Kenneth O. Stanley"}), :year 2009, :venue "GECCO (Companion)", :ncit 14, :string "Evolving neural networks.. Neuroevolution, i.e. evolution of artificial neural networks, has recently emerged as a powerful technique for solving challenging reinforcement learning problems. Compared to traditional (e.g. value-function based) methods, neuroevolution is especially strong in domains where the state of the world is not fully known: the state can be disambiguated through recurrency, and novel situations handled through pattern matching. In this tutorial, we will review (1) neuroevolution methods that evolve fixed-topology networks, network topologies, and network construction processes, (2) ways of combining traditional neural network learning algorithms with evolutionary methods, and (3) applications of neuroevolution to game playing, robot control, resource optimization, and cognitive science.", :doc-id "Evolving neural networks. 2009  ,  "}, 1257379 #search_api.search_api.Paper{:id 1257379, :key "conf/wirn/SerraGV08", :title "Genetic Regulatory Networks and Neural Networks.", :abstract "The discussion on the neural networks paradigm casted a bright light on the notion of network and on what we may define as the “dynamical systems approach”. The rebounds of this emphasis involved, during the years, the most disparate scientific areas, such as biology, social sciences and artificial life, contributing to reinforce the foundations of the complex systems science. In this work we will discuss about a particular network model, i.e. random Boolean networks, which have been introduced to represent the gene regulatory mechanism, and we will comment on the similarities and the differences with the model of neural networks. Moreover, we will present the results of the analysis of the dynamical behaviour that random Boolean networks show in presence of different sets of updating function and discuss on the concepts of criticality and biological suitability of such a model.", :author (#search_api.search_api.Author{:id 1269246, :first-name nil, :last-name nil, :full-name "Roberto Serra"} #search_api.search_api.Author{:id 365401, :first-name nil, :last-name nil, :full-name "Alex Graudenzi"} #search_api.search_api.Author{:id 1208963, :first-name nil, :last-name nil, :full-name "Marco Villani"}), :year 2008, :venue "WIRN", :ncit 6, :string "Genetic Regulatory Networks and Neural Networks.. The discussion on the neural networks paradigm casted a bright light on the notion of network and on what we may define as the “dynamical systems approach”. The rebounds of this emphasis involved, during the years, the most disparate scientific areas, such as biology, social sciences and artificial life, contributing to reinforce the foundations of the complex systems science. In this work we will discuss about a particular network model, i.e. random Boolean networks, which have been introduced to represent the gene regulatory mechanism, and we will comment on the similarities and the differences with the model of neural networks. Moreover, we will present the results of the analysis of the dynamical behaviour that random Boolean networks show in presence of different sets of updating function and discuss on the concepts of criticality and biological suitability of such a model.", :doc-id "Genetic Regulatory Networks and Neural Networks. 2008  ,  ,  "}, 3265573 #search_api.search_api.Paper{:id 3265573, :key "journals/neco/MarkovicG12", :title "Intrinsic Adaptation in Autonomous Recurrent Neural Networks.", :abstract nil, :author (#search_api.search_api.Author{:id 14218327, :first-name nil, :last-name nil, :full-name "Dimitrije Markovic"} #search_api.search_api.Author{:id 56932, :first-name nil, :last-name nil, :full-name "Claudius Gros"}), :year 2012, :venue "Neural Computation", :ncit 0, :string "Intrinsic Adaptation in Autonomous Recurrent Neural Networks.. ", :doc-id "Intrinsic Adaptation in Autonomous Recurrent Neural Networks. 2012  ,  "}, 1026309 #search_api.search_api.Paper{:id 1026309, :key "journals/nn/Funahashi89", :title "On the approximate realization of continuous mappings by neural networks.", :abstract nil, :author (#search_api.search_api.Author{:id 665355, :first-name nil, :last-name nil, :full-name "Ken-ichi Funahashi"}), :year 1989, :venue "Neural Networks", :ncit 2720, :string "On the approximate realization of continuous mappings by neural networks.. ", :doc-id "On the approximate realization of continuous mappings by neural networks. 1989  "}, 3474022 #search_api.search_api.Paper{:id 3474022, :key "journals/corr/abs-1209-4855", :title "The Future of Neural Networks", :abstract nil, :author (#search_api.search_api.Author{:id 5548, :first-name nil, :last-name nil, :full-name "Sachin Lakra"} #search_api.search_api.Author{:id 1270152, :first-name nil, :last-name nil, :full-name "T. V. Prasad"} #search_api.search_api.Author{:id 989837, :first-name nil, :last-name nil, :full-name "G. Ramakrishna"}), :year 2012, :venue "CoRR", :ncit 0, :string "The Future of Neural Networks. ", :doc-id "The Future of Neural Networks 2012  ,  ,  "}, 3614087 #search_api.search_api.Paper{:id 3614087, :key "journals/tnn/Kobayashi13", :title "Hyperbolic Hopfield Neural Networks.", :abstract nil, :author (#search_api.search_api.Author{:id 659370, :first-name nil, :last-name nil, :full-name "Masaki Kobayashi"}), :year 2013, :venue "IEEE Trans. Neural Netw. Learning Syst.", :ncit 0, :string "Hyperbolic Hopfield Neural Networks.. ", :doc-id "Hyperbolic Hopfield Neural Networks. 2013  "}, 3706407 #search_api.search_api.Paper{:id 3706407, :key "journals/nn/AlexandridisZ13", :title "Wavelet neural networks: A practical guide.", :abstract nil, :author (#search_api.search_api.Author{:id 861256, :first-name nil, :last-name nil, :full-name "Antonis Alexandridis"} #search_api.search_api.Author{:id 1101377, :first-name nil, :last-name nil, :full-name "Achilleas Zapranis"}), :year 2013, :venue "Neural Networks", :ncit 0, :string "Wavelet neural networks: A practical guide.. ", :doc-id "Wavelet neural networks: A practical guide. 2013  ,  "}, 3174055 #search_api.search_api.Paper{:id 3174055, :key "conf/gecco/Miikkulainen11", :title "Evolving neural networks.", :abstract nil, :author (#search_api.search_api.Author{:id 255328, :first-name nil, :last-name nil, :full-name "Risto Miikkulainen"}), :year 2011, :venue "GECCO (Companion)", :ncit 13, :string "Evolving neural networks.. ", :doc-id "Evolving neural networks. 2011  "}, 228199 #search_api.search_api.Paper{:id 228199, :key "conf/gecco/MiikkulainenS08", :title "Evolving neural networks.", :abstract "Neuroevolution, i.e. evolution of artificial neural networks, has recently emerged as a powerful technique for solving challenging reinforcement learning problems. Compared to traditional (e.g. value-function based) methods, neuroevolution is especially strong in domains where the state of the world is not fully known: the state can be disambiguated through recurrency, and novel situations handled through pattern matching. In this tutorial, we will review (1) neuroevolution methods that evolve fixed-topology networks, network topologies, and network construction processes, (2) ways of combining traditional neural network learning algorithms with evolutionary methods, and (3) applications of neuroevolution to game playing, robot control, resource optimization, and cognitive science.", :author (#search_api.search_api.Author{:id 255328, :first-name nil, :last-name nil, :full-name "Risto Miikkulainen"} #search_api.search_api.Author{:id 151275, :first-name nil, :last-name nil, :full-name "Kenneth O. Stanley"}), :year 2008, :venue "GECCO (Companion)", :ncit 14, :string "Evolving neural networks.. Neuroevolution, i.e. evolution of artificial neural networks, has recently emerged as a powerful technique for solving challenging reinforcement learning problems. Compared to traditional (e.g. value-function based) methods, neuroevolution is especially strong in domains where the state of the world is not fully known: the state can be disambiguated through recurrency, and novel situations handled through pattern matching. In this tutorial, we will review (1) neuroevolution methods that evolve fixed-topology networks, network topologies, and network construction processes, (2) ways of combining traditional neural network learning algorithms with evolutionary methods, and (3) applications of neuroevolution to game playing, robot control, resource optimization, and cognitive science.", :doc-id "Evolving neural networks. 2008  ,  "}, 3757064 #search_api.search_api.Paper{:id 3757064, :key "conf/ciss/JinDBFC13", :title "Tracking with deep neural networks.", :abstract nil, :author (#search_api.search_api.Author{:id 14312584, :first-name nil, :last-name nil, :full-name "Jonghoon Jin"} #search_api.search_api.Author{:id 14312583, :first-name nil, :last-name nil, :full-name "Aysegul Dundar"} #search_api.search_api.Author{:id 14384102, :first-name nil, :last-name nil, :full-name "Jordan Bates"} #search_api.search_api.Author{:id 1545660, :first-name nil, :last-name nil, :full-name "Clément Farabet"} #search_api.search_api.Author{:id 854877, :first-name nil, :last-name nil, :full-name "Eugenio Culurciello"}), :year 2013, :venue "CISS", :ncit 0, :string "Tracking with deep neural networks.. ", :doc-id "Tracking with deep neural networks. 2013  ,  ,  ,  ,  "}, 973928 #search_api.search_api.Paper{:id 973928, :key "journals/jcst/Zhou04", :title "Rule Extraction: Using Neural Networks or for Neural Networks?", :abstract "In the research of rule extraction from neural networks, fidelity describes how well the rules mimic the behavior of a neural network while accuracy describes how well the rules can be generalized. This paper identifies the fidelity-accuracy dilemma. It argues to distinguish rule extraction using neural networks and rule extraction for neural networks according to their different goals, where fidelity and accuracy should be excluded from the rule quality evaluation framework, respectively.", :author (#search_api.search_api.Author{:id 110846, :first-name nil, :last-name nil, :full-name "Zhi-Hua Zhou"}), :year 2004, :venue "J. Comput. Sci. Technol.", :ncit 43, :string "Rule Extraction: Using Neural Networks or for Neural Networks?. In the research of rule extraction from neural networks, fidelity describes how well the rules mimic the behavior of a neural network while accuracy describes how well the rules can be generalized. This paper identifies the fidelity-accuracy dilemma. It argues to distinguish rule extraction using neural networks and rule extraction for neural networks according to their different goals, where fidelity and accuracy should be excluded from the rule quality evaluation framework, respectively.", :doc-id "Rule Extraction: Using Neural Networks or for Neural Networks? 2004  "}, 3753352 #search_api.search_api.Paper{:id 3753352, :key "conf/isnn/LiuHW13", :title "Neural Network H ∞ Tracking Control of Nonlinear Systems Using GHJI Method.", :abstract nil, :author (#search_api.search_api.Author{:id 61801, :first-name nil, :last-name nil, :full-name "Derong Liu"} #search_api.search_api.Author{:id 14323665, :first-name nil, :last-name nil, :full-name "Yuzhu Huang"} #search_api.search_api.Author{:id 794918, :first-name nil, :last-name nil, :full-name "Qinglai Wei"}), :year 2013, :venue "ISNN (2)", :ncit 0, :string "Neural Network H ∞ Tracking Control of Nonlinear Systems Using GHJI Method.. ", :doc-id "Neural Network H ∞ Tracking Control of Nonlinear Systems Using GHJI Method. 2013  ,  ,  "}, 424 #search_api.search_api.Paper{:id 424, :key "books/crc/tucker97/JordanB97", :title "Neural Networks.", :abstract "We present an overview of current research on artificial neural networks, emphasizing a statistical perspective. We view neural networks as parameterized graphs that make probabilistic assumptions about data, and view learning algorithms as methods for finding parameter values that look probable in the light of the data. We discuss basic issues in representation and learning, and treat some of the practical issues that arise in fitting networks to data. We also discuss links between neural networks and the general formalism of graphical models.", :author (#search_api.search_api.Author{:id 221919, :first-name nil, :last-name nil, :full-name "Michael I. Jordan"} #search_api.search_api.Author{:id 1339062, :first-name nil, :last-name nil, :full-name "Christopher M. Bishop"}), :year 1997, :venue "The Computer Science and Engineering Handbook", :ncit 0, :string "Neural Networks.. We present an overview of current research on artificial neural networks, emphasizing a statistical perspective. We view neural networks as parameterized graphs that make probabilistic assumptions about data, and view learning algorithms as methods for finding parameter values that look probable in the light of the data. We discuss basic issues in representation and learning, and treat some of the practical issues that arise in fitting networks to data. We also discuss links between neural networks and the general formalism of graphical models.", :doc-id "Neural Networks. 1997  ,  "}, 1230536 #search_api.search_api.Paper{:id 1230536, :key "journals/tnn/ScarselliGTHM09", :title "The Graph Neural Network Model.", :abstract "Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function τ(G, n) ∈IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.", :author (#search_api.search_api.Author{:id 1397665, :first-name nil, :last-name nil, :full-name "Franco Scarselli"} #search_api.search_api.Author{:id 249441, :first-name nil, :last-name nil, :full-name "Marco Gori"} #search_api.search_api.Author{:id 1352354, :first-name nil, :last-name nil, :full-name "Ah Chung Tsoi"} #search_api.search_api.Author{:id 907769, :first-name nil, :last-name nil, :full-name "Markus Hagenbuchner"} #search_api.search_api.Author{:id 241568, :first-name nil, :last-name nil, :full-name "Gabriele Monfardini"}), :year 2009, :venue "IEEE Transactions on Neural Networks", :ncit 31, :string "The Graph Neural Network Model.. Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function τ(G, n) ∈IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.", :doc-id "The Graph Neural Network Model. 2009  ,  ,  ,  ,  "}, 3706249 #search_api.search_api.Paper{:id 3706249, :key "journals/nn/CiarelliOS12", :title "An incremental neural network with a reduced architecture.", :abstract nil, :author (#search_api.search_api.Author{:id 591506, :first-name nil, :last-name nil, :full-name "Patrick Marques Ciarelli"} #search_api.search_api.Author{:id 294624, :first-name nil, :last-name nil, :full-name "Elias Oliveira"} #search_api.search_api.Author{:id 976645, :first-name nil, :last-name nil, :full-name "Evandro O. T. Salles"}), :year 2012, :venue "Neural Networks", :ncit 0, :string "An incremental neural network with a reduced architecture.. ", :doc-id "An incremental neural network with a reduced architecture. 2012  ,  ,  "}, 3487561 #search_api.search_api.Paper{:id 3487561, :key "conf/gecco/Stanley12", :title "Evolving neural networks.", :abstract nil, :author (#search_api.search_api.Author{:id 151275, :first-name nil, :last-name nil, :full-name "Kenneth O. Stanley"}), :year 2012, :venue "GECCO (Companion)", :ncit 0, :string "Evolving neural networks.. ", :doc-id "Evolving neural networks. 2012  "}, 3384458 #search_api.search_api.Paper{:id 3384458, :key "journals/corr/abs-1202-4170", :title "Classification by Ensembles of Neural Networks", :abstract nil, :author (#search_api.search_api.Author{:id 14270941, :first-name nil, :last-name nil, :full-name "S. V. Kozyrev"}), :year 2012, :venue "CoRR", :ncit 0, :string "Classification by Ensembles of Neural Networks. ", :doc-id "Classification by Ensembles of Neural Networks 2012  "}, 1010122 #search_api.search_api.Paper{:id 1010122, :key "journals/ml/SilverPC08", :title "Inductive transfer with context-sensitive neural networks.", :abstract "Context-sensitive Multiple Task Learning, or csMTL, is presented as a method of inductive transfer which uses a single output neural network and additional contextual inputs for learning multiple tasks. Motivated by problems with the application of MTL networks to machine lifelong learning systems, csMTL encoding of multiple task examples was developed and found to improve predictive performance. As evidence, the csMTL method is tested on seven task domains and shown to produce hypotheses for primary tasks that are often better than standard MTL hypotheses when learning in the presence of related and unrelated tasks. We argue that the reason for this performance improvement is a reduction in the number of effective free parameters in the csMTL network brought about by the shared output node and weight update constraints due to the context inputs. An examination of IDT and SVM models developed from csMTL encoded data provides initial evidence that this improvement is not shared across all machine learning models.", :author (#search_api.search_api.Author{:id 1367934, :first-name nil, :last-name nil, :full-name "Daniel L. Silver"} #search_api.search_api.Author{:id 15963, :first-name nil, :last-name nil, :full-name "Ryan Poirier"} #search_api.search_api.Author{:id 145024, :first-name nil, :last-name nil, :full-name "Duane Currie"}), :year 2008, :venue "Machine Learning", :ncit 15, :string "Inductive transfer with context-sensitive neural networks.. Context-sensitive Multiple Task Learning, or csMTL, is presented as a method of inductive transfer which uses a single output neural network and additional contextual inputs for learning multiple tasks. Motivated by problems with the application of MTL networks to machine lifelong learning systems, csMTL encoding of multiple task examples was developed and found to improve predictive performance. As evidence, the csMTL method is tested on seven task domains and shown to produce hypotheses for primary tasks that are often better than standard MTL hypotheses when learning in the presence of related and unrelated tasks. We argue that the reason for this performance improvement is a reduction in the number of effective free parameters in the csMTL network brought about by the shared output node and weight update constraints due to the context inputs. An examination of IDT and SVM models developed from csMTL encoded data provides initial evidence that this improvement is not shared across all machine learning models.", :doc-id "Inductive transfer with context-sensitive neural networks. 2008  ,  ,  "}, 3371083 #search_api.search_api.Paper{:id 3371083, :key "journals/nn/Zhdanov12", :title "Neural networks including microRNAs.", :abstract nil, :author (#search_api.search_api.Author{:id 657799, :first-name nil, :last-name nil, :full-name "Vladimir P. Zhdanov"}), :year 2012, :venue "Neural Networks", :ncit 0, :string "Neural networks including microRNAs.. ", :doc-id "Neural networks including microRNAs. 2012  "}, 875051 #search_api.search_api.Paper{:id 875051, :key "journals/expert/Lewis96", :title "Neural Network Control of Robot Manipulators.", :abstract "In this article, the author describes neural network controllers for robot manipulators in a variety of applications, including position control, force control, parallel-link mechanisms, and digital neural network control. These \"model-free\" controllers offer a powerful and robust alternative to adaptive control.In recent years there has been increasing interest in universal model-free controllers. Mimicing the functions of human processes, these controllers learn about the systems they are controlling on-line, and thus automatically improve their performance. So far, neural networks have made their mark in the areas of classification and pattern recognition; with this success they've become an important tool in the repertoire of the signal processor and computer scientist. However, the same cannot be said for neural networks in system theory applications.There has been a good deal of research on the use of neural networks for control, although most of the articles have been ad hoc discussions lacking theoretical proofs and repeatable design algorithms. As a result, neither the control systems community nor US industry have fully accepted neural networks for closed-loop control applications. In this article, I address the major problems facing neural network control and demonstrate that neural networks do indeed fulfill the promise of providing model-free learning controllers for a class of nonlinear systems.The basic challenges for neural network control areproviding repeatable design algorithms, providing on-line learning algorithms that do not require preliminary off-line tuning,initializing the neural network weights for guaranteed stability,demonstrating closed-loop trajectory following,computing various weight tuning gradients, anddemonstrating that the neural network weights remain bounded despite unmodelled dynamics-because bounded weights guarantee bounded control signals.K.S. Narendra and others have paved the way for neural network control by studying the dynamical behavior of neural networks in closed-loop applications, including computation of the gradients needed for backpropagation tuning. (There are also several groups currently analyzing neural network controllers using a variety of techniques.) Unfortunately, the necessary gradients often depend on the unknown system or satisfy their own differential equations. Thus, though rigorously applying them to identification, researchers have not fully developed neural networks for direct closed-loop control.", :author (#search_api.search_api.Author{:id 604512, :first-name nil, :last-name nil, :full-name "Frank L. Lewis"}), :year 1996, :venue "IEEE Expert", :ncit 781, :string "Neural Network Control of Robot Manipulators.. In this article, the author describes neural network controllers for robot manipulators in a variety of applications, including position control, force control, parallel-link mechanisms, and digital neural network control. These \"model-free\" controllers offer a powerful and robust alternative to adaptive control.In recent years there has been increasing interest in universal model-free controllers. Mimicing the functions of human processes, these controllers learn about the systems they are controlling on-line, and thus automatically improve their performance. So far, neural networks have made their mark in the areas of classification and pattern recognition; with this success they've become an important tool in the repertoire of the signal processor and computer scientist. However, the same cannot be said for neural networks in system theory applications.There has been a good deal of research on the use of neural networks for control, although most of the articles have been ad hoc discussions lacking theoretical proofs and repeatable design algorithms. As a result, neither the control systems community nor US industry have fully accepted neural networks for closed-loop control applications. In this article, I address the major problems facing neural network control and demonstrate that neural networks do indeed fulfill the promise of providing model-free learning controllers for a class of nonlinear systems.The basic challenges for neural network control areproviding repeatable design algorithms, providing on-line learning algorithms that do not require preliminary off-line tuning,initializing the neural network weights for guaranteed stability,demonstrating closed-loop trajectory following,computing various weight tuning gradients, anddemonstrating that the neural network weights remain bounded despite unmodelled dynamics-because bounded weights guarantee bounded control signals.K.S. Narendra and others have paved the way for neural network control by studying the dynamical behavior of neural networks in closed-loop applications, including computation of the gradients needed for backpropagation tuning. (There are also several groups currently analyzing neural network controllers using a variety of techniques.) Unfortunately, the necessary gradients often depend on the unknown system or satisfy their own differential equations. Thus, though rigorously applying them to identification, researchers have not fully developed neural networks for direct closed-loop control.", :doc-id "Neural Network Control of Robot Manipulators. 1996  "}, 746059 #search_api.search_api.Paper{:id 746059, :key "journals/ai/TowellS94", :title "Knowledge-Based Artificial Neural Networks.", :abstract nil, :author (#search_api.search_api.Author{:id 456443, :first-name nil, :last-name nil, :full-name "Geoffrey G. Towell"} #search_api.search_api.Author{:id 7, :first-name nil, :last-name nil, :full-name "Jude W. Shavlik"}), :year 1994, :venue "Artif. Intell.", :ncit 628, :string "Knowledge-Based Artificial Neural Networks.. ", :doc-id "Knowledge-Based Artificial Neural Networks. 1994  ,  "}, 1009483 #search_api.search_api.Paper{:id 1009483, :key "journals/ml/Lee03", :title "A Noninformative Prior for Neural Networks.", :abstract nil, :author (#search_api.search_api.Author{:id 669791, :first-name nil, :last-name nil, :full-name "Herbert K. H. Lee"}), :year 2003, :venue "Machine Learning", :ncit 12, :string "A Noninformative Prior for Neural Networks.. ", :doc-id "A Noninformative Prior for Neural Networks. 2003  "}, 3656555 #search_api.search_api.Paper{:id 3656555, :key "journals/tcas/ZhangH13", :title "Network-Based Synchronization of Delayed Neural Networks.", :abstract nil, :author (#search_api.search_api.Author{:id 476405, :first-name nil, :last-name nil, :full-name "Yijun Zhang"} #search_api.search_api.Author{:id 76605, :first-name nil, :last-name nil, :full-name "Qing-Long Han"}), :year 2013, :venue "IEEE Trans. on Circuits and Systems", :ncit 0, :string "Network-Based Synchronization of Delayed Neural Networks.. ", :doc-id "Network-Based Synchronization of Delayed Neural Networks. 2013  ,  "}, 808108 #search_api.search_api.Paper{:id 808108, :key "journals/computer/SetionoL96", :title "Symbolic Representation of Neural Networks.", :abstract "The highly nonlinear nature of neural networks' input-to-output mapping makes it difficult to describe how they arrive at predictions. Thus, although their predictive accuracy is satisfactory for applications from finance to medicine, they have long been thought of as \"black boxes.\" The authors propose to understand a neural network via rules extracted from it. Their algorithm, NeuroRule, extracts rules from a standard feed-forward neural network, with network training and pruning via the simple, widely used back-propagation method. The extracted rules, a one-to-one mapping of the pruned network, are compact and comprehensible and do not involve weight values. The authors' experiments show that neural-network-based rules are as accurate and compact as decision-tree-based rules, which are widely regarded as explicit and understandable. Thus, using rules extracted by Neuro-Rule, neural networks become understandable and could lose their black-box reputation.", :author (#search_api.search_api.Author{:id 1046049, :first-name nil, :last-name nil, :full-name "Rudy Setiono"} #search_api.search_api.Author{:id 517599, :first-name nil, :last-name nil, :full-name "Huan Liu"}), :year 1996, :venue "IEEE Computer", :ncit 181, :string "Symbolic Representation of Neural Networks.. The highly nonlinear nature of neural networks' input-to-output mapping makes it difficult to describe how they arrive at predictions. Thus, although their predictive accuracy is satisfactory for applications from finance to medicine, they have long been thought of as \"black boxes.\" The authors propose to understand a neural network via rules extracted from it. Their algorithm, NeuroRule, extracts rules from a standard feed-forward neural network, with network training and pruning via the simple, widely used back-propagation method. The extracted rules, a one-to-one mapping of the pruned network, are compact and comprehensible and do not involve weight values. The authors' experiments show that neural-network-based rules are as accurate and compact as decision-tree-based rules, which are widely regarded as explicit and understandable. Thus, using rules extracted by Neuro-Rule, neural networks become understandable and could lose their black-box reputation.", :doc-id "Symbolic Representation of Neural Networks. 1996  ,  "}, 1031916 #search_api.search_api.Paper{:id 1031916, :key "journals/pami/IttiKN98", :title "A Model of Saliency-Based Visual Attention for Rapid Scene Analysis.", :abstract "A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail.", :author (#search_api.search_api.Author{:id 680963, :first-name nil, :last-name nil, :full-name "Laurent Itti"} #search_api.search_api.Author{:id 9218, :first-name nil, :last-name nil, :full-name "Christof Koch"} #search_api.search_api.Author{:id 1254094, :first-name nil, :last-name nil, :full-name "Ernst Niebur"}), :year 1998, :venue "IEEE Trans. Pattern Anal. Mach. Intell.", :ncit 4473, :string "A Model of Saliency-Based Visual Attention for Rapid Scene Analysis.. A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail.", :doc-id "A Model of Saliency-Based Visual Attention for Rapid Scene Analysis. 1998  ,  ,  "}, 1279884 #search_api.search_api.Paper{:id 1279884, :key "journals/neco/GemanBD92", :title "Neural Networks and the Bias/Variance Dilemma.", :abstract nil, :author (#search_api.search_api.Author{:id 258662, :first-name nil, :last-name nil, :full-name "Stuart Geman"} #search_api.search_api.Author{:id 8550, :first-name nil, :last-name nil, :full-name "Elie Bienenstock"} #search_api.search_api.Author{:id 1007227, :first-name nil, :last-name nil, :full-name "René Doursat"}), :year 1992, :venue "Neural Computation", :ncit 2265, :string "Neural Networks and the Bias/Variance Dilemma.. ", :doc-id "Neural Networks and the Bias/Variance Dilemma. 1992  ,  ,  "}, 3121357 #search_api.search_api.Paper{:id 3121357, :key "reference/ml/X10ox", :title "Neural Networks.", :abstract nil, :author (), :year 2010, :venue "J. Sensor Technology", :ncit 0, :string "Neural Networks.. ", :doc-id "Neural Networks. 2010 "}, 3706253 #search_api.search_api.Paper{:id 3706253, :key "journals/nn/OzyildirimA13", :title "Generalized classifier neural network.", :abstract nil, :author (#search_api.search_api.Author{:id 14419162, :first-name nil, :last-name nil, :full-name "Buse Melis Ozyildirim"} #search_api.search_api.Author{:id 313392, :first-name nil, :last-name nil, :full-name "Mutlu Avci"}), :year 2013, :venue "Neural Networks", :ncit 0, :string "Generalized classifier neural network.. ", :doc-id "Generalized classifier neural network. 2013  ,  "}, 3449421 #search_api.search_api.Paper{:id 3449421, :key "journals/eaai/KhasheiB12", :title "Hybridization of the probabilistic neural networks with feed-forward neural networks for forecasting.", :abstract nil, :author (#search_api.search_api.Author{:id 138765, :first-name nil, :last-name nil, :full-name "Mehdi Khashei"} #search_api.search_api.Author{:id 1106251, :first-name nil, :last-name nil, :full-name "Mehdi Bijari"}), :year 2012, :venue "Eng. Appl. of AI", :ncit 1, :string "Hybridization of the probabilistic neural networks with feed-forward neural networks for forecasting.. ", :doc-id "Hybridization of the probabilistic neural networks with feed-forward neural networks for forecasting. 2012  ,  "}, 3247757 #search_api.search_api.Paper{:id 3247757, :key "journals/pami/FrinkenFMB12", :title "A Novel Word Spotting Method Based on Recurrent Neural Networks.", :abstract nil, :author (#search_api.search_api.Author{:id 844000, :first-name nil, :last-name nil, :full-name "Volkmar Frinken"} #search_api.search_api.Author{:id 82228, :first-name nil, :last-name nil, :full-name "Andreas Fischer"} #search_api.search_api.Author{:id 852825, :first-name nil, :last-name nil, :full-name "R. Manmatha"} #search_api.search_api.Author{:id 1493079, :first-name nil, :last-name nil, :full-name "Horst Bunke"}), :year 2012, :venue "IEEE Trans. Pattern Anal. Mach. Intell.", :ncit 11, :string "A Novel Word Spotting Method Based on Recurrent Neural Networks.. ", :doc-id "A Novel Word Spotting Method Based on Recurrent Neural Networks. 2012  ,  ,  ,  "}, 2939757 #search_api.search_api.Paper{:id 2939757, :key "books/daglib/0024011", :title "Neural Networks and Micromechanics.", :abstract nil, :author (#search_api.search_api.Author{:id 435401, :first-name nil, :last-name nil, :full-name "Ernst M. Kussul"} #search_api.search_api.Author{:id 1244056, :first-name nil, :last-name nil, :full-name "Tatiana Baidyk"} #search_api.search_api.Author{:id 9152, :first-name nil, :last-name nil, :full-name "Donald C. Wunsch"}), :year 2010, :venue nil, :ncit 16, :string "Neural Networks and Micromechanics.. ", :doc-id "Neural Networks and Micromechanics. 2010  ,  ,  "}, 3732910 #search_api.search_api.Paper{:id 3732910, :key "journals/scholarpedia/Grossberg13", :title "Recurrent neural networks.", :abstract nil, :author (#search_api.search_api.Author{:id 1303332, :first-name nil, :last-name nil, :full-name "Stephen Grossberg"}), :year 2013, :venue "Scholarpedia", :ncit 0, :string "Recurrent neural networks.. ", :doc-id "Recurrent neural networks. 2013  "}, 1026510 #search_api.search_api.Paper{:id 1026510, :key "journals/nn/HornikSW89", :title "Multilayer feedforward networks are universal approximators.", :abstract nil, :author (#search_api.search_api.Author{:id 250536, :first-name nil, :last-name nil, :full-name "Kurt Hornik"} #search_api.search_api.Author{:id 1180392, :first-name nil, :last-name nil, :full-name "Maxwell B. Stinchcombe"} #search_api.search_api.Author{:id 1058794, :first-name nil, :last-name nil, :full-name "Halbert White"}), :year 1989, :venue "Neural Networks", :ncit 9550, :string "Multilayer feedforward networks are universal approximators.. ", :doc-id "Multilayer feedforward networks are universal approximators. 1989  ,  ,  "}, 3005198 #search_api.search_api.Paper{:id 3005198, :key "books/daglib/0082591", :title "Pattern recognition and neural networks.", :abstract nil, :author (#search_api.search_api.Author{:id 437988, :first-name nil, :last-name nil, :full-name "Brian D. Ripley"}), :year 1996, :venue nil, :ncit 4746, :string "Pattern recognition and neural networks.. ", :doc-id "Pattern recognition and neural networks. 1996  "}, 588815 #search_api.search_api.Paper{:id 588815, :key "conf/sigcomm/Matsumoto94", :title "On Optimization of Polling Policy Represented by Neural Network.", :abstract "This paper deals with the problem of scheduling a server in a polling system with multiple queues and complete information. We represent the polling policy by a neural network; namely, given the number of waiting customers in each queue, the server determines next queue he should visit according to the output of the neural network. By using the simulated annealing method, we improve the neural polling policy in such a way that the mean delay of customers is minimized. Numerical results show that the present approach is especially valid for asymmetric polling systems whose analytical optimization is considered intractable.", :author (#search_api.search_api.Author{:id 820887, :first-name nil, :last-name nil, :full-name "Yutaka Matsumoto"}), :year 1994, :venue "SIGCOMM", :ncit 5, :string "On Optimization of Polling Policy Represented by Neural Network.. This paper deals with the problem of scheduling a server in a polling system with multiple queues and complete information. We represent the polling policy by a neural network; namely, given the number of waiting customers in each queue, the server determines next queue he should visit according to the output of the neural network. By using the simulated annealing method, we improve the neural polling policy in such a way that the mean delay of customers is minimized. Numerical results show that the present approach is especially valid for asymmetric polling systems whose analytical optimization is considered intractable.", :doc-id "On Optimization of Polling Policy Represented by Neural Network. 1994  "}, 3548495 #search_api.search_api.Paper{:id 3548495, :key "journals/corr/abs-1210-6511", :title "Neural Networks for Complex Data", :abstract nil, :author (#search_api.search_api.Author{:id 698686, :first-name nil, :last-name nil, :full-name "Marie Cottrell"} #search_api.search_api.Author{:id 1004406, :first-name nil, :last-name nil, :full-name "Madalina Olteanu"} #search_api.search_api.Author{:id 1364782, :first-name nil, :last-name nil, :full-name "Fabrice Rossi"} #search_api.search_api.Author{:id 213043, :first-name nil, :last-name nil, :full-name "Joseph Rynkiewicz"} #search_api.search_api.Author{:id 81370, :first-name nil, :last-name nil, :full-name "Nathalie Villa-Vialaneix"}), :year 2012, :venue "CoRR", :ncit 1, :string "Neural Networks for Complex Data. ", :doc-id "Neural Networks for Complex Data 2012  ,  ,  ,  ,  "}, 3706287 #search_api.search_api.Paper{:id 3706287, :key "journals/nn/Werbos12", :title "Neural networks and the experience and cultivation of mind.", :abstract nil, :author (#search_api.search_api.Author{:id 849103, :first-name nil, :last-name nil, :full-name "Paul J. Werbos"}), :year 2012, :venue "Neural Networks", :ncit 0, :string "Neural networks and the experience and cultivation of mind.. ", :doc-id "Neural networks and the experience and cultivation of mind. 2012  "}, 1032847 #search_api.search_api.Paper{:id 1032847, :key "journals/pami/RowleyBK98", :title "Neural Network-Based Face Detection.", :abstract "We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates.", :author (#search_api.search_api.Author{:id 522033, :first-name nil, :last-name nil, :full-name "Henry A. Rowley"} #search_api.search_api.Author{:id 460689, :first-name nil, :last-name nil, :full-name "Shumeet Baluja"} #search_api.search_api.Author{:id 367506, :first-name nil, :last-name nil, :full-name "Takeo Kanade"}), :year 1998, :venue "IEEE Trans. Pattern Anal. Mach. Intell.", :ncit 3698, :string "Neural Network-Based Face Detection.. We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates.", :doc-id "Neural Network-Based Face Detection. 1998  ,  ,  "}, 3467280 #search_api.search_api.Paper{:id 3467280, :key "journals/bc/BitzerK12", :title "Recognizing recurrent neural networks (rRNN): Bayesian inference for recurrent neural networks.", :abstract nil, :author (#search_api.search_api.Author{:id 876675, :first-name nil, :last-name nil, :full-name "Sebastian Bitzer"} #search_api.search_api.Author{:id 797547, :first-name nil, :last-name nil, :full-name "Stefan J. Kiebel"}), :year 2012, :venue "Biological Cybernetics", :ncit 0, :string "Recognizing recurrent neural networks (rRNN): Bayesian inference for recurrent neural networks.. ", :doc-id "Recognizing recurrent neural networks (rRNN): Bayesian inference for recurrent neural networks. 2012  ,  "}, 3620656 #search_api.search_api.Paper{:id 3620656, :key "journals/corr/abs-1301-3583", :title "Big Neural Networks Waste Capacity", :abstract nil, :author (#search_api.search_api.Author{:id 14169497, :first-name nil, :last-name nil, :full-name "Yann Dauphin"} #search_api.search_api.Author{:id 198739, :first-name nil, :last-name nil, :full-name "Yoshua Bengio"}), :year 2013, :venue "CoRR", :ncit 3, :string "Big Neural Networks Waste Capacity. ", :doc-id "Big Neural Networks Waste Capacity 2013  ,  "}, 476112 #search_api.search_api.Paper{:id 476112, :key "conf/kes/Moraga07", :title "of Neural Networks.", :abstract "The paper offers a critical analysis of the procedure observed in many applications of neural networks. Given a problem to be solved, a favorite NN-architecture is chosen and its parameters tuned with some standard training algorithm, but without taking in consideration relevant features of the problem or possibly its interdisciplinary nature. Three relevant benchmark problems are discussed to illustrate the thesis that \"brute force solving is not the same as understanding\".", :author (#search_api.search_api.Author{:id 679384, :first-name nil, :last-name nil, :full-name "Claudio Moraga"}), :year 2007, :venue "KES (1)", :ncit 0, :string "of Neural Networks.. The paper offers a critical analysis of the procedure observed in many applications of neural networks. Given a problem to be solved, a favorite NN-architecture is chosen and its parameters tuned with some standard training algorithm, but without taking in consideration relevant features of the problem or possibly its interdisciplinary nature. Three relevant benchmark problems are discussed to illustrate the thesis that \"brute force solving is not the same as understanding\".", :doc-id "of Neural Networks. 2007  "}, 3722545 #search_api.search_api.Paper{:id 3722545, :key "journals/corr/abs-1304-5232", :title "Neural network function, density or geometry?", :abstract nil, :author (#search_api.search_api.Author{:id 14426260, :first-name nil, :last-name nil, :full-name "Anca Radulescu"}), :year 2013, :venue "CoRR", :ncit 0, :string "Neural network function, density or geometry?. ", :doc-id "Neural network function, density or geometry? 2013  "}, 12849 #search_api.search_api.Paper{:id 12849, :key "conf/aaai/GarcezG04", :title "Fibring Neural Networks.", :abstract "Neural-symbolic systems are hybrid systems that integrate symbolic logic and neural networks. The goal of neural-symbolic integration is to benefit from the combination of features of the symbolic and connectionist paradigms of artificial intelligence. This paper introduces a new neural network architecture based on the idea of fibring logical systems. Fibring allows one to combine different logical systems in a principled way. Fibred neural networks may be composed not only of interconnected neurons but also of other networks, forming a recursive architecture. A fibring function then defines how this recursive architecture must behave by defining how the networks in the ensemble relate to each other, typically by allowing the activation of neurons in one network (A) to influence the change of weights in another network (B). Intuitively, this can be seen as training network B at the same time that one runs network A. We show that, in addition to being universal approximators like standard feedforward networks, fibred neural networks can approximate any polynomial function to any desired degree of accuracy, thus being more expressive than standard feedforward networks.", :author (#search_api.search_api.Author{:id 318826, :first-name nil, :last-name nil, :full-name "Artur S. d'Avila Garcez"} #search_api.search_api.Author{:id 63389, :first-name nil, :last-name nil, :full-name "Dov M. Gabbay"}), :year 2004, :venue "AAAI", :ncit 28, :string "Fibring Neural Networks.. Neural-symbolic systems are hybrid systems that integrate symbolic logic and neural networks. The goal of neural-symbolic integration is to benefit from the combination of features of the symbolic and connectionist paradigms of artificial intelligence. This paper introduces a new neural network architecture based on the idea of fibring logical systems. Fibring allows one to combine different logical systems in a principled way. Fibred neural networks may be composed not only of interconnected neurons but also of other networks, forming a recursive architecture. A fibring function then defines how this recursive architecture must behave by defining how the networks in the ensemble relate to each other, typically by allowing the activation of neurons in one network (A) to influence the change of weights in another network (B). Intuitively, this can be seen as training network B at the same time that one runs network A. We show that, in addition to being universal approximators like standard feedforward networks, fibred neural networks can approximate any polynomial function to any desired degree of accuracy, thus being more expressive than standard feedforward networks.", :doc-id "Fibring Neural Networks. 2004  ,  "}, 3537713 #search_api.search_api.Paper{:id 3537713, :key "journals/pami/JiXYY13", :title "3D Convolutional Neural Networks for Human Action Recognition.", :abstract nil, :author (#search_api.search_api.Author{:id 1404867, :first-name nil, :last-name nil, :full-name "Shuiwang Ji"} #search_api.search_api.Author{:id 79953, :first-name nil, :last-name nil, :full-name "Wei Xu"} #search_api.search_api.Author{:id 341330, :first-name nil, :last-name nil, :full-name "Ming Yang"} #search_api.search_api.Author{:id 651344, :first-name nil, :last-name nil, :full-name "Kai Yu"}), :year 2013, :venue "IEEE Trans. Pattern Anal. Mach. Intell.", :ncit 34, :string "3D Convolutional Neural Networks for Human Action Recognition.. ", :doc-id "3D Convolutional Neural Networks for Human Action Recognition. 2013  ,  ,  ,  "}, 3706258 #search_api.search_api.Paper{:id 3706258, :key "journals/nn/WarlaumontWBO13", :title "Prespeech motor learning in a neural network using reinforcement.", :abstract nil, :author (#search_api.search_api.Author{:id 14128844, :first-name nil, :last-name nil, :full-name "Anne S. Warlaumont"} #search_api.search_api.Author{:id 1429132, :first-name nil, :last-name nil, :full-name "Gert Westermann"} #search_api.search_api.Author{:id 14110078, :first-name nil, :last-name nil, :full-name "Eugene H. Buder"} #search_api.search_api.Author{:id 14285267, :first-name nil, :last-name nil, :full-name "D. Kimbrough Oller"}), :year 2013, :venue "Neural Networks", :ncit 0, :string "Prespeech motor learning in a neural network using reinforcement.. ", :doc-id "Prespeech motor learning in a neural network using reinforcement. 2013  ,  ,  ,  "}, 3745362 #search_api.search_api.Paper{:id 3745362, :key "reference/nc/0003K12", :title "Neural Networks in Bioinformatics.", :abstract nil, :author (#search_api.search_api.Author{:id 1105041, :first-name nil, :last-name nil, :full-name "Ke Chen"} #search_api.search_api.Author{:id 1079333, :first-name nil, :last-name nil, :full-name "Lukasz A. Kurgan"}), :year 2012, :venue "Handbook of Natural Computing", :ncit 0, :string "Neural Networks in Bioinformatics.. ", :doc-id "Neural Networks in Bioinformatics. 2012  ,  "}, 1230546 #search_api.search_api.Paper{:id 1230546, :key "journals/tnn/ScarselliGTHM09a", :title "Computational Capabilities of Graph Neural Networks.", :abstract "In this paper, we will consider the approximation properties of a recently introduced neural network model called graph neural network (GNN), which can be used to process-structured data inputs, e.g., acyclic graphs, cyclic graphs, and directed or undirected graphs. This class of neural networks implements a function τ(G, n) ∈ IRm that maps a graph G and one of its nodes n onto an m-dimensional Euclidean space. We characterize the functions that can be approximated by GNNs, in probability, up to any prescribed degree of precision. This set contains the maps that satisfy a property called preservation of the unfolding equivalence, and includes most of the practically useful functions on graphs; the only known exception is when the input graph contains particular patterns of symmetries when unfolding equivalence may not be preserved. The result can be considered an extension of the universal approximation property established for the classic feedforward neural networks (FNNs). Some experimental examples are used to show the computational capabilities of the proposed model.", :author (#search_api.search_api.Author{:id 1397665, :first-name nil, :last-name nil, :full-name "Franco Scarselli"} #search_api.search_api.Author{:id 249441, :first-name nil, :last-name nil, :full-name "Marco Gori"} #search_api.search_api.Author{:id 1352354, :first-name nil, :last-name nil, :full-name "Ah Chung Tsoi"} #search_api.search_api.Author{:id 907769, :first-name nil, :last-name nil, :full-name "Markus Hagenbuchner"} #search_api.search_api.Author{:id 241568, :first-name nil, :last-name nil, :full-name "Gabriele Monfardini"}), :year 2009, :venue "IEEE Transactions on Neural Networks", :ncit 19, :string "Computational Capabilities of Graph Neural Networks.. In this paper, we will consider the approximation properties of a recently introduced neural network model called graph neural network (GNN), which can be used to process-structured data inputs, e.g., acyclic graphs, cyclic graphs, and directed or undirected graphs. This class of neural networks implements a function τ(G, n) ∈ IRm that maps a graph G and one of its nodes n onto an m-dimensional Euclidean space. We characterize the functions that can be approximated by GNNs, in probability, up to any prescribed degree of precision. This set contains the maps that satisfy a property called preservation of the unfolding equivalence, and includes most of the practically useful functions on graphs; the only known exception is when the input graph contains particular patterns of symmetries when unfolding equivalence may not be preserved. The result can be considered an extension of the universal approximation property established for the classic feedforward neural networks (FNNs). Some experimental examples are used to show the computational capabilities of the proposed model.", :doc-id "Computational Capabilities of Graph Neural Networks. 2009  ,  ,  ,  ,  "}, 2864499 #search_api.search_api.Paper{:id 2864499, :key "journals/jmlr/DugasBBNG09", :title "Incorporating Functional Knowledge in Neural Networks.", :abstract "Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in its two arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of Lipschitz functions with these and other properties. We apply this new class of functions to the task of modelling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints.", :author (#search_api.search_api.Author{:id 936309, :first-name nil, :last-name nil, :full-name "Charles Dugas"} #search_api.search_api.Author{:id 198739, :first-name nil, :last-name nil, :full-name "Yoshua Bengio"} #search_api.search_api.Author{:id 313059, :first-name nil, :last-name nil, :full-name "François Bélisle"} #search_api.search_api.Author{:id 956110, :first-name nil, :last-name nil, :full-name "Claude Nadeau"} #search_api.search_api.Author{:id 1399504, :first-name nil, :last-name nil, :full-name "René Garcia"}), :year 2009, :venue "Journal of Machine Learning Research", :ncit 3, :string "Incorporating Functional Knowledge in Neural Networks.. Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in its two arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of Lipschitz functions with these and other properties. We apply this new class of functions to the task of modelling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints.", :doc-id "Incorporating Functional Knowledge in Neural Networks. 2009  ,  ,  ,  ,  "}, 3655091 #search_api.search_api.Paper{:id 3655091, :key "journals/npl/AguilarM13", :title "The Multilayer Random Neural Network.", :abstract nil, :author (#search_api.search_api.Author{:id 458973, :first-name nil, :last-name nil, :full-name "Jose Aguilar"} #search_api.search_api.Author{:id 14399199, :first-name nil, :last-name nil, :full-name "Cristhian Molina"}), :year 2013, :venue "Neural Processing Letters", :ncit 0, :string "The Multilayer Random Neural Network.. ", :doc-id "The Multilayer Random Neural Network. 2013  ,  "}, 3721651 #search_api.search_api.Paper{:id 3721651, :key "journals/corr/abs-1303-0818", :title "Riemannian metrics for neural networks", :abstract nil, :author (#search_api.search_api.Author{:id 784926, :first-name nil, :last-name nil, :full-name "Yann Ollivier"}), :year 2013, :venue "CoRR", :ncit 0, :string "Riemannian metrics for neural networks. ", :doc-id "Riemannian metrics for neural networks 2013  "}, 1279955 #search_api.search_api.Paper{:id 1279955, :key "journals/nn/Sun10", :title "Local coupled feedforward neural network.", :abstract "In this paper, the local coupled feedforward neural network is presented. Its connection structure is same as that of Multilayer Perceptron with one hidden layer. In the local coupled feedforward neural network, each hidden node is assigned an address in an input space, and each input activates only the hidden nodes near it. For each input, only the activated hidden nodes take part in forward and backward propagation processes. Theoretical analysis and simulation results show that this neural network owns the ''universal approximation'' property and can solve the learning problem of feedforward neural networks. In addition, its characteristic of local coupling makes knowledge accumulation possible.", :author (#search_api.search_api.Author{:id 711930, :first-name nil, :last-name nil, :full-name "Jianye Sun"}), :year 2010, :venue "Neural Networks", :ncit 2, :string "Local coupled feedforward neural network.. In this paper, the local coupled feedforward neural network is presented. Its connection structure is same as that of Multilayer Perceptron with one hidden layer. In the local coupled feedforward neural network, each hidden node is assigned an address in an input space, and each input activates only the hidden nodes near it. For each input, only the activated hidden nodes take part in forward and backward propagation processes. Theoretical analysis and simulation results show that this neural network owns the ''universal approximation'' property and can solve the learning problem of feedforward neural networks. In addition, its characteristic of local coupling makes knowledge accumulation possible.", :doc-id "Local coupled feedforward neural network. 2010  "}, 1009908 #search_api.search_api.Paper{:id 1009908, :key "journals/ml/Schmitt99", :title "On the Sample Complexity for Nonoverlapping Neural Networks.", :abstract "A neural network is said to be nonoverlapping if there is at most one edge outgoing from each node. We investigate the number of examples that a learning algorithm needs when using nonoverlapping neural networks as hypotheses. We derive bounds for this sample complexity in terms of the Vapnik-Chervonenkis dimension. In particular, we consider networks consisting of threshold, sigmoidal and linear gates. We show that the class of nonoverlapping threshold networks and the class of nonoverlapping sigmoidal networks on n inputs both have Vapnik-Chervonenkis dimension &Omega;(nlog n). This bound is asymptotically tight for the class of nonoverlapping threshold networks. We also present an upper bound for this class where the constants involved are considerably smaller than in a previous calculation. Finally, we argue that the Vapnik-Chervonenkis dimension of nonoverlapping threshold or sigmoidal networks cannot become larger by allowing the nodes to compute linear functions. This sheds some light on a recent result that exhibited neural networks with quadratic Vapnik-Chervonenkis dimension.", :author (#search_api.search_api.Author{:id 62459, :first-name nil, :last-name nil, :full-name "Michael Schmitt"}), :year 1999, :venue "Machine Learning", :ncit 3, :string "On the Sample Complexity for Nonoverlapping Neural Networks.. A neural network is said to be nonoverlapping if there is at most one edge outgoing from each node. We investigate the number of examples that a learning algorithm needs when using nonoverlapping neural networks as hypotheses. We derive bounds for this sample complexity in terms of the Vapnik-Chervonenkis dimension. In particular, we consider networks consisting of threshold, sigmoidal and linear gates. We show that the class of nonoverlapping threshold networks and the class of nonoverlapping sigmoidal networks on n inputs both have Vapnik-Chervonenkis dimension &Omega;(nlog n). This bound is asymptotically tight for the class of nonoverlapping threshold networks. We also present an upper bound for this class where the constants involved are considerably smaller than in a previous calculation. Finally, we argue that the Vapnik-Chervonenkis dimension of nonoverlapping threshold or sigmoidal networks cannot become larger by allowing the nodes to compute linear functions. This sheds some light on a recent result that exhibited neural networks with quadratic Vapnik-Chervonenkis dimension.", :doc-id "On the Sample Complexity for Nonoverlapping Neural Networks. 1999  "}, 1279956 #search_api.search_api.Paper{:id 1279956, :key "journals/nn/Du10", :title "Clustering: A neural network approach.", :abstract "Clustering is a fundamental data analysis method. It is widely used for pattern recognition, feature extraction, vector quantization (VQ), image segmentation, function approximation, and data mining. As an unsupervised classification technique, clustering identifies some inherent structures present in a set of objects based on a similarity measure. Clustering methods can be based on statistical model identification (McLachlan & Basford, 1988) or competitive learning. In this paper, we give a comprehensive overview of competitive learning based clustering methods. Importance is attached to a number of competitive learning based clustering neural networks such as the self-organizing map (SOM), the learning vector quantization (LVQ), the neural gas, and the ART model, and clustering algorithms such as the C-means, mountain/subtractive clustering, and fuzzy C-means (FCM) algorithms. Associated topics such as the under-utilization problem, fuzzy clustering, robust clustering, clustering based on non-Euclidean distance measures, supervised clustering, hierarchical clustering as well as cluster validity are also described. Two examples are given to demonstrate the use of the clustering methods.", :author (#search_api.search_api.Author{:id 1308303, :first-name nil, :last-name nil, :full-name "K.-L. Du"}), :year 2010, :venue "Neural Networks", :ncit 0, :string "Clustering: A neural network approach.. Clustering is a fundamental data analysis method. It is widely used for pattern recognition, feature extraction, vector quantization (VQ), image segmentation, function approximation, and data mining. As an unsupervised classification technique, clustering identifies some inherent structures present in a set of objects based on a similarity measure. Clustering methods can be based on statistical model identification (McLachlan & Basford, 1988) or competitive learning. In this paper, we give a comprehensive overview of competitive learning based clustering methods. Importance is attached to a number of competitive learning based clustering neural networks such as the self-organizing map (SOM), the learning vector quantization (LVQ), the neural gas, and the ART model, and clustering algorithms such as the C-means, mountain/subtractive clustering, and fuzzy C-means (FCM) algorithms. Associated topics such as the under-utilization problem, fuzzy clustering, robust clustering, clustering based on non-Euclidean distance measures, supervised clustering, hierarchical clustering as well as cluster validity are also described. Two examples are given to demonstrate the use of the clustering methods.", :doc-id "Clustering: A neural network approach. 2010  "}, 1173909 #search_api.search_api.Paper{:id 1173909, :key "journals/tnn/SuttonB98", :title "Reinforcement Learning: An Introduction.", :abstract nil, :author (#search_api.search_api.Author{:id 759360, :first-name nil, :last-name nil, :full-name "Richard S. Sutton"} #search_api.search_api.Author{:id 138227, :first-name nil, :last-name nil, :full-name "Andrew G. Barto"}), :year 1998, :venue "IEEE Transactions on Neural Networks", :ncit 15127, :string "Reinforcement Learning: An Introduction.. ", :doc-id "Reinforcement Learning: An Introduction. 1998  ,  "}, 2878037 #search_api.search_api.Paper{:id 2878037, :key "journals/scholarpedia/RoskaP09", :title "Cellular neural network.", :abstract nil, :author (#search_api.search_api.Author{:id 1224345, :first-name nil, :last-name nil, :full-name "Tamás Roska"} #search_api.search_api.Author{:id 1246229, :first-name nil, :last-name nil, :full-name "Giovanni Egidio Pazienza"}), :year 2009, :venue "Scholarpedia", :ncit 0, :string "Cellular neural network.. ", :doc-id "Cellular neural network. 2009  ,  "}, 1022645 #search_api.search_api.Paper{:id 1022645, :key "journals/neco/BellS95", :title "An information-maximization approach to blind separation and blind deconvolution.", :abstract "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing.", :author (#search_api.search_api.Author{:id 248601, :first-name nil, :last-name nil, :full-name "Anthony J. Bell"} #search_api.search_api.Author{:id 207416, :first-name nil, :last-name nil, :full-name "Terrence J. Sejnowski"}), :year 1995, :venue "Neural Computation", :ncit 6227, :string "An information-maximization approach to blind separation and blind deconvolution.. We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing.", :doc-id "An information-maximization approach to blind separation and blind deconvolution. 1995  ,  "}, 3387061 #search_api.search_api.Paper{:id 3387061, :key "journals/neco/CabessaS12", :title "The Computational Power of Interactive Recurrent Neural Networks.", :abstract nil, :author (#search_api.search_api.Author{:id 1196304, :first-name nil, :last-name nil, :full-name "Jérémie Cabessa"} #search_api.search_api.Author{:id 352646, :first-name nil, :last-name nil, :full-name "Hava T. Siegelmann"}), :year 2012, :venue "Neural Computation", :ncit 3, :string "The Computational Power of Interactive Recurrent Neural Networks.. ", :doc-id "The Computational Power of Interactive Recurrent Neural Networks. 2012  ,  "}, 2866933 #search_api.search_api.Paper{:id 2866933, :key "journals/nn/Sun10a", :title "Neural belief network.", :abstract "In this paper, a new neural belief network, which has considered backward inferences and the influence of the belief sources on belief propagations, is developed. In this new neural network, a link record set is built for every conclusion node for handling the multiple conditions of inference rules, and a route record set is built for every active node and every active link for handling the dependency of belief propagations on the belief sources. In addition, a temporary node is added for every evidence node. The assignment of the temporary nodes releases the evidence nodes from the role as belief sources and allows belief propagations in them. As a result, the new neural belief network can handle both definite evidences and indefinite evidences, and the evidences may come from observations or the prior knowledge of experts. The inference processes of the new neural belief network are based on available evidences and if...then rules. Therefore, it can solve the problems of Bayesian networks caused by the prior knowledge reliance and may be an alternative technique to the popular Bayesian networks.", :author (#search_api.search_api.Author{:id 711930, :first-name nil, :last-name nil, :full-name "Jianye Sun"}), :year 2010, :venue "Neural Networks", :ncit 1, :string "Neural belief network.. In this paper, a new neural belief network, which has considered backward inferences and the influence of the belief sources on belief propagations, is developed. In this new neural network, a link record set is built for every conclusion node for handling the multiple conditions of inference rules, and a route record set is built for every active node and every active link for handling the dependency of belief propagations on the belief sources. In addition, a temporary node is added for every evidence node. The assignment of the temporary nodes releases the evidence nodes from the role as belief sources and allows belief propagations in them. As a result, the new neural belief network can handle both definite evidences and indefinite evidences, and the evidences may come from observations or the prior knowledge of experts. The inference processes of the new neural belief network are based on available evidences and if...then rules. Therefore, it can solve the problems of Bayesian networks caused by the prior knowledge reliance and may be an alternative technique to the popular Bayesian networks.", :doc-id "Neural belief network. 2010  "}, 3621175 #search_api.search_api.Paper{:id 3621175, :key "journals/neco/SporeaG13", :title "Supervised Learning in Multilayer Spiking Neural Networks.", :abstract nil, :author (#search_api.search_api.Author{:id 14205193, :first-name nil, :last-name nil, :full-name "Ioana Sporea"} #search_api.search_api.Author{:id 1547670, :first-name nil, :last-name nil, :full-name "André Grüning"}), :year 2013, :venue "Neural Computation", :ncit 0, :string "Supervised Learning in Multilayer Spiking Neural Networks.. ", :doc-id "Supervised Learning in Multilayer Spiking Neural Networks. 2013  ,  "}, 1027479 #search_api.search_api.Paper{:id 1027479, :key "journals/nn/WangH03", :title "Extension neural network and its applications.", :abstract "In this paper, a novel extension neural network (ENN) is proposed. This new neural network is a combination of extension theory and neural network. It uses an extension distance (ED) to measure the similarity between data and cluster center. The learning speed of the proposed ENN is shown to be faster than the traditional neural networks and other fuzzy classification methods. Moreover, the new scheme has been proved to have high accuracy and less memory consumption. Experimental results from two different examples verify the effectiveness and applicability of the proposed work.", :author (#search_api.search_api.Author{:id 735970, :first-name nil, :last-name nil, :full-name "M. H. Wang"} #search_api.search_api.Author{:id 615382, :first-name nil, :last-name nil, :full-name "C. P. Hung"}), :year 2003, :venue "Neural Networks", :ncit 31, :string "Extension neural network and its applications.. In this paper, a novel extension neural network (ENN) is proposed. This new neural network is a combination of extension theory and neural network. It uses an extension distance (ED) to measure the similarity between data and cluster center. The learning speed of the proposed ENN is shown to be faster than the traditional neural networks and other fuzzy classification methods. Moreover, the new scheme has been proved to have high accuracy and less memory consumption. Experimental results from two different examples verify the effectiveness and applicability of the proposed work.", :doc-id "Extension neural network and its applications. 2003  ,  "}, 3070519 #search_api.search_api.Paper{:id 3070519, :key "journals/ijon/YeZ11", :title "The bridge relating process neural networks and traditional neural networks.", :abstract nil, :author (#search_api.search_api.Author{:id 391255, :first-name nil, :last-name nil, :full-name "Tao Ye"} #search_api.search_api.Author{:id 572465, :first-name nil, :last-name nil, :full-name "Xuefeng Zhu"}), :year 2011, :venue "Neurocomputing", :ncit 1, :string "The bridge relating process neural networks and traditional neural networks.. ", :doc-id "The bridge relating process neural networks and traditional neural networks. 2011  ,  "}, 3381943 #search_api.search_api.Paper{:id 3381943, :key "journals/pieee/EbongM12", :title "CMOS and Memristor-Based Neural Network Design for Position Detection.", :abstract nil, :author (#search_api.search_api.Author{:id 1600985, :first-name nil, :last-name nil, :full-name "Idongesit E. Ebong"} #search_api.search_api.Author{:id 400937, :first-name nil, :last-name nil, :full-name "Pinaki Mazumder"}), :year 2012, :venue "Proceedings of the IEEE", :ncit 3, :string "CMOS and Memristor-Based Neural Network Design for Position Detection.. ", :doc-id "CMOS and Memristor-Based Neural Network Design for Position Detection. 2012  ,  "}, 3705655 #search_api.search_api.Paper{:id 3705655, :key "journals/mcs/WangZTQC12", :title "Neural coding in networks of multi-populations of neural oscillators.", :abstract nil, :author (#search_api.search_api.Author{:id 489313, :first-name nil, :last-name nil, :full-name "Rubin Wang"} #search_api.search_api.Author{:id 20596, :first-name nil, :last-name nil, :full-name "Zhikang Zhang"} #search_api.search_api.Author{:id 74805, :first-name nil, :last-name nil, :full-name "Chi K. Tse"} #search_api.search_api.Author{:id 1376546, :first-name nil, :last-name nil, :full-name "Jingyi Qu"} #search_api.search_api.Author{:id 2502, :first-name nil, :last-name nil, :full-name "Jianting Cao"}), :year 2012, :venue "Mathematics and Computers in Simulation", :ncit 0, :string "Neural coding in networks of multi-populations of neural oscillators.. ", :doc-id "Neural coding in networks of multi-populations of neural oscillators. 2012  ,  ,  ,  ,  "}, 3656760 #search_api.search_api.Paper{:id 3656760, :key "journals/air/DingLSYJ13", :title "Evolutionary artificial neural networks: a review.", :abstract nil, :author (#search_api.search_api.Author{:id 106215, :first-name nil, :last-name nil, :full-name "Shifei Ding"} #search_api.search_api.Author{:id 663448, :first-name nil, :last-name nil, :full-name "Hui Li"} #search_api.search_api.Author{:id 784756, :first-name nil, :last-name nil, :full-name "Chunyang Su"} #search_api.search_api.Author{:id 14168184, :first-name nil, :last-name nil, :full-name "Junzhao Yu"} #search_api.search_api.Author{:id 132843, :first-name nil, :last-name nil, :full-name "Fengxiang Jin"}), :year 2013, :venue "Artif. Intell. Rev.", :ncit 3, :string "Evolutionary artificial neural networks: a review.. ", :doc-id "Evolutionary artificial neural networks: a review. 2013  ,  ,  ,  ,  "}, 744984 #search_api.search_api.Paper{:id 744984, :key "journals/ai/GranittoVC05", :title "Neural network ensembles: evaluation of aggregation algorithms.", :abstract "Ensembles of artificial neural networks show improved generalization capabilities that outperform those of single networks. However, for aggregation to be effective, the individual networks must be as accurate and diverse as possible. An important problem is, then, how to tune the aggregate members in order to have an optimal compromise between these two conflicting conditions. We present here an extensive evaluation of several algorithms for ensemble construction, including new proposals and comparing them with standard methods in the literature. We also discuss a potential problem with sequential aggregation algorithms: the non-frequent but damaging selection through their heuristics of particularly bad ensemble members. We introduce modified algorithms that cope with this problem by allowing individual weighting of aggregate members. Our algorithms and their weighted modifications are favorably tested against other methods in the literature, producing a sensible improvement in performance on most of the standard statistical databases used as benchmarks.", :author (#search_api.search_api.Author{:id 1019048, :first-name nil, :last-name nil, :full-name "Pablo M. Granitto"} #search_api.search_api.Author{:id 947900, :first-name nil, :last-name nil, :full-name "Pablo F. Verdes"} #search_api.search_api.Author{:id 1253412, :first-name nil, :last-name nil, :full-name "H. Alejandro Ceccatto"}), :year 2005, :venue "Artif. Intell.", :ncit 65, :string "Neural network ensembles: evaluation of aggregation algorithms.. Ensembles of artificial neural networks show improved generalization capabilities that outperform those of single networks. However, for aggregation to be effective, the individual networks must be as accurate and diverse as possible. An important problem is, then, how to tune the aggregate members in order to have an optimal compromise between these two conflicting conditions. We present here an extensive evaluation of several algorithms for ensemble construction, including new proposals and comparing them with standard methods in the literature. We also discuss a potential problem with sequential aggregation algorithms: the non-frequent but damaging selection through their heuristics of particularly bad ensemble members. We introduce modified algorithms that cope with this problem by allowing individual weighting of aggregate members. Our algorithms and their weighted modifications are favorably tested against other methods in the literature, producing a sensible improvement in performance on most of the standard statistical databases used as benchmarks.", :doc-id "Neural network ensembles: evaluation of aggregation algorithms. 2005  ,  ,  "}, 1009913 #search_api.search_api.Paper{:id 1009913, :key "journals/ml/TowellS93", :title "Extracting Refined Rules from Knowledge-Based Neural Networks.", :abstract "Neural networks, despite their empirically proven abilities, have been little used for the refinement of existing knowledge because this task requires a three-step process. First, knowledge must be inserted into a neural network. Second, the network must be refined. Third, the refined knowledge must be extracted from the network. We have previously described a method for the first step of this process. Standard neural learning techniques can accomplish the second step. In this article, we propose and empirically evaluate a method for the final, and possibly most difficult, step. Our method efficiently extracts symbolic rules from trained neural networks. The four major results of empirical tests of this method are that the extracted rules 1) closely reproduce the accuracy of the network from which they are extracted&semi; 2) are superior to the rules produced by methods that directly refine symbolic rules&semi; 3) are superior to those produced by previous techniques for extracting rules from trained neural networks&semi; and 4) are &ldquo;human comprehensible.&rdquo; Thus, this method demonstrates that neural networks can be used to effectively refine symbolic knowledge. Moreover, the rule-extraction technique developed herein contributes to the understanding of how symbolic and connectionist approaches to artificial intelligence can be profitably integrated.", :author (#search_api.search_api.Author{:id 456443, :first-name nil, :last-name nil, :full-name "Geoffrey G. Towell"} #search_api.search_api.Author{:id 7, :first-name nil, :last-name nil, :full-name "Jude W. Shavlik"}), :year 1993, :venue "Machine Learning", :ncit 770, :string "Extracting Refined Rules from Knowledge-Based Neural Networks.. Neural networks, despite their empirically proven abilities, have been little used for the refinement of existing knowledge because this task requires a three-step process. First, knowledge must be inserted into a neural network. Second, the network must be refined. Third, the refined knowledge must be extracted from the network. We have previously described a method for the first step of this process. Standard neural learning techniques can accomplish the second step. In this article, we propose and empirically evaluate a method for the final, and possibly most difficult, step. Our method efficiently extracts symbolic rules from trained neural networks. The four major results of empirical tests of this method are that the extracted rules 1) closely reproduce the accuracy of the network from which they are extracted&semi; 2) are superior to the rules produced by methods that directly refine symbolic rules&semi; 3) are superior to those produced by previous techniques for extracting rules from trained neural networks&semi; and 4) are &ldquo;human comprehensible.&rdquo; Thus, this method demonstrates that neural networks can be used to effectively refine symbolic knowledge. Moreover, the rule-extraction technique developed herein contributes to the understanding of how symbolic and connectionist approaches to artificial intelligence can be profitably integrated.", :doc-id "Extracting Refined Rules from Knowledge-Based Neural Networks. 1993  ,  "}, 1031801 #search_api.search_api.Paper{:id 1031801, :key "journals/pami/HansenS90", :title "Neural Network Ensembles.", :abstract "Several means for improving the performance and training of neural networks for classification are proposed. Crossvalidation is used as a tool for optimizing network parameters and architecture. It is shown that the remaining residual generalization error can be reduced by invoking ensembles of similar networks.", :author (#search_api.search_api.Author{:id 177472, :first-name nil, :last-name nil, :full-name "Lars Kai Hansen"} #search_api.search_api.Author{:id 235369, :first-name nil, :last-name nil, :full-name "Peter Salamon"}), :year 1990, :venue "IEEE Trans. Pattern Anal. Mach. Intell.", :ncit 2123, :string "Neural Network Ensembles.. Several means for improving the performance and training of neural networks for classification are proposed. Crossvalidation is used as a tool for optimizing network parameters and architecture. It is shown that the remaining residual generalization error can be reduced by invoking ensembles of similar networks.", :doc-id "Neural Network Ensembles. 1990  ,  "}, 3732698 #search_api.search_api.Paper{:id 3732698, :key "journals/npl/Ster13", :title "Selective Recurrent Neural Network.", :abstract nil, :author (#search_api.search_api.Author{:id 1274552, :first-name nil, :last-name nil, :full-name "Branko Ster"}), :year 2013, :venue "Neural Processing Letters", :ncit 0, :string "Selective Recurrent Neural Network.. ", :doc-id "Selective Recurrent Neural Network. 2013  "}, 2864474 #search_api.search_api.Paper{:id 2864474, :key "journals/jmlr/LarochelleBLL09", :title "Exploring Strategies for Training Deep Neural Networks.", :abstract "Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms.", :author (#search_api.search_api.Author{:id 200637, :first-name nil, :last-name nil, :full-name "Hugo Larochelle"} #search_api.search_api.Author{:id 198739, :first-name nil, :last-name nil, :full-name "Yoshua Bengio"} #search_api.search_api.Author{:id 1186667, :first-name nil, :last-name nil, :full-name "Jérôme Louradour"} #search_api.search_api.Author{:id 43176, :first-name nil, :last-name nil, :full-name "Pascal Lamblin"}), :year 2009, :venue "Journal of Machine Learning Research", :ncit 106, :string "Exploring Strategies for Training Deep Neural Networks.. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms.", :doc-id "Exploring Strategies for Training Deep Neural Networks. 2009  ,  ,  ,  "}, 3461243 #search_api.search_api.Paper{:id 3461243, :key "journals/nn/SzuHJWL12", :title "Capturing significant events with neural networks.", :abstract nil, :author (#search_api.search_api.Author{:id 306570, :first-name nil, :last-name nil, :full-name "Harold Szu"} #search_api.search_api.Author{:id 6784, :first-name nil, :last-name nil, :full-name "Charles Hsu"} #search_api.search_api.Author{:id 1250322, :first-name nil, :last-name nil, :full-name "Jeffrey Jenkins"} #search_api.search_api.Author{:id 14305869, :first-name nil, :last-name nil, :full-name "Jefferson Willey"} #search_api.search_api.Author{:id 455765, :first-name nil, :last-name nil, :full-name "Joseph Landa"}), :year 2012, :venue "Neural Networks", :ncit 9, :string "Capturing significant events with neural networks.. ", :doc-id "Capturing significant events with neural networks. 2012  ,  ,  ,  ,  "}, 3706299 #search_api.search_api.Paper{:id 3706299, :key "journals/nn/ThiviergeMSAG12", :title "A year of neural network research: Special Issue on the 2011 International Joint Conference on Neural Networks.", :abstract nil, :author (#search_api.search_api.Author{:id 1375055, :first-name nil, :last-name nil, :full-name "Jean-Philippe Thivierge"} #search_api.search_api.Author{:id 159203, :first-name nil, :last-name nil, :full-name "Ali Minai"} #search_api.search_api.Author{:id 352646, :first-name nil, :last-name nil, :full-name "Hava T. Siegelmann"} #search_api.search_api.Author{:id 822756, :first-name nil, :last-name nil, :full-name "Cesare Alippi"} #search_api.search_api.Author{:id 507346, :first-name nil, :last-name nil, :full-name "Michael Georgiopoulos"}), :year 2012, :venue "Neural Networks", :ncit 0, :string "A year of neural network research: Special Issue on the 2011 International Joint Conference on Neural Networks.. ", :doc-id "A year of neural network research: Special Issue on the 2011 International Joint Conference on Neural Networks. 2012  ,  ,  ,  ,  "}, 3644 #search_api.search_api.Paper{:id 3644, :key "books/sp/Rojas96", :title "Neural Networks - A Systematic Introduction", :abstract nil, :author (#search_api.search_api.Author{:id 327600, :first-name nil, :last-name nil, :full-name "Raúl Rojas"}), :year 1996, :venue nil, :ncit 1249, :string "Neural Networks - A Systematic Introduction. ", :doc-id "Neural Networks - A Systematic Introduction 1996  "}, 860924 #search_api.search_api.Paper{:id 860924, :key "journals/ec/StanleyM02", :title "Evolving Neural Network through Augmenting Topologies.", :abstract "An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.", :author (#search_api.search_api.Author{:id 151275, :first-name nil, :last-name nil, :full-name "Kenneth O. Stanley"} #search_api.search_api.Author{:id 255328, :first-name nil, :last-name nil, :full-name "Risto Miikkulainen"}), :year 2002, :venue "Evolutionary Computation", :ncit 973, :string "Evolving Neural Network through Augmenting Topologies.. An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.", :doc-id "Evolving Neural Network through Augmenting Topologies. 2002  ,  "}, 746236 #search_api.search_api.Paper{:id 746236, :key "journals/ai/ZhouWT02", :title "Ensembling neural networks: Many could be better than all.", :abstract "Neural network ensemble is a learning paradigm where many neural networks are jointly used to solve a problem. In this paper, the relationship between the ensemble and its component neural networks is analyzed from the context of both regression and classification, which reveals that it may be better to ensemble many instead of all of the neural networks at hand. This result is interesting because at present, most approaches ensemble all the available neural networks for prediction. Then, in order to show that the appropriate neural networks for composing an ensemble can be effectively selected from a set of available neural networks, an approach named GASEN is presented. GASEN trains a number of neural networks at first. Then it assigns random weights to those networks and employs genetic algorithm to evolve the weights so that they can characterize to some extent the fitness of the neural networks in constituting an ensemble. Finally it selects some neural networks based on the evolved weights to make up the ensemble. A large empirical study shows that, compared with some popular ensemble approaches such as Bagging and Boosting, GASEN can generate neural network ensembles with far smaller sizes but stronger generalization ability. Furthermore, in order to understand the working mechanism of GASEN, the bias-variance decomposition of the error is provided in this paper, which shows that the success of GASEN may lie in that it can significantly reduce the bias as well as the variance.", :author (#search_api.search_api.Author{:id 110846, :first-name nil, :last-name nil, :full-name "Zhi-Hua Zhou"} #search_api.search_api.Author{:id 528667, :first-name nil, :last-name nil, :full-name "Jianxin Wu"} #search_api.search_api.Author{:id 1002123, :first-name nil, :last-name nil, :full-name "Wei Tang"}), :year 2002, :venue "Artif. Intell.", :ncit 904, :string "Ensembling neural networks: Many could be better than all.. Neural network ensemble is a learning paradigm where many neural networks are jointly used to solve a problem. In this paper, the relationship between the ensemble and its component neural networks is analyzed from the context of both regression and classification, which reveals that it may be better to ensemble many instead of all of the neural networks at hand. This result is interesting because at present, most approaches ensemble all the available neural networks for prediction. Then, in order to show that the appropriate neural networks for composing an ensemble can be effectively selected from a set of available neural networks, an approach named GASEN is presented. GASEN trains a number of neural networks at first. Then it assigns random weights to those networks and employs genetic algorithm to evolve the weights so that they can characterize to some extent the fitness of the neural networks in constituting an ensemble. Finally it selects some neural networks based on the evolved weights to make up the ensemble. A large empirical study shows that, compared with some popular ensemble approaches such as Bagging and Boosting, GASEN can generate neural network ensembles with far smaller sizes but stronger generalization ability. Furthermore, in order to understand the working mechanism of GASEN, the bias-variance decomposition of the error is provided in this paper, which shows that the success of GASEN may lie in that it can significantly reduce the bias as well as the variance.", :doc-id "Ensembling neural networks: Many could be better than all. 2002  ,  ,  "}, 3074940 #search_api.search_api.Paper{:id 3074940, :key "journals/ml/UwentsMBGS11", :title "Neural networks for relational learning: an experimental comparison.", :abstract nil, :author (#search_api.search_api.Author{:id 519633, :first-name nil, :last-name nil, :full-name "Werner Uwents"} #search_api.search_api.Author{:id 241568, :first-name nil, :last-name nil, :full-name "Gabriele Monfardini"} #search_api.search_api.Author{:id 882341, :first-name nil, :last-name nil, :full-name "Hendrik Blockeel"} #search_api.search_api.Author{:id 249441, :first-name nil, :last-name nil, :full-name "Marco Gori"} #search_api.search_api.Author{:id 1397665, :first-name nil, :last-name nil, :full-name "Franco Scarselli"}), :year 2011, :venue "Machine Learning", :ncit 5, :string "Neural networks for relational learning: an experimental comparison.. ", :doc-id "Neural networks for relational learning: an experimental comparison. 2011  ,  ,  ,  ,  "}, 3077116 #search_api.search_api.Paper{:id 3077116, :key "journals/nn/Anastassiou11", :title "Multivariate sigmoidal neural network approximation.", :abstract nil, :author (#search_api.search_api.Author{:id 37921, :first-name nil, :last-name nil, :full-name "George A. Anastassiou"}), :year 2011, :venue "Neural Networks", :ncit 13, :string "Multivariate sigmoidal neural network approximation.. ", :doc-id "Multivariate sigmoidal neural network approximation. 2011  "}, 3706365 #search_api.search_api.Paper{:id 3706365, :key "journals/nn/BanC13", :title "The learning problem of multi-layer neural networks.", :abstract nil, :author (#search_api.search_api.Author{:id 924142, :first-name nil, :last-name nil, :full-name "Jung-Chao Ban"} #search_api.search_api.Author{:id 1520103, :first-name nil, :last-name nil, :full-name "Chih-Hung Chang"}), :year 2013, :venue "Neural Networks", :ncit 0, :string "The learning problem of multi-layer neural networks.. ", :doc-id "The learning problem of multi-layer neural networks. 2013  ,  "}, 593405 #search_api.search_api.Paper{:id 593405, :key "conf/siggraph/GrzeszczukTH98", :title "NeuroAnimator: Fast Neural Network Emulation and Control of Physics-based Models.", :abstract nil, :author (#search_api.search_api.Author{:id 740845, :first-name nil, :last-name nil, :full-name "Radek Grzeszczuk"} #search_api.search_api.Author{:id 102614, :first-name nil, :last-name nil, :full-name "Demetri Terzopoulos"} #search_api.search_api.Author{:id 996398, :first-name nil, :last-name nil, :full-name "Geoffrey E. Hinton"}), :year 1998, :venue "SIGGRAPH", :ncit 219, :string "NeuroAnimator: Fast Neural Network Emulation and Control of Physics-based Models.. ", :doc-id "NeuroAnimator: Fast Neural Network Emulation and Control of Physics-based Models. 1998  ,  ,  "}, 3005471 #search_api.search_api.Paper{:id 3005471, :key "books/daglib/0091775", :title "An introduction to neural networks.", :abstract nil, :author (#search_api.search_api.Author{:id 925163, :first-name nil, :last-name nil, :full-name "Kevin N. Gurney"}), :year 1997, :venue nil, :ncit 683, :string "An introduction to neural networks.. ", :doc-id "An introduction to neural networks. 1997  "}, 2824255 #search_api.search_api.Paper{:id 2824255, :key "conf/nocs/VainbrandG10", :title "Network-on-Chip Architectures for Neural Networks.", :abstract "Providing highly flexible connectivity is a major architectural challenge for hardware implementation of reconfigurable neural networks. We perform an analytical evaluation and comparison of different configurable interconnect architectures (mesh NoC, tree, shared bus and point-to-point) emulating variants of two neural network topologies (having full and random exponential configurable connectivity). We derive analytical expressions and asymptotic limits for performance (in terms of bandwidth) and cost (in terms of area and power) of the interconnect architectures considering three communication methods (unicast, multicast and broadcast). It is shown that multicast mesh NoC provides the highest performance/cost ratio and consequently it is the most suitable interconnect architecture for configurable neural network implementation. Simulation results successfully validate the analytical models and the asymptotic behavior of the network as a function of its size.", :author (#search_api.search_api.Author{:id 1558551, :first-name nil, :last-name nil, :full-name "Dmitri Vainbrand"} #search_api.search_api.Author{:id 1534434, :first-name nil, :last-name nil, :full-name "Ran Ginosar"}), :year 2010, :venue "NOCS", :ncit 5, :string "Network-on-Chip Architectures for Neural Networks.. Providing highly flexible connectivity is a major architectural challenge for hardware implementation of reconfigurable neural networks. We perform an analytical evaluation and comparison of different configurable interconnect architectures (mesh NoC, tree, shared bus and point-to-point) emulating variants of two neural network topologies (having full and random exponential configurable connectivity). We derive analytical expressions and asymptotic limits for performance (in terms of bandwidth) and cost (in terms of area and power) of the interconnect architectures considering three communication methods (unicast, multicast and broadcast). It is shown that multicast mesh NoC provides the highest performance/cost ratio and consequently it is the most suitable interconnect architecture for configurable neural network implementation. Simulation results successfully validate the analytical models and the asymptotic behavior of the network as a function of its size.", :doc-id "Network-on-Chip Architectures for Neural Networks. 2010  ,  "}, 832703 #search_api.search_api.Paper{:id 832703, :key "journals/csur/JordanB96", :title "Neural Networks.", :abstract nil, :author (#search_api.search_api.Author{:id 221919, :first-name nil, :last-name nil, :full-name "Michael I. Jordan"} #search_api.search_api.Author{:id 1339062, :first-name nil, :last-name nil, :full-name "Christopher M. Bishop"}), :year 1996, :venue "ACM Comput. Surv.", :ncit 0, :string "Neural Networks.. ", :doc-id "Neural Networks. 1996  ,  "}, 3546399 #search_api.search_api.Paper{:id 3546399, :key "journals/pieee/WheelerB10", :title "Designing Neural Networks in Culture.", :abstract nil, :author (#search_api.search_api.Author{:id 1112160, :first-name nil, :last-name nil, :full-name "Bruce C. Wheeler"} #search_api.search_api.Author{:id 14314490, :first-name nil, :last-name nil, :full-name "Gregory J. Brewer"}), :year 2010, :venue "Proceedings of the IEEE", :ncit 0, :string "Designing Neural Networks in Culture.. ", :doc-id "Designing Neural Networks in Culture. 2010  ,  "}, 806847 #search_api.search_api.Paper{:id 806847, :key "journals/computer/JainMM96", :title "Artificial Neural Networks: A Tutorial.", :abstract "Numerous advances have been made in developing intelligent programs, some inspired by biological neural networks. Researchers from many scientific disciplines are designing artificial neural networks (ANNs) to solve a variety of problems in pattern recognition, prediction, optimization, associative memory, and control. Although successful conventional applications can be found in certain well-constrained environments, none is flexible enough to perform well outside its domain. ANNs provide exciting alternatives, and many applications could benefit from using them. This article is for those readers with little or no knowledge of ANNs to help them understand the other articles in this issue of Computer. It discusses the motivation behind the development of ANNs; describes the basic biological neuron and the artificial computation model; outlines network architectures and learning processes; and presents multilayer feed-forward networks, Kohonen's self-organizing maps, Carpenter and Grossberg's Adaptive Resonance Theory models, and the Hopfield network. It concludes with character recognition, a successful ANN application.", :author (#search_api.search_api.Author{:id 324696, :first-name nil, :last-name nil, :full-name "Anil K. Jain"} #search_api.search_api.Author{:id 447337, :first-name nil, :last-name nil, :full-name "Jianchang Mao"} #search_api.search_api.Author{:id 375658, :first-name nil, :last-name nil, :full-name "K. Moidin Mohiuddin"}), :year 1996, :venue "IEEE Computer", :ncit 953, :string "Artificial Neural Networks: A Tutorial.. Numerous advances have been made in developing intelligent programs, some inspired by biological neural networks. Researchers from many scientific disciplines are designing artificial neural networks (ANNs) to solve a variety of problems in pattern recognition, prediction, optimization, associative memory, and control. Although successful conventional applications can be found in certain well-constrained environments, none is flexible enough to perform well outside its domain. ANNs provide exciting alternatives, and many applications could benefit from using them. This article is for those readers with little or no knowledge of ANNs to help them understand the other articles in this issue of Computer. It discusses the motivation behind the development of ANNs; describes the basic biological neuron and the artificial computation model; outlines network architectures and learning processes; and presents multilayer feed-forward networks, Kohonen's self-organizing maps, Carpenter and Grossberg's Adaptive Resonance Theory models, and the Hopfield network. It concludes with character recognition, a successful ANN application.", :doc-id "Artificial Neural Networks: A Tutorial. 1996  ,  ,  "}, 3547103 #search_api.search_api.Paper{:id 3547103, :key "journals/corr/abs-1210-0118", :title "Self-Delimiting Neural Networks", :abstract nil, :author (#search_api.search_api.Author{:id 279083, :first-name nil, :last-name nil, :full-name "Jürgen Schmidhuber"}), :year 2012, :venue "CoRR", :ncit 3, :string "Self-Delimiting Neural Networks. ", :doc-id "Self-Delimiting Neural Networks 2012  "}, 3746783 #search_api.search_api.Paper{:id 3746783, :key "conf/gecco/Miikkulainen13", :title "Evolving neural networks.", :abstract nil, :author (#search_api.search_api.Author{:id 255328, :first-name nil, :last-name nil, :full-name "Risto Miikkulainen"}), :year 2013, :venue "GECCO (Companion)", :ncit 0, :string "Evolving neural networks.. ", :doc-id "Evolving neural networks. 2013  "}}}