#topic_maps.core.TopicMap{:topic-graph #graphs.core.Digraph{:nodes #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "markov processes"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "behaviorism"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational statistics"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "mathematical modeling"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "instance-based learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "first order methods"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "boolean algebra"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "exponential growth"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "probability theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "dilemmas"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control engineering"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "learning methods"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "reality"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "game theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "time series analysis"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "justification"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "history of the internet"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning algorithms"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "time"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "digital electronics"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "statistical models"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "unsupervised learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "thought experiments"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "user interfaces"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "conceptual distinctions"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "exponentials"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "stochastic approximation"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "mathematical analysis stubs"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "types of functions"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "optimal decisions"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "empiricism"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "gradient descent"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "dynamic programming"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "internet search engines"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "problem solving"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "coding theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "unmanned vehicles"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "theory of cryptography"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "hamming distance"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "belief revision"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mobile robot"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimization problem"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "telecommunications engineering"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "biological neural network"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "pattern recognition"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "rationalism"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "automata theory"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "prisoner's dilemma"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dialog system"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "internet terminology"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "feedforward neural network"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "social learning theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "models of computation"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "random walk"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "imitation"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "articles with example c++ code"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "continuous function"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "automation"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems engineering"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational neuroscience"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "continuous mappings"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "real robot"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "fuzzy control system"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "markov models"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "adaptive control"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "statistical approximations"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "concepts in physics"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "formal sciences"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control system"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "ordinary differential equations"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "information retrieval"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "optimization algorithms and methods"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "trial and error"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "symmetric-key cryptography"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "string similarity measures"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "concepts in epistemology"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "telecommunications network"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "metric geometry"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational problems"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "discrete time"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "robot learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "gradient methods"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "sources of knowledge"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "function approximation"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "nervous system"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "autonomy"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "telecommunications infrastructure"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "web search engine"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "a priori"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "subjective experience"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "classification algorithms"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "boolean function"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "cubes"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "robots"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "probability space"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "kantianism"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "decision problem"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "decision theory"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mit press"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computability theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "heuristics"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "supervised learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "autonomous robot"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "statistics stubs"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "history of artificial intelligence"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control theory"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "finite-state machine"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "fuzzy logic"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "error"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "q-learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "signal processing"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "latin philosophical phrases"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "mathematical logic stubs"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "philosophical logic"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "calculus"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "nash equilibrium"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "cognition"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "philosophy of science"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "fixed points (mathematics)"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "intelligent agent"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov chain mixing time"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence stubs"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "term logic"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "network architecture"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic optimization"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "variants of random walks"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "numerical analysis"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "robot control"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "polynomial time"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "latin logical phrases"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic processes"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic control"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "neural networks"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "multimodal interaction"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "curse of dimensionality"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "blocks world"}}, :in-map {#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "markov processes"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic processes"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "behaviorism"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational statistics"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "mathematical modeling"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "instance-based learning"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence stubs"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "first order methods"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "numerical analysis"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "boolean algebra"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "exponential growth"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "mathematical modeling"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "exponentials"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "ordinary differential equations"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "probability theory"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "dilemmas"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control engineering"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems engineering"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control theory"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "learning methods"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "reality"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "game theory"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "problem solving"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "formal sciences"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "decision theory"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "time series analysis"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic processes"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "justification"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "concepts in epistemology"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "history of the internet"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning algorithms"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "time"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "concepts in physics"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "digital electronics"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "statistical models"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "unsupervised learning"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "thought experiments"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "user interfaces"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "conceptual distinctions"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "exponentials"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "stochastic approximation"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "statistical approximations"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic optimization"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "mathematical analysis stubs"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "types of functions"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "optimal decisions"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "decision theory"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "empiricism"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "justification"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "gradient descent"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "first order methods"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "optimization algorithms and methods"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "gradient methods"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "dynamic programming"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "optimal decisions"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems engineering"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "optimization algorithms and methods"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "internet search engines"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning algorithms"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "belief revision"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "markov models"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "problem solving"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "coding theory"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "unmanned vehicles"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "robots"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "theory of cryptography"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "hamming distance"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "coding theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "articles with example c++ code"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "string similarity measures"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "metric geometry"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "cubes"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "belief revision"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mobile robot"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "robots"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimization problem"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational problems"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "telecommunications engineering"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "biological neural network"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "nervous system"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "cognition"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "neural networks"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "pattern recognition"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "formal sciences"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "rationalism"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "justification"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "a priori"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "automata theory"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "prisoner's dilemma"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "dilemmas"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "game theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "thought experiments"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dialog system"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "user interfaces"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems engineering"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "multimodal interaction"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "internet terminology"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "feedforward neural network"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "neural networks"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "social learning theory"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "models of computation"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "random walk"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "concepts in physics"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "variants of random walks"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic processes"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "imitation"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "behaviorism"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "social learning theory"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "articles with example c++ code"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "continuous function"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "types of functions"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "continuous mappings"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "calculus"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "automation"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control engineering"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems engineering"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems theory"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational statistics"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational neuroscience"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "continuous mappings"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "real robot"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "fuzzy control system"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control engineering"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "fuzzy logic"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "markov models"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "markov processes"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational statistics"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational neuroscience"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "classification algorithms"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "neural networks"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "adaptive control"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control theory"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems theory"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "philosophy of science"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "statistical approximations"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "concepts in physics"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "formal sciences"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control system"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control engineering"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "automation"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems engineering"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control theory"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "ordinary differential equations"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "information retrieval"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "optimization algorithms and methods"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "trial and error"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "learning methods"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "problem solving"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "heuristics"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "error"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "philosophy of science"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "symmetric-key cryptography"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "string similarity measures"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "information retrieval"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "metric geometry"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "concepts in epistemology"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "telecommunications network"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "telecommunications engineering"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "telecommunications infrastructure"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "network architecture"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "reality"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "justification"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "conceptual distinctions"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "empiricism"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "rationalism"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "concepts in epistemology"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "sources of knowledge"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "a priori"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "subjective experience"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "kantianism"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "latin philosophical phrases"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "philosophical logic"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "term logic"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "latin logical phrases"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "metric geometry"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational problems"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "discrete time"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "time series analysis"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "time"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "signal processing"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "robot learning"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "robot control"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "gradient methods"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "first order methods"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "optimization algorithms and methods"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "sources of knowledge"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "concepts in epistemology"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "function approximation"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "statistical models"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "mathematical analysis stubs"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "statistical approximations"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "statistics stubs"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "nervous system"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "autonomy"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "telecommunications infrastructure"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "web search engine"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "history of the internet"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "internet search engines"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "internet terminology"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "information retrieval"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "a priori"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "justification"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "sources of knowledge"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "subjective experience"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "kantianism"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "subjective experience"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "classification algorithms"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "boolean function"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "boolean algebra"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "theory of cryptography"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "symmetric-key cryptography"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "mathematical logic stubs"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "cubes"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "robots"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "probability space"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "probability theory"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "kantianism"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "decision problem"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational problems"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computability theory"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "decision theory"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "probability theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "problem solving"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "formal sciences"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control theory"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mit press"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computability theory"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "heuristics"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "problem solving"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "cognition"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "supervised learning"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "autonomous robot"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "unmanned vehicles"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "autonomy"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "robots"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "statistics stubs"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "history of artificial intelligence"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control theory"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "formal sciences"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "finite-state machine"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "digital electronics"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "automata theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "models of computation"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "fuzzy logic"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "error"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "q-learning"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning algorithms"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence stubs"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "signal processing"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "telecommunications engineering"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "latin philosophical phrases"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "mathematical logic stubs"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "philosophical logic"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "calculus"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "nash equilibrium"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "game theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "decision theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "fixed points (mathematics)"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "cognition"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "philosophy of science"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "fixed points (mathematics)"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "intelligent agent"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov chain mixing time"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "markov processes"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence stubs"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "term logic"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "markov processes"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "optimal decisions"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "dynamic programming"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic control"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "network architecture"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic optimization"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "optimization algorithms and methods"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "variants of random walks"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic processes"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "numerical analysis"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "robot control"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "polynomial time"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "latin logical phrases"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "latin philosophical phrases"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic processes"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "probability theory"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic control"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic processes"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "neural networks"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "mathematical modeling"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational neuroscience"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "multimodal interaction"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "curse of dimensionality"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "dynamic programming"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "numerical analysis"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "blocks world"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "history of artificial intelligence"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence stubs"}}}, :out-map {#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "markov processes"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "markov models"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov chain mixing time"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "behaviorism"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "imitation"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational statistics"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "mathematical modeling"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "exponential growth"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "neural networks"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "instance-based learning"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "first order methods"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "gradient descent"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "gradient methods"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "boolean algebra"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "boolean function"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "exponential growth"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "probability theory"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "probability space"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "decision theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic processes"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "dilemmas"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "prisoner's dilemma"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control engineering"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "automation"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "fuzzy control system"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control system"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "learning methods"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "trial and error"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "reality"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "game theory"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "prisoner's dilemma"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "nash equilibrium"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "time series analysis"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "discrete time"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "justification"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "empiricism"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "rationalism"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "a priori"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "history of the internet"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "web search engine"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning algorithms"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "q-learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "time"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "discrete time"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "digital electronics"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "finite-state machine"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "statistical models"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "function approximation"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "unsupervised learning"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "thought experiments"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "prisoner's dilemma"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "user interfaces"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dialog system"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "conceptual distinctions"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "exponentials"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "exponential growth"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "stochastic approximation"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "mathematical analysis stubs"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "function approximation"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "types of functions"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "continuous function"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "optimal decisions"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "dynamic programming"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "empiricism"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "gradient descent"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "dynamic programming"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "curse of dimensionality"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "internet search engines"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "web search engine"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "gradient descent"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "supervised learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "q-learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "problem solving"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "game theory"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "trial and error"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "decision theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "heuristics"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "coding theory"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "hamming distance"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "unmanned vehicles"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "autonomous robot"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "theory of cryptography"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "boolean function"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "hamming distance"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "belief revision"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mobile robot"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimization problem"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "telecommunications engineering"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "telecommunications network"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "signal processing"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "biological neural network"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "pattern recognition"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "rationalism"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "automata theory"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "finite-state machine"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "prisoner's dilemma"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dialog system"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "internet terminology"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "web search engine"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "feedforward neural network"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "social learning theory"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "imitation"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "models of computation"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "finite-state machine"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "random walk"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "imitation"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "articles with example c++ code"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "hamming distance"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "continuous function"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "automation"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control system"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems engineering"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control engineering"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "dynamic programming"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dialog system"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control system"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "instance-based learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning algorithms"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "unsupervised learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "pattern recognition"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "markov models"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "robot learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "classification algorithms"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "supervised learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "neural networks"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "curse of dimensionality"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational neuroscience"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "neural networks"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "continuous mappings"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "continuous function"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "real robot"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "fuzzy control system"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "markov models"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "adaptive control"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems theory"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control engineering"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems engineering"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control system"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control theory"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "statistical approximations"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "stochastic approximation"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "function approximation"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "concepts in physics"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "time"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "random walk"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "formal sciences"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "game theory"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "pattern recognition"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "decision theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control theory"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control system"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "ordinary differential equations"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "exponential growth"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "information retrieval"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "string similarity measures"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "web search engine"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "optimization algorithms and methods"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "gradient descent"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "dynamic programming"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "gradient methods"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic optimization"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "trial and error"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "symmetric-key cryptography"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "boolean function"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "string similarity measures"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "hamming distance"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "concepts in epistemology"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "justification"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "sources of knowledge"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "telecommunications network"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "metric geometry"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "hamming distance"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "string similarity measures"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational problems"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimization problem"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "decision problem"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "discrete time"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "robot learning"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "gradient methods"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "gradient descent"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "sources of knowledge"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "a priori"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "function approximation"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "nervous system"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "biological neural network"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "autonomy"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "autonomous robot"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "telecommunications infrastructure"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "telecommunications network"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "web search engine"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "a priori"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "rationalism"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "subjective experience"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "a priori"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "classification algorithms"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "boolean function"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "cubes"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "hamming distance"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "robots"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "unmanned vehicles"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mobile robot"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "autonomous robot"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "probability space"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "kantianism"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "a priori"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "decision problem"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "decision theory"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "game theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "optimal decisions"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "nash equilibrium"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mit press"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computability theory"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "decision problem"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "heuristics"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "trial and error"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "supervised learning"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "autonomous robot"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "statistics stubs"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "function approximation"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "history of artificial intelligence"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "blocks world"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control theory"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control engineering"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "adaptive control"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control system"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "decision theory"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic control"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "finite-state machine"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "fuzzy logic"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "fuzzy control system"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "error"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "trial and error"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "q-learning"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "signal processing"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "discrete time"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "latin philosophical phrases"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "latin logical phrases"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "mathematical logic stubs"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "boolean function"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "philosophical logic"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "calculus"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "continuous function"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "nash equilibrium"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "cognition"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "biological neural network"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "heuristics"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "philosophy of science"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "systems theory"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "trial and error"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "fixed points (mathematics)"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "nash equilibrium"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "intelligent agent"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov chain mixing time"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence stubs"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "instance-based learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "q-learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "blocks world"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "term logic"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "problem solving"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "robots"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "history of artificial intelligence"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "fuzzy logic"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "intelligent agent"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "artificial intelligence stubs"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "network architecture"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "telecommunications network"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic optimization"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "stochastic approximation"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "variants of random walks"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "random walk"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "numerical analysis"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "first order methods"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "curse of dimensionality"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "robot control"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "robot learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "polynomial time"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "latin logical phrases"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic processes"} #{#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "markov processes"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "time series analysis"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "random walk"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "variants of random walks"} #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic control"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "stochastic control"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "neural networks"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "biological neural network"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "feedforward neural network"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "multimodal interaction"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dialog system"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "curse of dimensionality"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "blocks world"} #{}}}, :topic-docs #graphs.core.Digraph{:nodes #{"A menu of designs of reinforcement learning over time 1990 Werbos" "Coordination of multiple behaviors acquired by a vision-based reinforcement learning 1994 Asada, Uchibe, Noda, Tawaratsumida, Hosoda" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "instance-based learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "exponential growth"} "Relational Reinforcement Learning 1998 Dzeroski, Raedt, Blockeel" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dynamical system"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mathematical optimization"} "Frustrative nonreward in partial reinforcement and discrimination learning: Some recent history and a theoretical extension 1962 Amsel" "Efficient Reinforcement Learning through Symbiotic Evolution 1996 Moriarty, Miikkulainen" "Average Reward Reinforcement Learning: Foundations, Algorithms, and Empirical Results 1996 Mahadevan" "Reinforcement Learning with Hierarchies of Machines 1997 Parr, Russell" "Reinforcement Learning with Perceptual Aliasing: The Perceptual Distinctions Approach 1992 Chrisman" "Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms 1998 Singh, Jaakkola" "Reinforcement learning with hierarchies of machines 1998 Parr, Russell" "Algorithms for Inverse Reinforcement Learning 2000 Ng, Russell" "Experiments with Reinforcement Learning in Problems with Continuous State and Action Spaces 1998 Santamaría, Sutton, Ram" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimal control"} "A stochastic reinforcement learning algorithm for learning real-valued functions 1990 Gullapalli" "Recent Advances in Hierarchical Reinforcement Learning 2003 Barto, Mahadevan" "Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System 2002 Singh, Litman, Kearns, Walker" "Reinforcement learning architectures for animats 1991 Sutton" "Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning 1994 ASADA, NODA, TAWARATSUMIDA, HOSODA, Franklin, Mitchell, Thrun" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "unsupervised learning"} "Reinforcement Learning: An Introduction 1998 Sutton, Barto" "Reinforcement learning: An introduction 1998 Sutton, Barto" "A Reinforcement Learning Method for Maximizing Undiscounted Rewards 1993 Schwartz" "Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time 1999 Moore, Atkeson" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "stochastic approximation"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "computational geometry"} "A Distributed Reinforcement Learning Scheme for Network Routing 1993 Boyan, Littman" "Reinforcement Learning in Continuous Time and Space 2000 Doya" "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density 2001 Mcgovern, Barto" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "gradient descent"} "Markov Games as a Framework for Multi-Agent Reinforcement Learning 1994 Littman" "Consistency of HDP applied to a simple reinforcement learning problem 1990 Werbos" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "cognition"} "A Complexity Analysis of Cooperative Mechanisms in Reinforcement Learning 1991 Whitehead" "Reinforcement Learning Methods for Continuous-Time Markov Decision Problems 1994 Bradtke, Duff" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} "Reinforcement Learning with a Hierarchy of Abstract Models 1992 Singh" "Reinforcement structure/parameter learning for neural-network-based fuzzy logic control systems 1994 Lin, Lee" "Learning from Demonstration 1996 Schaal" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "hamming distance"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mobile robot"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimization problem"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "biological neural network"} "Policy Gradient Methods for Reinforcement Learning with Function Approximation 1999 Sutton, Mcallester, Singh, Mansour" "Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm 1998 Hu, Wellman" "Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning 1996 Asada, Noda, Tawaratsumida, Hosoda" "A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies 2000 Levin, Pieraccini, Eckert" "Reinforcement Learning in the Multi-Robot Domain 1997 Mataric" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "pattern recognition"} "Efficient Exploration In Reinforcement Learning 1992 Thrun" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "prisoner's dilemma"} "The MAXQ Method for Hierarchical Reinforcement Learning 1998 Dietterich" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dialog system"} "Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons 1991 Chapman, Kaelbling" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "feedforward neural network"} "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning 1999 Sutton, Precup, Singh" "Vision-Based Reinforcement Learning for Purposive Behavior Acquisition 1995 Asada, Noda, Tawaratsumida, Hosoda" "Prioritized Sweeping: Reinforcement Learning With Less Data and Less Time 1993 Moore, Atkeson" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "random walk"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "imitation"} "Using reinforcement learning to spider the Web efficiently 1999 Rennie, McCallum" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "routing"} "Gradient Descent for General Reinforcement Learning 1998 III, Moore" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "continuous function"} "Tree-Based Batch Mode Reinforcement Learning 2005 Ernst, Geurts, Wehenkel" "Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley, Dominic, Das, Anderson" "Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems 1997 Bertsekas, Singh" "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding 1995 Sutton" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "real robot"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control theory"} "Active Perception and Reinforcement Learning 1990 Whitehead, Ballard" "The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems 1998 Claus, Boutilier" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "fuzzy logic"} "Associative search network: A reinforcement learning associative memory 1981 Barto, Sutton, Brouwer" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "genetic algorithm"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "fuzzy control system"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "education"} "Reinforcement Learning with Soft State Aggregation 1995 Jordan, Singh, Jaakkola" "A Graph-Based Evolutionary Algorithm: Genetic Network Programming (GNP) and Its Extension Using Reinforcement Learning 2007 Mabu, Hirasawa, Hu" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "inductive logic programming"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "adaptive control"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "human–computer interaction"} "Feudal Reinforcement Learning 1992 Dayan, Hinton" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control system"} "Generalization in Reinforcement Learning: Safely Approximating the Value Function 1994 Boyan, Moore" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "trial and error"} "Residual Algorithms: Reinforcement Learning with Function Approximation 1995 III" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "telecommunications network"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "discrete time"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "robot learning"} "Introduction to Reinforcement Learning 1998 Sutton, Barto" "Apprenticeship learning via inverse reinforcement learning 2004 Abbeel, Ng" "Effective Reinforcement Learning for Mobile Robots 2002 Smart, Kaelbling" "Reinforcement, expectancy, and learning 1972 Bolles" "Programming Robots Using Reinforcement Learning and Teaching 1991 Lin" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "function approximation"} "Near-Optimal Reinforcement Learning in Polynominal Time 1998 Kearns, Singh" "Moore Reinforcement Learning: A Survey 1996 Kaelbling, Littman" "Reinforcement learning is direct adaptive optimal control 1992 Sutton, Barto, Williams" "Automatic Programming of Behavior-Based Robots Using Reinforcement Learning 1991 Mahadevan, Connell" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "web search engine"} "The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-spaces 1995 Moore, Atkeson" "Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents 1993 Tan" "Improving Elevator Performance Using Reinforcement Learning 1995 Crites, Barto" "Reinforcement Learning for Robots Using Neural Networks 1993 Lin" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "graph coloring"} "Multiagent Reinforcement Learning in the Iterated Prisoner's Dilemma 1995 Sandholm, Crites" "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems 1995 Jordan, Singh, Jaakkola" "On the Computational Economics of Reinforcement Learning 1990 Barto" "Genetic reinforcement learning through symbiotic evolution for fuzzy controller design 2000 Juang, Lin, Lin" "Reinforcement Learning: A Survey 1996 Kaelbling, Littman, Moore" "Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley" "Finding Structure in Reinforcement Learning 1995 Thrun" "Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching 1992 Lin" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "quantum field theory"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "boolean function"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "computational economics"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "probability space"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "decision problem"} "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning 2002 Brafman, Tennenholtz" "A Reinforcement Learning Approach to Job-Shop Scheduling 1995 Zhang, Dietterich" "Instance-Based Utile Distinctions for Reinforcement Learning with Hidden State 1995 Mccallum" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mit press"} "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition 2000 Dietterich" "Relational Reinforcement Learning 2001 Driessens, Raedt" "The Parti-Game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-Spaces 1993 Moore" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "supervised learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "autonomous robot"} "Behavior coordination for a mobile robot using modular reinforcement learning 1996 Uchibe, Asada, Hosoda" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "finite-state machine"} "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning 2001 Brafman, Tennenholtz" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "q-learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimal decision"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "travelling salesman problem"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "nonlinear system"} "The neural basis of human error processing: Reinforcement learning, dopamine, and the error-related negativity 2002 Holroyd, Coles" "Policy Gradient Reinforcement Learning for Fast Quadrupedal Locomotion 2004 Kohl, Stone" "Vicarious reinforcement and imitative learning 1963 Bandura, Ross, Ross" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "evolutionary algorithm"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "nash equilibrium"} "Ants and Reinforcement Learning: A Case Study in Routing in Dynamic Networks 1997 Subramanian, Druschel, Chen" "Temporal credit assignment in reinforcement learning 1984 Sutton" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "intelligent agent"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov chain mixing time"} "Reinforcement Learning with Replacing Eligibility Traces 1996 Singh, Sutton" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "machine learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"} "A sensor-based navigation for a mobile robot using fuzzy logic and reinforcement learning 1995 Beom, Cho" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "search algorithm"} "Generalization in Reinforcement Learning: Safely Approximating theValue Function 1995 Boyan, Moore" "Reinforcement Learning with Selective Perception and Hidden State 1995 Mccallum" "Generalization in Reinforcement Learning: Safely Approximating the Value Function 1995 " "Gradient Descent fo r General Reinf orcement Learning 0 Baird, Moore" "Reinforcement Learning Applied to Linear Quadratic Regulation 1992 Bradtke" "Packet Routing in Dynamically Changing Networks: A Reinforcement Learning Approach 1993 Boyan, Littman" "Ant-Q: A Reinforcement Learning Approach to the Traveling Salesman Problem 1995 Gambardella, Dorigo" #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "polynomial time"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dynamic programming"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "necessity and sufficiency"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "curse of dimensionality"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "blocks world"} "Issues in Using Function Approximation for Reinforcement Learning 1993 Thrun, Schwartz"}, :in-map {"A menu of designs of reinforcement learning over time 1990 Werbos" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Coordination of multiple behaviors acquired by a vision-based reinforcement learning 1994 Asada, Uchibe, Noda, Tawaratsumida, Hosoda" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "instance-based learning"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "exponential growth"} #{}, "Relational Reinforcement Learning 1998 Dzeroski, Raedt, Blockeel" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "inductive logic programming"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "blocks world"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dynamical system"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mathematical optimization"} #{}, "Frustrative nonreward in partial reinforcement and discrimination learning: Some recent history and a theoretical extension 1962 Amsel" #{}, "Efficient Reinforcement Learning through Symbiotic Evolution 1996 Moriarty, Miikkulainen" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"}}, "Average Reward Reinforcement Learning: Foundations, Algorithms, and Empirical Results 1996 Mahadevan" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimal control"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dynamic programming"}}, "Reinforcement Learning with Hierarchies of Machines 1997 Parr, Russell" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "cognition"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "search algorithm"}}, "Reinforcement Learning with Perceptual Aliasing: The Perceptual Distinctions Approach 1992 Chrisman" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms 1998 Singh, Jaakkola" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Reinforcement learning with hierarchies of machines 1998 Parr, Russell" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Algorithms for Inverse Reinforcement Learning 2000 Ng, Russell" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Experiments with Reinforcement Learning in Problems with Continuous State and Action Spaces 1998 Santamaría, Sutton, Ram" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimal control"} #{}, "A stochastic reinforcement learning algorithm for learning real-valued functions 1990 Gullapalli" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Recent Advances in Hierarchical Reinforcement Learning 2003 Barto, Mahadevan" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "exponential growth"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "curse of dimensionality"}}, "Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System 2002 Singh, Litman, Kearns, Walker" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Reinforcement learning architectures for animats 1991 Sutton" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning 1994 ASADA, NODA, TAWARATSUMIDA, HOSODA, Franklin, Mitchell, Thrun" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "real robot"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "probability space"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "unsupervised learning"} #{}, "Reinforcement Learning: An Introduction 1998 Sutton, Barto" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Reinforcement learning: An introduction 1998 Sutton, Barto" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "A Reinforcement Learning Method for Maximizing Undiscounted Rewards 1993 Schwartz" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time 1999 Moore, Atkeson" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "stochastic approximation"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "computational geometry"} #{}, "A Distributed Reinforcement Learning Scheme for Network Routing 1993 Boyan, Littman" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "routing"}}, "Reinforcement Learning in Continuous Time and Space 2000 Doya" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dynamical system"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"}}, "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density 2001 Mcgovern, Barto" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "gradient descent"} #{}, "Markov Games as a Framework for Multi-Agent Reinforcement Learning 1994 Littman" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "finite-state machine"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"}}, "Consistency of HDP applied to a simple reinforcement learning problem 1990 Werbos" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "cognition"} #{}, "A Complexity Analysis of Cooperative Mechanisms in Reinforcement Learning 1991 Whitehead" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Reinforcement Learning Methods for Continuous-Time Markov Decision Problems 1994 Bradtke, Duff" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "stochastic approximation"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "continuous function"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "discrete time"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dynamic programming"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #{}, "Reinforcement Learning with a Hierarchy of Abstract Models 1992 Singh" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "trial and error"}}, "Reinforcement structure/parameter learning for neural-network-based fuzzy logic control systems 1994 Lin, Lee" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control theory"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "fuzzy logic"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control system"}}, "Learning from Demonstration 1996 Schaal" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "hamming distance"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mobile robot"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimization problem"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "biological neural network"} #{}, "Policy Gradient Methods for Reinforcement Learning with Function Approximation 1999 Sutton, Mcallester, Singh, Mansour" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "function approximation"}}, "Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm 1998 Hu, Wellman" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "nash equilibrium"}}, "Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning 1996 Asada, Noda, Tawaratsumida, Hosoda" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "real robot"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "probability space"}}, "A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies 2000 Levin, Pieraccini, Eckert" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mathematical optimization"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dialog system"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "human–computer interaction"}}, "Reinforcement Learning in the Multi-Robot Domain 1997 Mataric" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "pattern recognition"} #{}, "Efficient Exploration In Reinforcement Learning 1992 Thrun" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "random walk"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "prisoner's dilemma"} #{}, "The MAXQ Method for Hierarchical Reinforcement Learning 1998 Dietterich" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dialog system"} #{}, "Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons 1991 Chapman, Kaelbling" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "unsupervised learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "intelligent agent"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "feedforward neural network"} #{}, "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning 1999 Sutton, Precup, Singh" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "quantum field theory"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"}}, "Vision-Based Reinforcement Learning for Purposive Behavior Acquisition 1995 Asada, Noda, Tawaratsumida, Hosoda" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "real robot"}}, "Prioritized Sweeping: Reinforcement Learning With Less Data and Less Time 1993 Moore, Atkeson" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "random walk"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "imitation"} #{}, "Using reinforcement learning to spider the Web efficiently 1999 Rennie, McCallum" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "web search engine"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "machine learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "routing"} #{}, "Gradient Descent for General Reinforcement Learning 1998 III, Moore" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "gradient descent"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "continuous function"} #{}, "Tree-Based Batch Mode Reinforcement Learning 2005 Ernst, Geurts, Wehenkel" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley, Dominic, Das, Anderson" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "genetic algorithm"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "supervised learning"}}, "Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems 1997 Bertsekas, Singh" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mathematical optimization"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding 1995 Sutton" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "function approximation"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "real robot"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control theory"} #{}, "Active Perception and Reinforcement Learning 1990 Whitehead, Ballard" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "adaptive control"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimal decision"}}, "The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems 1998 Claus, Boutilier" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "cognition"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "fuzzy logic"} #{}, "Associative search network: A reinforcement learning associative memory 1981 Barto, Sutton, Brouwer" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "pattern recognition"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "genetic algorithm"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "fuzzy control system"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "education"} #{}, "Reinforcement Learning with Soft State Aggregation 1995 Jordan, Singh, Jaakkola" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "A Graph-Based Evolutionary Algorithm: Genetic Network Programming (GNP) and Its Extension Using Reinforcement Learning 2007 Mabu, Hirasawa, Hu" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "evolutionary algorithm"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "inductive logic programming"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "adaptive control"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "human–computer interaction"} #{}, "Feudal Reinforcement Learning 1992 Dayan, Hinton" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control system"} #{}, "Generalization in Reinforcement Learning: Safely Approximating the Value Function 1994 Boyan, Moore" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "trial and error"} #{}, "Residual Algorithms: Reinforcement Learning with Function Approximation 1995 III" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "instance-based learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimization problem"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "feedforward neural network"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "telecommunications network"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "discrete time"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "robot learning"} #{}, "Introduction to Reinforcement Learning 1998 Sutton, Barto" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Apprenticeship learning via inverse reinforcement learning 2004 Abbeel, Ng" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"}}, "Effective Reinforcement Learning for Mobile Robots 2002 Smart, Kaelbling" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mobile robot"}}, "Reinforcement, expectancy, and learning 1972 Bolles" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "necessity and sufficiency"}}, "Programming Robots Using Reinforcement Learning and Teaching 1991 Lin" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "function approximation"} #{}, "Near-Optimal Reinforcement Learning in Polynominal Time 1998 Kearns, Singh" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov chain mixing time"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"}}, "Moore Reinforcement Learning: A Survey 1996 Kaelbling, Littman" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Reinforcement learning is direct adaptive optimal control 1992 Sutton, Barto, Williams" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimal control"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "stochastic approximation"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "nonlinear system"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dynamic programming"}}, "Automatic Programming of Behavior-Based Robots Using Reinforcement Learning 1991 Mahadevan, Connell" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "hamming distance"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "trial and error"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "q-learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "web search engine"} #{}, "The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-spaces 1995 Moore, Atkeson" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "computational geometry"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "curse of dimensionality"}}, "Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents 1993 Tan" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "education"}}, "Improving Elevator Performance Using Reinforcement Learning 1995 Crites, Barto" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dynamical system"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "continuous function"}}, "Reinforcement Learning for Robots Using Neural Networks 1993 Lin" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "graph coloring"} #{}, "Multiagent Reinforcement Learning in the Iterated Prisoner's Dilemma 1995 Sandholm, Crites" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "prisoner's dilemma"}}, "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems 1995 Jordan, Singh, Jaakkola" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "machine learning"}}, "On the Computational Economics of Reinforcement Learning 1990 Barto" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "adaptive control"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "computational economics"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "decision problem"}}, "Genetic reinforcement learning through symbiotic evolution for fuzzy controller design 2000 Juang, Lin, Lin" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "fuzzy control system"}}, "Reinforcement Learning: A Survey 1996 Kaelbling, Littman, Moore" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "machine learning"}}, "Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "genetic algorithm"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "supervised learning"}}, "Finding Structure in Reinforcement Learning 1995 Thrun" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching 1992 Lin" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "quantum field theory"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "boolean function"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "computational economics"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "probability space"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "decision problem"} #{}, "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning 2002 Brafman, Tennenholtz" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "A Reinforcement Learning Approach to Job-Shop Scheduling 1995 Zhang, Dietterich" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"}}, "Instance-Based Utile Distinctions for Reinforcement Learning with Hidden State 1995 Mccallum" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mit press"} #{}, "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition 2000 Dietterich" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"}}, "Relational Reinforcement Learning 2001 Driessens, Raedt" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "inductive logic programming"}}, "The Parti-Game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-Spaces 1993 Moore" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "computational geometry"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "probability space"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "curse of dimensionality"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "supervised learning"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "autonomous robot"} #{}, "Behavior coordination for a mobile robot using modular reinforcement learning 1996 Uchibe, Asada, Hosoda" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "robot learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "autonomous robot"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "finite-state machine"} #{}, "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning 2001 Brafman, Tennenholtz" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "polynomial time"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "q-learning"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimal decision"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "travelling salesman problem"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "nonlinear system"} #{}, "The neural basis of human error processing: Reinforcement learning, dopamine, and the error-related negativity 2002 Holroyd, Coles" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "biological neural network"}}, "Policy Gradient Reinforcement Learning for Fast Quadrupedal Locomotion 2004 Kohl, Stone" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "machine learning"}}, "Vicarious reinforcement and imitative learning 1963 Bandura, Ross, Ross" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "imitation"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "evolutionary algorithm"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "nash equilibrium"} #{}, "Ants and Reinforcement Learning: A Case Study in Routing in Dynamic Networks 1997 Subramanian, Druschel, Chen" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "telecommunications network"}}, "Temporal credit assignment in reinforcement learning 1984 Sutton" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "intelligent agent"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov chain mixing time"} #{}, "Reinforcement Learning with Replacing Eligibility Traces 1996 Singh, Sutton" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "machine learning"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"} #{}, "A sensor-based navigation for a mobile robot using fuzzy logic and reinforcement learning 1995 Beom, Cho" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "fuzzy logic"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "boolean function"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "autonomous robot"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "search algorithm"} #{}, "Generalization in Reinforcement Learning: Safely Approximating theValue Function 1995 Boyan, Moore" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Reinforcement Learning with Selective Perception and Hidden State 1995 Mccallum" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Generalization in Reinforcement Learning: Safely Approximating the Value Function 1995 " #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mit press"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dynamic programming"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "curse of dimensionality"}}, "Gradient Descent fo r General Reinf orcement Learning 0 Baird, Moore" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "gradient descent"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "graph coloring"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "machine learning"}}, "Reinforcement Learning Applied to Linear Quadratic Regulation 1992 Bradtke" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}}, "Packet Routing in Dynamically Changing Networks: A Reinforcement Learning Approach 1993 Boyan, Littman" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "routing"}}, "Ant-Q: A Reinforcement Learning Approach to the Traveling Salesman Problem 1995 Gambardella, Dorigo" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mathematical optimization"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "travelling salesman problem"}}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "polynomial time"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dynamic programming"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "necessity and sufficiency"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "curse of dimensionality"} #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "blocks world"} #{}, "Issues in Using Function Approximation for Reinforcement Learning 1993 Thrun, Schwartz" #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"} #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "function approximation"}}}, :out-map {"A menu of designs of reinforcement learning over time 1990 Werbos" #{}, "Coordination of multiple behaviors acquired by a vision-based reinforcement learning 1994 Asada, Uchibe, Noda, Tawaratsumida, Hosoda" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "instance-based learning"} #{"Residual Algorithms: Reinforcement Learning with Function Approximation 1995 III"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "exponential growth"} #{"Recent Advances in Hierarchical Reinforcement Learning 2003 Barto, Mahadevan"}, "Relational Reinforcement Learning 1998 Dzeroski, Raedt, Blockeel" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dynamical system"} #{"Reinforcement Learning in Continuous Time and Space 2000 Doya" "Improving Elevator Performance Using Reinforcement Learning 1995 Crites, Barto"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mathematical optimization"} #{"A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies 2000 Levin, Pieraccini, Eckert" "Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems 1997 Bertsekas, Singh" "Ant-Q: A Reinforcement Learning Approach to the Traveling Salesman Problem 1995 Gambardella, Dorigo"}, "Frustrative nonreward in partial reinforcement and discrimination learning: Some recent history and a theoretical extension 1962 Amsel" #{}, "Efficient Reinforcement Learning through Symbiotic Evolution 1996 Moriarty, Miikkulainen" #{}, "Average Reward Reinforcement Learning: Foundations, Algorithms, and Empirical Results 1996 Mahadevan" #{}, "Reinforcement Learning with Hierarchies of Machines 1997 Parr, Russell" #{}, "Reinforcement Learning with Perceptual Aliasing: The Perceptual Distinctions Approach 1992 Chrisman" #{}, "Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms 1998 Singh, Jaakkola" #{}, "Reinforcement learning with hierarchies of machines 1998 Parr, Russell" #{}, "Algorithms for Inverse Reinforcement Learning 2000 Ng, Russell" #{}, "Experiments with Reinforcement Learning in Problems with Continuous State and Action Spaces 1998 Santamaría, Sutton, Ram" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimal control"} #{"Average Reward Reinforcement Learning: Foundations, Algorithms, and Empirical Results 1996 Mahadevan" "Reinforcement learning is direct adaptive optimal control 1992 Sutton, Barto, Williams"}, "A stochastic reinforcement learning algorithm for learning real-valued functions 1990 Gullapalli" #{}, "Recent Advances in Hierarchical Reinforcement Learning 2003 Barto, Mahadevan" #{}, "Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System 2002 Singh, Litman, Kearns, Walker" #{}, "Reinforcement learning architectures for animats 1991 Sutton" #{}, "Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning 1994 ASADA, NODA, TAWARATSUMIDA, HOSODA, Franklin, Mitchell, Thrun" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "unsupervised learning"} #{"Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons 1991 Chapman, Kaelbling"}, "Reinforcement Learning: An Introduction 1998 Sutton, Barto" #{}, "Reinforcement learning: An introduction 1998 Sutton, Barto" #{}, "A Reinforcement Learning Method for Maximizing Undiscounted Rewards 1993 Schwartz" #{}, "Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time 1999 Moore, Atkeson" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "stochastic approximation"} #{"Reinforcement Learning Methods for Continuous-Time Markov Decision Problems 1994 Bradtke, Duff" "Reinforcement learning is direct adaptive optimal control 1992 Sutton, Barto, Williams"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "computational geometry"} #{"The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-spaces 1995 Moore, Atkeson" "The Parti-Game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-Spaces 1993 Moore"}, "A Distributed Reinforcement Learning Scheme for Network Routing 1993 Boyan, Littman" #{}, "Reinforcement Learning in Continuous Time and Space 2000 Doya" #{}, "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density 2001 Mcgovern, Barto" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "gradient descent"} #{"Gradient Descent for General Reinforcement Learning 1998 III, Moore" "Gradient Descent fo r General Reinf orcement Learning 0 Baird, Moore"}, "Markov Games as a Framework for Multi-Agent Reinforcement Learning 1994 Littman" #{}, "Consistency of HDP applied to a simple reinforcement learning problem 1990 Werbos" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "cognition"} #{"Reinforcement Learning with Hierarchies of Machines 1997 Parr, Russell" "The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems 1998 Claus, Boutilier"}, "A Complexity Analysis of Cooperative Mechanisms in Reinforcement Learning 1991 Whitehead" #{}, "Reinforcement Learning Methods for Continuous-Time Markov Decision Problems 1994 Bradtke, Duff" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"} #{"A menu of designs of reinforcement learning over time 1990 Werbos" "Coordination of multiple behaviors acquired by a vision-based reinforcement learning 1994 Asada, Uchibe, Noda, Tawaratsumida, Hosoda" "Relational Reinforcement Learning 1998 Dzeroski, Raedt, Blockeel" "Efficient Reinforcement Learning through Symbiotic Evolution 1996 Moriarty, Miikkulainen" "Average Reward Reinforcement Learning: Foundations, Algorithms, and Empirical Results 1996 Mahadevan" "Reinforcement Learning with Hierarchies of Machines 1997 Parr, Russell" "Reinforcement Learning with Perceptual Aliasing: The Perceptual Distinctions Approach 1992 Chrisman" "Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms 1998 Singh, Jaakkola" "Reinforcement learning with hierarchies of machines 1998 Parr, Russell" "Algorithms for Inverse Reinforcement Learning 2000 Ng, Russell" "Experiments with Reinforcement Learning in Problems with Continuous State and Action Spaces 1998 Santamaría, Sutton, Ram" "A stochastic reinforcement learning algorithm for learning real-valued functions 1990 Gullapalli" "Recent Advances in Hierarchical Reinforcement Learning 2003 Barto, Mahadevan" "Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System 2002 Singh, Litman, Kearns, Walker" "Reinforcement learning architectures for animats 1991 Sutton" "Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning 1994 ASADA, NODA, TAWARATSUMIDA, HOSODA, Franklin, Mitchell, Thrun" "Reinforcement Learning: An Introduction 1998 Sutton, Barto" "Reinforcement learning: An introduction 1998 Sutton, Barto" "A Reinforcement Learning Method for Maximizing Undiscounted Rewards 1993 Schwartz" "Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time 1999 Moore, Atkeson" "A Distributed Reinforcement Learning Scheme for Network Routing 1993 Boyan, Littman" "Reinforcement Learning in Continuous Time and Space 2000 Doya" "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density 2001 Mcgovern, Barto" "Markov Games as a Framework for Multi-Agent Reinforcement Learning 1994 Littman" "Consistency of HDP applied to a simple reinforcement learning problem 1990 Werbos" "A Complexity Analysis of Cooperative Mechanisms in Reinforcement Learning 1991 Whitehead" "Reinforcement Learning Methods for Continuous-Time Markov Decision Problems 1994 Bradtke, Duff" "Reinforcement Learning with a Hierarchy of Abstract Models 1992 Singh" "Reinforcement structure/parameter learning for neural-network-based fuzzy logic control systems 1994 Lin, Lee" "Policy Gradient Methods for Reinforcement Learning with Function Approximation 1999 Sutton, Mcallester, Singh, Mansour" "Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm 1998 Hu, Wellman" "Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning 1996 Asada, Noda, Tawaratsumida, Hosoda" "Reinforcement Learning in the Multi-Robot Domain 1997 Mataric" "Efficient Exploration In Reinforcement Learning 1992 Thrun" "The MAXQ Method for Hierarchical Reinforcement Learning 1998 Dietterich" "Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons 1991 Chapman, Kaelbling" "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning 1999 Sutton, Precup, Singh" "Vision-Based Reinforcement Learning for Purposive Behavior Acquisition 1995 Asada, Noda, Tawaratsumida, Hosoda" "Prioritized Sweeping: Reinforcement Learning With Less Data and Less Time 1993 Moore, Atkeson" "Using reinforcement learning to spider the Web efficiently 1999 Rennie, McCallum" "Gradient Descent for General Reinforcement Learning 1998 III, Moore" "Tree-Based Batch Mode Reinforcement Learning 2005 Ernst, Geurts, Wehenkel" "Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley, Dominic, Das, Anderson" "Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems 1997 Bertsekas, Singh" "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding 1995 Sutton" "Active Perception and Reinforcement Learning 1990 Whitehead, Ballard" "The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems 1998 Claus, Boutilier" "Associative search network: A reinforcement learning associative memory 1981 Barto, Sutton, Brouwer" "Reinforcement Learning with Soft State Aggregation 1995 Jordan, Singh, Jaakkola" "A Graph-Based Evolutionary Algorithm: Genetic Network Programming (GNP) and Its Extension Using Reinforcement Learning 2007 Mabu, Hirasawa, Hu" "Feudal Reinforcement Learning 1992 Dayan, Hinton" "Generalization in Reinforcement Learning: Safely Approximating the Value Function 1994 Boyan, Moore" "Residual Algorithms: Reinforcement Learning with Function Approximation 1995 III" "Introduction to Reinforcement Learning 1998 Sutton, Barto" "Apprenticeship learning via inverse reinforcement learning 2004 Abbeel, Ng" "Effective Reinforcement Learning for Mobile Robots 2002 Smart, Kaelbling" "Programming Robots Using Reinforcement Learning and Teaching 1991 Lin" "Near-Optimal Reinforcement Learning in Polynominal Time 1998 Kearns, Singh" "Moore Reinforcement Learning: A Survey 1996 Kaelbling, Littman" "Reinforcement learning is direct adaptive optimal control 1992 Sutton, Barto, Williams" "Automatic Programming of Behavior-Based Robots Using Reinforcement Learning 1991 Mahadevan, Connell" "The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-spaces 1995 Moore, Atkeson" "Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents 1993 Tan" "Improving Elevator Performance Using Reinforcement Learning 1995 Crites, Barto" "Reinforcement Learning for Robots Using Neural Networks 1993 Lin" "Multiagent Reinforcement Learning in the Iterated Prisoner's Dilemma 1995 Sandholm, Crites" "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems 1995 Jordan, Singh, Jaakkola" "On the Computational Economics of Reinforcement Learning 1990 Barto" "Genetic reinforcement learning through symbiotic evolution for fuzzy controller design 2000 Juang, Lin, Lin" "Reinforcement Learning: A Survey 1996 Kaelbling, Littman, Moore" "Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley" "Finding Structure in Reinforcement Learning 1995 Thrun" "Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching 1992 Lin" "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning 2002 Brafman, Tennenholtz" "A Reinforcement Learning Approach to Job-Shop Scheduling 1995 Zhang, Dietterich" "Instance-Based Utile Distinctions for Reinforcement Learning with Hidden State 1995 Mccallum" "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition 2000 Dietterich" "Relational Reinforcement Learning 2001 Driessens, Raedt" "The Parti-Game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-Spaces 1993 Moore" "Behavior coordination for a mobile robot using modular reinforcement learning 1996 Uchibe, Asada, Hosoda" "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning 2001 Brafman, Tennenholtz" "The neural basis of human error processing: Reinforcement learning, dopamine, and the error-related negativity 2002 Holroyd, Coles" "Policy Gradient Reinforcement Learning for Fast Quadrupedal Locomotion 2004 Kohl, Stone" "Ants and Reinforcement Learning: A Case Study in Routing in Dynamic Networks 1997 Subramanian, Druschel, Chen" "Temporal credit assignment in reinforcement learning 1984 Sutton" "Reinforcement Learning with Replacing Eligibility Traces 1996 Singh, Sutton" "A sensor-based navigation for a mobile robot using fuzzy logic and reinforcement learning 1995 Beom, Cho" "Generalization in Reinforcement Learning: Safely Approximating theValue Function 1995 Boyan, Moore" "Reinforcement Learning with Selective Perception and Hidden State 1995 Mccallum" "Generalization in Reinforcement Learning: Safely Approximating the Value Function 1995 " "Gradient Descent fo r General Reinf orcement Learning 0 Baird, Moore" "Reinforcement Learning Applied to Linear Quadratic Regulation 1992 Bradtke" "Packet Routing in Dynamically Changing Networks: A Reinforcement Learning Approach 1993 Boyan, Littman" "Ant-Q: A Reinforcement Learning Approach to the Traveling Salesman Problem 1995 Gambardella, Dorigo" "Issues in Using Function Approximation for Reinforcement Learning 1993 Thrun, Schwartz"}, "Reinforcement Learning with a Hierarchy of Abstract Models 1992 Singh" #{}, "Reinforcement structure/parameter learning for neural-network-based fuzzy logic control systems 1994 Lin, Lee" #{}, "Learning from Demonstration 1996 Schaal" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "hamming distance"} #{"Automatic Programming of Behavior-Based Robots Using Reinforcement Learning 1991 Mahadevan, Connell"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mobile robot"} #{"Effective Reinforcement Learning for Mobile Robots 2002 Smart, Kaelbling"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimization problem"} #{"Residual Algorithms: Reinforcement Learning with Function Approximation 1995 III"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "biological neural network"} #{"The neural basis of human error processing: Reinforcement learning, dopamine, and the error-related negativity 2002 Holroyd, Coles"}, "Policy Gradient Methods for Reinforcement Learning with Function Approximation 1999 Sutton, Mcallester, Singh, Mansour" #{}, "Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm 1998 Hu, Wellman" #{}, "Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning 1996 Asada, Noda, Tawaratsumida, Hosoda" #{}, "A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies 2000 Levin, Pieraccini, Eckert" #{}, "Reinforcement Learning in the Multi-Robot Domain 1997 Mataric" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "pattern recognition"} #{"Associative search network: A reinforcement learning associative memory 1981 Barto, Sutton, Brouwer"}, "Efficient Exploration In Reinforcement Learning 1992 Thrun" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "prisoner's dilemma"} #{"Multiagent Reinforcement Learning in the Iterated Prisoner's Dilemma 1995 Sandholm, Crites"}, "The MAXQ Method for Hierarchical Reinforcement Learning 1998 Dietterich" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dialog system"} #{"A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies 2000 Levin, Pieraccini, Eckert"}, "Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons 1991 Chapman, Kaelbling" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "feedforward neural network"} #{"Residual Algorithms: Reinforcement Learning with Function Approximation 1995 III"}, "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning 1999 Sutton, Precup, Singh" #{}, "Vision-Based Reinforcement Learning for Purposive Behavior Acquisition 1995 Asada, Noda, Tawaratsumida, Hosoda" #{}, "Prioritized Sweeping: Reinforcement Learning With Less Data and Less Time 1993 Moore, Atkeson" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "random walk"} #{"Efficient Exploration In Reinforcement Learning 1992 Thrun"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "imitation"} #{"Vicarious reinforcement and imitative learning 1963 Bandura, Ross, Ross"}, "Using reinforcement learning to spider the Web efficiently 1999 Rennie, McCallum" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "routing"} #{"A Distributed Reinforcement Learning Scheme for Network Routing 1993 Boyan, Littman" "Packet Routing in Dynamically Changing Networks: A Reinforcement Learning Approach 1993 Boyan, Littman"}, "Gradient Descent for General Reinforcement Learning 1998 III, Moore" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "continuous function"} #{"Reinforcement Learning Methods for Continuous-Time Markov Decision Problems 1994 Bradtke, Duff" "Improving Elevator Performance Using Reinforcement Learning 1995 Crites, Barto"}, "Tree-Based Batch Mode Reinforcement Learning 2005 Ernst, Geurts, Wehenkel" #{}, "Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley, Dominic, Das, Anderson" #{}, "Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems 1997 Bertsekas, Singh" #{}, "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding 1995 Sutton" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "real robot"} #{"Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning 1994 ASADA, NODA, TAWARATSUMIDA, HOSODA, Franklin, Mitchell, Thrun" "Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning 1996 Asada, Noda, Tawaratsumida, Hosoda" "Vision-Based Reinforcement Learning for Purposive Behavior Acquisition 1995 Asada, Noda, Tawaratsumida, Hosoda"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control theory"} #{"Reinforcement structure/parameter learning for neural-network-based fuzzy logic control systems 1994 Lin, Lee"}, "Active Perception and Reinforcement Learning 1990 Whitehead, Ballard" #{}, "The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems 1998 Claus, Boutilier" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "fuzzy logic"} #{"Reinforcement structure/parameter learning for neural-network-based fuzzy logic control systems 1994 Lin, Lee" "A sensor-based navigation for a mobile robot using fuzzy logic and reinforcement learning 1995 Beom, Cho"}, "Associative search network: A reinforcement learning associative memory 1981 Barto, Sutton, Brouwer" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "genetic algorithm"} #{"Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley, Dominic, Das, Anderson" "Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "fuzzy control system"} #{"Genetic reinforcement learning through symbiotic evolution for fuzzy controller design 2000 Juang, Lin, Lin"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "education"} #{"Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents 1993 Tan"}, "Reinforcement Learning with Soft State Aggregation 1995 Jordan, Singh, Jaakkola" #{}, "A Graph-Based Evolutionary Algorithm: Genetic Network Programming (GNP) and Its Extension Using Reinforcement Learning 2007 Mabu, Hirasawa, Hu" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "inductive logic programming"} #{"Relational Reinforcement Learning 1998 Dzeroski, Raedt, Blockeel" "Relational Reinforcement Learning 2001 Driessens, Raedt"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "artificial neural network"} #{"Efficient Reinforcement Learning through Symbiotic Evolution 1996 Moriarty, Miikkulainen" "Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley, Dominic, Das, Anderson" "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding 1995 Sutton" "Reinforcement learning is direct adaptive optimal control 1992 Sutton, Barto, Williams" "Reinforcement Learning for Robots Using Neural Networks 1993 Lin" "Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley" "A Reinforcement Learning Approach to Job-Shop Scheduling 1995 Zhang, Dietterich" "Issues in Using Function Approximation for Reinforcement Learning 1993 Thrun, Schwartz"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "adaptive control"} #{"Active Perception and Reinforcement Learning 1990 Whitehead, Ballard" "On the Computational Economics of Reinforcement Learning 1990 Barto"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "human–computer interaction"} #{"A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies 2000 Levin, Pieraccini, Eckert"}, "Feudal Reinforcement Learning 1992 Dayan, Hinton" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control system"} #{"Reinforcement structure/parameter learning for neural-network-based fuzzy logic control systems 1994 Lin, Lee"}, "Generalization in Reinforcement Learning: Safely Approximating the Value Function 1994 Boyan, Moore" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "trial and error"} #{"Reinforcement Learning with a Hierarchy of Abstract Models 1992 Singh" "Automatic Programming of Behavior-Based Robots Using Reinforcement Learning 1991 Mahadevan, Connell"}, "Residual Algorithms: Reinforcement Learning with Function Approximation 1995 III" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "telecommunications network"} #{"Ants and Reinforcement Learning: A Case Study in Routing in Dynamic Networks 1997 Subramanian, Druschel, Chen"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "a priori and a posteriori"} #{"Reinforcement Learning in Continuous Time and Space 2000 Doya"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "discrete time"} #{"Reinforcement Learning Methods for Continuous-Time Markov Decision Problems 1994 Bradtke, Duff"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "robot learning"} #{"Behavior coordination for a mobile robot using modular reinforcement learning 1996 Uchibe, Asada, Hosoda"}, "Introduction to Reinforcement Learning 1998 Sutton, Barto" #{}, "Apprenticeship learning via inverse reinforcement learning 2004 Abbeel, Ng" #{}, "Effective Reinforcement Learning for Mobile Robots 2002 Smart, Kaelbling" #{}, "Reinforcement, expectancy, and learning 1972 Bolles" #{}, "Programming Robots Using Reinforcement Learning and Teaching 1991 Lin" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "function approximation"} #{"Policy Gradient Methods for Reinforcement Learning with Function Approximation 1999 Sutton, Mcallester, Singh, Mansour" "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding 1995 Sutton" "Issues in Using Function Approximation for Reinforcement Learning 1993 Thrun, Schwartz"}, "Near-Optimal Reinforcement Learning in Polynominal Time 1998 Kearns, Singh" #{}, "Moore Reinforcement Learning: A Survey 1996 Kaelbling, Littman" #{}, "Reinforcement learning is direct adaptive optimal control 1992 Sutton, Barto, Williams" #{}, "Automatic Programming of Behavior-Based Robots Using Reinforcement Learning 1991 Mahadevan, Connell" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "web search engine"} #{"Using reinforcement learning to spider the Web efficiently 1999 Rennie, McCallum"}, "The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-spaces 1995 Moore, Atkeson" #{}, "Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents 1993 Tan" #{}, "Improving Elevator Performance Using Reinforcement Learning 1995 Crites, Barto" #{}, "Reinforcement Learning for Robots Using Neural Networks 1993 Lin" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "graph coloring"} #{"Gradient Descent fo r General Reinf orcement Learning 0 Baird, Moore"}, "Multiagent Reinforcement Learning in the Iterated Prisoner's Dilemma 1995 Sandholm, Crites" #{}, "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems 1995 Jordan, Singh, Jaakkola" #{}, "On the Computational Economics of Reinforcement Learning 1990 Barto" #{}, "Genetic reinforcement learning through symbiotic evolution for fuzzy controller design 2000 Juang, Lin, Lin" #{}, "Reinforcement Learning: A Survey 1996 Kaelbling, Littman, Moore" #{}, "Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley" #{}, "Finding Structure in Reinforcement Learning 1995 Thrun" #{}, "Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching 1992 Lin" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "quantum field theory"} #{"Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning 1999 Sutton, Precup, Singh"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "boolean function"} #{"A sensor-based navigation for a mobile robot using fuzzy logic and reinforcement learning 1995 Beom, Cho"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "computational economics"} #{"On the Computational Economics of Reinforcement Learning 1990 Barto"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "probability space"} #{"Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning 1994 ASADA, NODA, TAWARATSUMIDA, HOSODA, Franklin, Mitchell, Thrun" "Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning 1996 Asada, Noda, Tawaratsumida, Hosoda" "The Parti-Game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-Spaces 1993 Moore"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "decision problem"} #{"On the Computational Economics of Reinforcement Learning 1990 Barto"}, "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning 2002 Brafman, Tennenholtz" #{}, "A Reinforcement Learning Approach to Job-Shop Scheduling 1995 Zhang, Dietterich" #{}, "Instance-Based Utile Distinctions for Reinforcement Learning with Hidden State 1995 Mccallum" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mit press"} #{"Generalization in Reinforcement Learning: Safely Approximating the Value Function 1995 "}, "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition 2000 Dietterich" #{}, "Relational Reinforcement Learning 2001 Driessens, Raedt" #{}, "The Parti-Game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-Spaces 1993 Moore" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "supervised learning"} #{"Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley, Dominic, Das, Anderson" "Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "autonomous robot"} #{"Behavior coordination for a mobile robot using modular reinforcement learning 1996 Uchibe, Asada, Hosoda" "A sensor-based navigation for a mobile robot using fuzzy logic and reinforcement learning 1995 Beom, Cho"}, "Behavior coordination for a mobile robot using modular reinforcement learning 1996 Uchibe, Asada, Hosoda" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "finite-state machine"} #{"Markov Games as a Framework for Multi-Agent Reinforcement Learning 1994 Littman"}, "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning 2001 Brafman, Tennenholtz" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "q-learning"} #{"Automatic Programming of Behavior-Based Robots Using Reinforcement Learning 1991 Mahadevan, Connell"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimal decision"} #{"Active Perception and Reinforcement Learning 1990 Whitehead, Ballard"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "travelling salesman problem"} #{"Ant-Q: A Reinforcement Learning Approach to the Traveling Salesman Problem 1995 Gambardella, Dorigo"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "nonlinear system"} #{"Reinforcement learning is direct adaptive optimal control 1992 Sutton, Barto, Williams"}, "The neural basis of human error processing: Reinforcement learning, dopamine, and the error-related negativity 2002 Holroyd, Coles" #{}, "Policy Gradient Reinforcement Learning for Fast Quadrupedal Locomotion 2004 Kohl, Stone" #{}, "Vicarious reinforcement and imitative learning 1963 Bandura, Ross, Ross" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "evolutionary algorithm"} #{"A Graph-Based Evolutionary Algorithm: Genetic Network Programming (GNP) and Its Extension Using Reinforcement Learning 2007 Mabu, Hirasawa, Hu"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "nash equilibrium"} #{"Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm 1998 Hu, Wellman"}, "Ants and Reinforcement Learning: A Case Study in Routing in Dynamic Networks 1997 Subramanian, Druschel, Chen" #{}, "Temporal credit assignment in reinforcement learning 1984 Sutton" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "intelligent agent"} #{"Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons 1991 Chapman, Kaelbling"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov chain mixing time"} #{"Near-Optimal Reinforcement Learning in Polynominal Time 1998 Kearns, Singh"}, "Reinforcement Learning with Replacing Eligibility Traces 1996 Singh, Sutton" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "machine learning"} #{"Using reinforcement learning to spider the Web efficiently 1999 Rennie, McCallum" "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems 1995 Jordan, Singh, Jaakkola" "Reinforcement Learning: A Survey 1996 Kaelbling, Littman, Moore" "Policy Gradient Reinforcement Learning for Fast Quadrupedal Locomotion 2004 Kohl, Stone" "Gradient Descent fo r General Reinf orcement Learning 0 Baird, Moore"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "markov decision process"} #{"Markov Games as a Framework for Multi-Agent Reinforcement Learning 1994 Littman" "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning 1999 Sutton, Precup, Singh" "Apprenticeship learning via inverse reinforcement learning 2004 Abbeel, Ng" "Near-Optimal Reinforcement Learning in Polynominal Time 1998 Kearns, Singh" "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition 2000 Dietterich"}, "A sensor-based navigation for a mobile robot using fuzzy logic and reinforcement learning 1995 Beom, Cho" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "search algorithm"} #{"Reinforcement Learning with Hierarchies of Machines 1997 Parr, Russell"}, "Generalization in Reinforcement Learning: Safely Approximating theValue Function 1995 Boyan, Moore" #{}, "Reinforcement Learning with Selective Perception and Hidden State 1995 Mccallum" #{}, "Generalization in Reinforcement Learning: Safely Approximating the Value Function 1995 " #{}, "Gradient Descent fo r General Reinf orcement Learning 0 Baird, Moore" #{}, "Reinforcement Learning Applied to Linear Quadratic Regulation 1992 Bradtke" #{}, "Packet Routing in Dynamically Changing Networks: A Reinforcement Learning Approach 1993 Boyan, Littman" #{}, "Ant-Q: A Reinforcement Learning Approach to the Traveling Salesman Problem 1995 Gambardella, Dorigo" #{}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "polynomial time"} #{"R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning 2001 Brafman, Tennenholtz"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dynamic programming"} #{"Average Reward Reinforcement Learning: Foundations, Algorithms, and Empirical Results 1996 Mahadevan" "Reinforcement Learning Methods for Continuous-Time Markov Decision Problems 1994 Bradtke, Duff" "Reinforcement learning is direct adaptive optimal control 1992 Sutton, Barto, Williams" "Generalization in Reinforcement Learning: Safely Approximating the Value Function 1995 "}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "necessity and sufficiency"} #{"Reinforcement, expectancy, and learning 1972 Bolles"}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "curse of dimensionality"} #{"Recent Advances in Hierarchical Reinforcement Learning 2003 Barto, Mahadevan" "The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-spaces 1995 Moore, Atkeson" "The Parti-Game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-Spaces 1993 Moore" "Generalization in Reinforcement Learning: Safely Approximating the Value Function 1995 "}, #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "blocks world"} #{"Relational Reinforcement Learning 1998 Dzeroski, Raedt, Blockeel"}, "Issues in Using Function Approximation for Reinforcement Learning 1993 Thrun, Schwartz" #{}}}, :doc-map {"A menu of designs of reinforcement learning over time 1990 Werbos" #cikm13_exp.core.Paper{:title "A menu of designs of reinforcement learning over time", :abstract "", :citation-count 140, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Werbos", :first-name "Paul", :native-name nil, :hindex 0, :gindex 0, :id 249466, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 0, :full-version-url (), :year 1990, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 2006583, :__type "Publication:http://research.microsoft.com"}, "Coordination of multiple behaviors acquired by a vision-based reinforcement learning 1994 Asada, Uchibe, Noda, Tawaratsumida, Hosoda" #cikm13_exp.core.Paper{:title "Coordination of multiple behaviors acquired by a vision-based reinforcement learning", :abstract "A method is proposed which accomplishes a whole task consisting of plural subtasks by coordinating multiple behaviors acquired by a vision-based reinforcement learning. First, individual behaviors which achieve the corresponding subtasks are independently acquired by Q-learning, a widely used reinforcement learning method. Each learned behavior can be represented by an action-value function in terms of state of the environment and", :citation-count 58, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Asada", :first-name "Minoru", :native-name nil, :hindex 0, :gindex 0, :id 2181945, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Uchibe", :first-name "Eiji", :native-name nil, :hindex 0, :gindex 0, :id 762002, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Noda", :first-name "Shoichi", :native-name nil, :hindex 0, :gindex 0, :id 47369611, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Tawaratsumida", :first-name "Sukoya", :native-name nil, :hindex 0, :gindex 0, :id 319589, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Hosoda", :first-name "Koh", :native-name nil, :hindex 0, :gindex 0, :id 87436, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 17, :full-version-url ("http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=407484" "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=00407484"), :year 1994, :doi "10.1109/IROS.1994.407484", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 6430, :name "Collision Avoidance", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43806, :name "Value Function", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Intelligent RObots and Systems - IROS", :short-name "IROS", :cfp nil, :end-year 0, :start-year 0, :id 2821, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 290996, :__type "Publication:http://research.microsoft.com"}, "Relational Reinforcement Learning 1998 Dzeroski, Raedt, Blockeel" #cikm13_exp.core.Paper{:title "Relational Reinforcement Learning", :abstract " Relational reinforcement learning is presented,a learning technique that combinesreinforcement learning with relational learningor inductive logic programming. Due tothe use of a more expressive representationlanguage to represent states, actions and Qfunctions,relational reinforcement learningcan be potentially applied to a new range oflearning tasks. One such task that we investigateis planning in the blocks world, whereit is assumed that the effects of the", :citation-count 134, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Dzeroski", :first-name "Saso", :native-name nil, :hindex 0, :gindex 0, :id 11896, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "De", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Raedt", :first-name "Luc", :native-name nil, :hindex 0, :gindex 0, :id 1195621, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Blockeel", :first-name "Hendrik", :native-name nil, :hindex 0, :gindex 0, :id 1169413, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 13, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/conf/icml/icml1998.html#DzeroskiRB98" "http://www.springerlink.com/content/d4876533486t0h66" "http://www.springerlink.com/index/d4876533486t0h66.pdf"), :year 1998, :doi "10.1007/BFb0027307", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 19633, :name "Inductive Logic Programming", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 67950, :name "Relational Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Machine Learning", :short-name "ICML", :cfp nil, :end-year 0, :start-year 0, :id 35, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 122211, :__type "Publication:http://research.microsoft.com"}, "Frustrative nonreward in partial reinforcement and discrimination learning: Some recent history and a theoretical extension 1962 Amsel" #cikm13_exp.core.Paper{:title "Frustrative nonreward in partial reinforcement and discrimination learning: Some recent history and a theoretical extension", :abstract "Support is presented \"for the position that an \"active' role must be assigned to nonreward in both motivation and inhibition  [Similarities] and differences between recent cognitive-expectancy and neo-Hullian interpretations of the partial reinforcement effect\" are pointed out. Current \"neo-Hullian approaches, employing conditioning-expectancy concepts, go beyond cognitive-expectancy approaches in both specificity and predictive power.\" (104 ref.)", :citation-count 170, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Amsel", :first-name "Abram", :native-name nil, :hindex 0, :gindex 0, :id 54280367, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Psychological Review", :short-name "PSYCHOL REV", :end-year 0, :issn nil, :start-year 0, :id 9936, :__type "Journal:http://research.microsoft.com"}, :reference-count 0, :full-version-url ("http://content.apa.org/journals/rev/69/4/306" "http://dx.doi.org/10.1037/h0046200"), :year 1962, :doi "10.1037/h0046200", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7301, :name "Conditional Expectation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 10381, :name "Discrimination Learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 36921709, :__type "Publication:http://research.microsoft.com"}, "Efficient Reinforcement Learning through Symbiotic Evolution 1996 Moriarty, Miikkulainen" #cikm13_exp.core.Paper{:title "Efficient Reinforcement Learning through Symbiotic Evolution", :abstract " . This article presents a new reinforcement learning method called SANE (Symbiotic,Adaptive Neuro-Evolution), which evolves a population of neurons through genetic algorithmsto form a neural network capable of performing a task. Symbiotic evolution promotes bothcooperation and specialization, which results in a fast, efficient genetic search and discouragesconvergence to suboptimal solutions. In the inverted pendulum problem, SANE formed effectivenetworks 9 to", :citation-count 200, :author ({:citation-count 0, :middle-name "E.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Moriarty", :first-name "David", :native-name nil, :hindex 0, :gindex 0, :id 53105, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Miikkulainen", :first-name "Risto", :native-name nil, :hindex 0, :gindex 0, :id 560507, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Machine Learning", :short-name "ML", :end-year 0, :issn nil, :start-year 0, :id 157, :__type "Journal:http://research.microsoft.com"}, :reference-count 43, :full-version-url ("http://www.springerlink.com/content/um4024hr10450502" "http://www.springerlink.com/index/um4024hr10450502.pdf" "http://www.springerlink.com/content/rt803283061q2622" "http://www.springerlink.com/index/rt803283061q2622.pdf" "http://www.informatik.uni-trier.de/~ley/db/journals/ml/ml22.html#MoriartyM96" "http://www.springerlink.com/index/10.1007/BF00114722" "http://www.springerlink.com/index/pdf/10.1007/BF00114722"), :year 1996, :doi "10.1007/BF00114722", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 16135, :name "Genetic Algorithm", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 16236, :name "Genetics", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 20986, :name "Inverted Pendulum", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 34591, :name "Real World Application", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 46494, :name "Adaptive Heuristic Critic", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 63512, :name "Neural Network", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 405489, :__type "Publication:http://research.microsoft.com"}, "Average Reward Reinforcement Learning: Foundations, Algorithms, and Empirical Results 1996 Mahadevan" #cikm13_exp.core.Paper{:title "Average Reward Reinforcement Learning: Foundations, Algorithms, and Empirical Results", :abstract " . This paper presents a detailed study of average reward reinforcement learning, anundiscounted optimality framework that is more appropriate for cyclical tasks than the much betterstudied discounted framework. A wide spectrum of average reward algorithms are described,ranging from synchronous dynamic programming methods to several (provably convergent) asynchronousalgorithms from optimal control and learning automata. A general sensitive discountoptimality metric called...", :citation-count 139, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Mahadevan", :first-name "Sridhar", :native-name nil, :hindex 0, :gindex 0, :id 905886, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Machine Learning", :short-name "ML", :end-year 0, :issn nil, :start-year 0, :id 157, :__type "Journal:http://research.microsoft.com"}, :reference-count 42, :full-version-url ("http://www.springerlink.com/index/qp72660636n06k12.pdf" "http://www.springerlink.com/content/qp72660636n06k12" "http://www.springerlink.com/index/10.1007/BF00114727" "http://www.springerlink.com/index/pdf/10.1007/BF00114727"), :year 1996, :doi "10.1007/BF00114727", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11245, :name "Dynamic Program", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22413, :name "Learning Automata", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29345, :name "Optimal Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39344, :name "Spectrum", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 61196, :name "Markov Decision Process", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 174286, :__type "Publication:http://research.microsoft.com"}, "Reinforcement Learning with Hierarchies of Machines 1997 Parr, Russell" #cikm13_exp.core.Paper{:title "Reinforcement Learning with Hierarchies of Machines", :abstract "We present a new approach to reinforcement learning in which the poli- cies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to", :citation-count 211, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Parr", :first-name "Ronald", :native-name nil, :hindex 0, :gindex 0, :id 1317199, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "J.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Russell", :first-name "Stuart", :native-name nil, :hindex 0, :gindex 0, :id 755374, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 23, :full-version-url ("http://www.kddresearch.org/Courses/Spring-2004/CIS830/Handouts/PaperReviews/parr97reinforcement.pdf" "http://www.cs.tufts.edu/~roni/Teaching/RL/Papers/HRL-parr-nips97.pdf" "http://www.cs.berkeley.edu/~russell/classes/cs294/f05/papers/parr+russell-1998.pdf" "http://reference.kfupm.edu.sa/content/r/e/reinforcement_learning_with_hierarchies__129688.pdf"), :year 1997, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22470, :name "Learning Process", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 32566, :name "Prior Knowledge", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 32700, :name "Problem Solving", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 36520, :name "Search Space", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 67615, :name "Reinforcement Learn Ing", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 380643, :__type "Publication:http://research.microsoft.com"}, "Reinforcement Learning with Perceptual Aliasing: The Perceptual Distinctions Approach 1992 Chrisman" #cikm13_exp.core.Paper{:title "Reinforcement Learning with Perceptual Aliasing: The Perceptual Distinctions Approach", :abstract " It is known that Perceptual Aliasing may significantlydiminish the effectiveness of reinforcementlearning algorithms [ Whitehead and Ballard,1991 ] . Perceptual aliasing occurs when multiplesituations that are indistinguishable from immediateperceptual input require different responsesfrom the system. For example, if a robot can onlysee forward, yet the presence of a battery chargerbehind it determines whether or not it shouldbackup, immediate perception alone", :citation-count 184, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Chrisman", :first-name "Lonnie", :native-name nil, :hindex 0, :gindex 0, :id 518953, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 0, :full-version-url (), :year 1992, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "National Conference on Artificial Intelligence", :short-name "AAAI", :cfp nil, :end-year 0, :start-year 0, :id 251, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 395766, :__type "Publication:http://research.microsoft.com"}, "Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms 1998 Singh, Jaakkola" #cikm13_exp.core.Paper{:title "Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms", :abstract " An important application of reinforcement learning (RL) is to finite-state control problems and one of the most difficult problems in learning for control is balancing the exploration/exploitation tradeoff. Existing theoretical results for RL give very little guidance on reasonable ways to perform exploration. In this paper, we examine the convergence of single-step on-policy RL algorithms for control. On-policy algorithms cannot", :citation-count 149, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Singh", :first-name "Satinder", :native-name nil, :hindex 0, :gindex 0, :id 306978, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Jaakkola", :first-name "Tommi", :native-name nil, :hindex 0, :gindex 0, :id 1820153, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Machine Learning", :short-name "ML", :end-year 0, :issn nil, :start-year 0, :id 157, :__type "Journal:http://research.microsoft.com"}, :reference-count 40, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/journals/ml/ml38.html#SinghJLS00" "http://www.sztaki.hu/~szcsaba/papers/singh98convergence.pdf"), :year 1998, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7884, :name "Control Problem", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29374, :name "Optimal Policy", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 61196, :name "Markov Decision Process", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 304845, :__type "Publication:http://research.microsoft.com"}, "Reinforcement learning with hierarchies of machines 1998 Parr, Russell" #cikm13_exp.core.Paper{:title "Reinforcement learning with hierarchies of machines", :abstract "", :citation-count 148, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Parr", :first-name "R.", :native-name nil, :hindex 0, :gindex 0, :id 1317199, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Russell", :first-name "S.", :native-name nil, :hindex 0, :gindex 0, :id 755374, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 0, :full-version-url (), :year 1998, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 2022896, :__type "Publication:http://research.microsoft.com"}, "Algorithms for Inverse Reinforcement Learning 2000 Ng, Russell" #cikm13_exp.core.Paper{:title "Algorithms for Inverse Reinforcement Learning", :abstract " This paper addresses the problem of inversereinforcement learning (IRL) in Markov decisionprocesses, that is, the problem of extractinga reward function given observed,optimal behavior. IRL may be useful forapprenticeship learning to acquire skilled behavior,and for ascertaining the reward functionbeing optimized by a natural system. Werst characterize the set of all reward functionsfor which a given policy is optimal. Wethen derive three", :citation-count 127, :author ({:citation-count 0, :middle-name "Y.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Ng", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 838823, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "J.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Russell", :first-name "Stuart", :native-name nil, :hindex 0, :gindex 0, :id 755374, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 3, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/conf/icml/icml2000.html#NgR00"), :year 2000, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 58516, :name "Inverse Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Machine Learning", :short-name "ICML", :cfp nil, :end-year 0, :start-year 0, :id 35, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 246687, :__type "Publication:http://research.microsoft.com"}, "Experiments with Reinforcement Learning in Problems with Continuous State and Action Spaces 1998 Santamaría, Sutton, Ram" #cikm13_exp.core.Paper{:title "Experiments with Reinforcement Learning in Problems with Continuous State and Action Spaces", :abstract " A key element in the solution of reinforcement learning problems is the value function.The purpose of this function is to measure the long-term utility or value of anygiven state and it is important because an agent can use it to decide what to do next. Acommon problem in reinforcement learning when applied to systems having continuousstates and action spaces is", :citation-count 125, :author ({:citation-count 0, :middle-name "Carlos", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Santamaría", :first-name "Juan", :native-name nil, :hindex 0, :gindex 0, :id 52659372, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "S.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Sutton", :first-name "Richard", :native-name nil, :hindex 0, :gindex 0, :id 359696, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Ram", :first-name "Ashwin", :native-name nil, :hindex 0, :gindex 0, :id 956771, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 17, :full-version-url (), :year 1998, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43806, :name "Value Function", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 6026, :name "Closed Form Solution", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15320, :name "Function Approximation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29374, :name "Optimal Policy", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 341009, :__type "Publication:http://research.microsoft.com"}, "A stochastic reinforcement learning algorithm for learning real-valued functions 1990 Gullapalli" #cikm13_exp.core.Paper{:title "A stochastic reinforcement learning algorithm for learning real-valued functions", :abstract "", :citation-count 141, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Gullapalli", :first-name "Vijaykumar", :native-name nil, :hindex 0, :gindex 0, :id 1128595, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Networks", :short-name "", :end-year 0, :issn nil, :start-year 0, :id 913, :__type "Journal:http://research.microsoft.com"}, :reference-count 0, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/journals/nn/nn3.html#Gullapalli90" "http://dx.doi.org/10.1016/0893-6080(90)90056-Q" "http://linkinghub.elsevier.com/retrieve/pii/089360809090056Q"), :year 1990, :doi "10.1016/0893-6080(90)90056-Q", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43806, :name "Value Function", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 1973217, :__type "Publication:http://research.microsoft.com"}, "Recent Advances in Hierarchical Reinforcement Learning 2003 Barto, Mahadevan" #cikm13_exp.core.Paper{:title "Recent Advances in Hierarchical Reinforcement Learning", :abstract "Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended", :citation-count 269, :author ({:citation-count 0, :middle-name "G.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Barto", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 633931, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Mahadevan", :first-name "Sridhar", :native-name nil, :hindex 0, :gindex 0, :id 905886, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Discrete Event Dynamic Systems", :short-name "DEDS", :end-year 0, :issn nil, :start-year 0, :id 823, :__type "Journal:http://research.microsoft.com"}, :reference-count 102, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/journals/deds/deds13.html#BartoM03a" "http://www.springerlink.com/content/p128555707665043" "http://reference.kfupm.edu.sa/content/r/e/recent_advances_in_hierarchical_reinforc_68181.pdf" "http://www.springerlink.com/openurl.asp?id=doi:10.1023/A:1022140919877" "http://dx.doi.org/10.1023/A:1025696116075"), :year 2003, :doi "10.1023/A:1022140919877", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 2256, :name "Associative Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 8755, :name "Curse of Dimensionality", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 18017, :name "Hierarchical Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 23782, :name "Machine Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 30211, :name "Partial Observation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 36950, :name "Semi Markov Decision Process", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 41579, :name "Temporal Abstraction", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 421579, :__type "Publication:http://research.microsoft.com"}, "Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System 2002 Singh, Litman, Kearns, Walker" #cikm13_exp.core.Paper{:title "Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System", :abstract " Designing the dialogue policy of a spoken dialogue system involves many nontrivial choices. This paper presents a reinforcement learning approach for automatically optimizing a dialogue policy, which addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users. We report on the design, construction and empirical evaluation of NJFun, an experimental spoken dialogue system that", :citation-count 133, :author ({:citation-count 0, :middle-name "P.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Singh", :first-name "Satinder", :native-name nil, :hindex 0, :gindex 0, :id 306978, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "J.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Litman", :first-name "Diane", :native-name nil, :hindex 0, :gindex 0, :id 2307297, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "J.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Kearns", :first-name "Michael", :native-name nil, :hindex 0, :gindex 0, :id 299153, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "A.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Walker", :first-name "Marilyn", :native-name nil, :hindex 0, :gindex 0, :id 326171, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Journal of Artificial Intelligence Research", :short-name "JAIR", :end-year 0, :issn nil, :start-year 0, :id 93, :__type "Journal:http://research.microsoft.com"}, :reference-count 20, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/journals/jair/jair16.html#SinghLKW02" "http://www.cs.washington.edu/research/jair/abstracts/singh02a.html"), :year 2002, :doi "10.1613/jair.859", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 304, :name "Access To Information", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 9743, :name "Dialogue Manager", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 9744, :name "Dialogue System", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 12217, :name "Empirical Evaluation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39502, :name "Spoken Dialogue System", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 79971, :__type "Publication:http://research.microsoft.com"}, "Reinforcement learning architectures for animats 1991 Sutton" #cikm13_exp.core.Paper{:title "Reinforcement learning architectures for animats", :abstract "", :citation-count 118, :author ({:citation-count 0, :middle-name "S.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Sutton", :first-name "R.", :native-name nil, :hindex 0, :gindex 0, :id 359696, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 0, :full-version-url (), :year 1991, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Simulation of Adaptive Behavior", :short-name "SAB", :cfp nil, :end-year 0, :start-year 0, :id 2261, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 1410853, :__type "Publication:http://research.microsoft.com"}, "Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning 1994 ASADA, NODA, TAWARATSUMIDA, HOSODA, Franklin, Mitchell, Thrun" #cikm13_exp.core.Paper{:title "Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning", :abstract "This paper presents a method of vision-based reinforcement learning by which a robot learns to shoot a ball into a goal. We discuss several issues in applying the reinforcement learning method to a real robot with vision sensor by which the robot can obtain information about the changes in an environment. First, we construct a state space in terms of", :citation-count 115, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "ASADA", :first-name "MINORU", :native-name nil, :hindex 0, :gindex 0, :id 2181945, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "NODA", :first-name "SHOICHI", :native-name nil, :hindex 0, :gindex 0, :id 47369611, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "TAWARATSUMIDA", :first-name "SUKOYA", :native-name nil, :hindex 0, :gindex 0, :id 319589, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "HOSODA", :first-name "KOH", :native-name nil, :hindex 0, :gindex 0, :id 87436, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "A.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Franklin", :first-name "Judy", :native-name nil, :hindex 0, :gindex 0, :id 377319, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "M.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Mitchell", :first-name "Tom", :native-name nil, :hindex 0, :gindex 0, :id 316778, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Thrun", :first-name "Sebastian", :native-name nil, :hindex 0, :gindex 0, :id 361346, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 17, :full-version-url (), :year 1994, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7139, :name "Computer Simulation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 21111, :name "Is Success", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22953, :name "Linear Order", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 25663, :name "Mobile Robot", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35860, :name "Robot Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 37087, :name "Sensors and Actuators", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39753, :name "State Space", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 358940, :__type "Publication:http://research.microsoft.com"}, "Reinforcement Learning: An Introduction 1998 Sutton, Barto" #cikm13_exp.core.Paper{:title "Reinforcement Learning: An Introduction", :abstract "", :citation-count 134, :author ({:citation-count 0, :middle-name "S.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Sutton", :first-name "R.", :native-name nil, :hindex 0, :gindex 0, :id 359696, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "G.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Barto", :first-name "A.", :native-name nil, :hindex 0, :gindex 0, :id 633931, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 0, :full-version-url (), :year 1998, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 1984681, :__type "Publication:http://research.microsoft.com"}, "Reinforcement learning: An introduction 1998 Sutton, Barto" #cikm13_exp.core.Paper{:title "Reinforcement learning: An introduction", :abstract "", :citation-count 5592, :author ({:citation-count 0, :middle-name "S.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Sutton", :first-name "Richard", :native-name nil, :hindex 0, :gindex 0, :id 359696, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "G.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Barto", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 633931, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 0, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/journals/tnn/tnn9.html#SuttonB98" "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=712192" "http://doi.ieeecomputersociety.org/10.1109/TNN.1998.712192" "http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=712192"), :year 1998, :doi "10.1109/TNN.1998.712192", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 1315091, :__type "Publication:http://research.microsoft.com"}, "A Reinforcement Learning Method for Maximizing Undiscounted Rewards 1993 Schwartz" #cikm13_exp.core.Paper{:title "A Reinforcement Learning Method for Maximizing Undiscounted Rewards", :abstract "", :citation-count 107, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Schwartz", :first-name "Anton", :native-name nil, :hindex 0, :gindex 0, :id 9203524, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 0, :full-version-url (), :year 1993, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Machine Learning", :short-name "ICML", :cfp nil, :end-year 0, :start-year 0, :id 35, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 530960, :__type "Publication:http://research.microsoft.com"}, "Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time 1999 Moore, Atkeson" #cikm13_exp.core.Paper{:title "Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time", :abstract "", :citation-count 140, :author ({:citation-count 0, :middle-name "W.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Moore", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 1802181, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "G", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Atkeson", :first-name "Christopher", :native-name nil, :hindex 0, :gindex 0, :id 2262019, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 0, :full-version-url (), :year 1999, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 68078, :name "Real Time", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 3606503, :__type "Publication:http://research.microsoft.com"}, "A Distributed Reinforcement Learning Scheme for Network Routing 1993 Boyan, Littman" #cikm13_exp.core.Paper{:title "A Distributed Reinforcement Learning Scheme for Network Routing", :abstract " In this paper we describe a self-adjusting algorithm for packet routing, inwhich a reinforcement learning module is embedded into each node of aswitching network. Only local communication is used to keep accurate statisticsat each node on which routing policies lead to minimal delivery times.In simple experiments involving a 36-node, irregularly connected network,this learning approach proves superior to a nonadaptive algorithm", :citation-count 79, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Boyan", :first-name "Justin", :native-name nil, :hindex 0, :gindex 0, :id 1042087, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Littman", :first-name "Michael", :native-name nil, :hindex 0, :gindex 0, :id 171434, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 5, :full-version-url (), :year 1993, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 23213, :name "Local Community", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 27442, :name "Network Routing", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29870, :name "Packet Routing", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 36017, :name "Routing Policies", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 69965, :name "Shortest Path", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 360029, :__type "Publication:http://research.microsoft.com"}, "Reinforcement Learning in Continuous Time and Space 2000 Doya" #cikm13_exp.core.Paper{:title "Reinforcement Learning in Continuous Time and Space", :abstract "This article presents a reinforcement learning framework for continuous- time dynamical systems without a priori discretization of time, state, and action. Based on the Hamilton-Jacobi-Bellman (HJB) equation for infinite- horizon, discounted reward problems, we derive algorithms for estimat- ing value functions and improving policies with the use of function ap- proximators. The process of value function estimation is formulated as", :citation-count 219, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Doya", :first-name "Kenji", :native-name nil, :hindex 0, :gindex 0, :id 1685133, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Computation", :short-name "NECO", :end-year 0, :issn nil, :start-year 0, :id 183, :__type "Journal:http://research.microsoft.com"}, :reference-count 40, :full-version-url ("http://vorlon.case.edu/~sray/mlrg/continuous_rl.pdf" "http://www.informatik.uni-trier.de/~ley/db/journals/neco/neco12.html#Doya00" "http://www.mitpressjournals.org/doi/abs/10.1162/089976600300015961" "http://dx.doi.org/10.1162/089976600300015961"), :year 2000, :doi "10.1162/089976600300015961", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7806, :name "Continuous Time", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11219, :name "Dynamic Model", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11283, :name "Dynamic System", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 13904, :name "Feedback Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 18306, :name "hjb equation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 19748, :name "Infinite Horizon", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 27974, :name "Non-linear Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43806, :name "Value Function", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 56815, :name "hamilton jacobi bellman", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 71127, :name "Temporal Difference", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 208434, :__type "Publication:http://research.microsoft.com"}, "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density 2001 Mcgovern, Barto" #cikm13_exp.core.Paper{:title "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density", :abstract "This paper presents a method by which a rein- forcement learning agent can automatically dis- cover certain types of subgoals online. By creat- ing useful new subgoals while learning, the agent is able to accelerate learning on the current task and to transfer its expertise to other, related tasks through the reuse of its ability to attain subgoals. The agent", :citation-count 122, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Mcgovern", :first-name "Amy", :native-name nil, :hindex 0, :gindex 0, :id 326820, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "G.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Barto", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 633931, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 20, :full-version-url ("http://gandalf.psych.umn.edu/users/schrater/schrater_lab/courses/Labmeeting/mcgovern01automatic.pdf" "http://www.cs.ou.edu/~amy/pubs/mcgovern_barto_icml2001.pdf" "http://www.cs.berkeley.edu/~russell/classes/cs294/f05/papers/mcgovern+barto-2001.pdf"), :year 2001, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 26672, :name "Multiple Instance Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 67614, :name "rein forcement learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Machine Learning", :short-name "ICML", :cfp nil, :end-year 0, :start-year 0, :id 35, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 124116, :__type "Publication:http://research.microsoft.com"}, "Markov Games as a Framework for Multi-Agent Reinforcement Learning 1994 Littman" #cikm13_exp.core.Paper{:title "Markov Games as a Framework for Multi-Agent Reinforcement Learning", :abstract "In the Markov decision process (MDP) formaliza- tion of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsis- tic view, secondary agents can only be part of the environment and are therefore fixed in their be- havior. The framework of Markov games allows us to widen this view to include", :citation-count 604, :author ({:citation-count 0, :middle-name "L.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Littman", :first-name "Michael", :native-name nil, :hindex 0, :gindex 0, :id 171434, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 19, :full-version-url ("http://sca2002.cs.brown.edu/courses/cs244/readings/minimaxQ.pdf" "http://www.cs.duke.edu/courses/cps296.3/spring07/littman94markov.pdf" "http://www.cs.brown.edu/courses/csci2440/readings/minimaxQ.pdf" "http://sca2002.cs.brown.edu/courses/csci2440/readings/minimaxQ.pdf" "http://www.cs.brown.edu/courses/cs244/readings/minimaxQ.pdf" "http://people.ee.duke.edu/~lcarin/emag/seminar_presentations/Markov_Games_Littman.pdf" "http://www.eecs.harvard.edu/~parkes/cs286r/spring06/papers/littman94markov.pdf" "http://www.cs.uwaterloo.ca/~klarson/teaching/W06-886/papers/Littman94.pdf" "http://www.informatik.uni-trier.de/~ley/db/conf/icml/icml1994.html#Littman94" "http://www.cs.rutgers.edu/~mlittman/papers/ml94-final.pdf"), :year 1994, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 1007, :name "Agent Interaction", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29374, :name "Optimal Policy", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 60841, :name "Multi Agent Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 61196, :name "Markov Decision Process", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Machine Learning", :short-name "ICML", :cfp nil, :end-year 0, :start-year 0, :id 35, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 2159, :__type "Publication:http://research.microsoft.com"}, "Consistency of HDP applied to a simple reinforcement learning problem 1990 Werbos" #cikm13_exp.core.Paper{:title "Consistency of HDP applied to a simple reinforcement learning problem", :abstract "", :citation-count 70, :author ({:citation-count 0, :middle-name "J.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Werbos", :first-name "Paul", :native-name nil, :hindex 0, :gindex 0, :id 249466, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Networks", :short-name "", :end-year 0, :issn nil, :start-year 0, :id 913, :__type "Journal:http://research.microsoft.com"}, :reference-count 0, :full-version-url ("http://linkinghub.elsevier.com/retrieve/pii/0893608090900883" "http://dx.doi.org/10.1016/0893-6080(90)90088-3" "http://www.informatik.uni-trier.de/~ley/db/journals/nn/nn3.html#Werbos90"), :year 1990, :doi "10.1016/0893-6080(90)90088-3", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 2153798, :__type "Publication:http://research.microsoft.com"}, "A Complexity Analysis of Cooperative Mechanisms in Reinforcement Learning 1991 Whitehead" #cikm13_exp.core.Paper{:title "A Complexity Analysis of Cooperative Mechanisms in Reinforcement Learning", :abstract "", :citation-count 104, :author ({:citation-count 0, :middle-name "D.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Whitehead", :first-name "Steven", :native-name nil, :hindex 0, :gindex 0, :id 49310, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 0, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/conf/aaai/aaai91-2.html#Whitehead91"), :year 1991, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 6886, :name "Complexity Analysis", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "National Conference on Artificial Intelligence", :short-name "AAAI", :cfp nil, :end-year 0, :start-year 0, :id 251, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 490553, :__type "Publication:http://research.microsoft.com"}, "Reinforcement Learning Methods for Continuous-Time Markov Decision Problems 1994 Bradtke, Duff" #cikm13_exp.core.Paper{:title "Reinforcement Learning Methods for Continuous-Time Markov Decision Problems", :abstract " Semi-Markov Decision Problems are continuous time generalizations of discrete time MarkovDecision Problems. A number of reinforcement learning algorithms have been developedrecently for the solution of Markov Decision Problems, based on the ideas of asynchronousdynamic programming and stochastic approximation. Among these are TD(), Q-learning,and Real-time Dynamic Programming. After reviewing semi-Markov Decision Problemsand Bellman's optimality equation in that context, we propose...", :citation-count 93, :author ({:citation-count 0, :middle-name "J.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Bradtke", :first-name "Steven", :native-name nil, :hindex 0, :gindex 0, :id 758092, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "O.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Duff", :first-name "Michael", :native-name nil, :hindex 0, :gindex 0, :id 868331, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 5, :full-version-url (), :year 1994, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7806, :name "Continuous Time", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 10352, :name "Discrete Time", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39996, :name "Stochastic Approximation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 61195, :name "Markov Decision Problem", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 68095, :name "Real Time Dynamic Programming", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 172533, :__type "Publication:http://research.microsoft.com"}, "Reinforcement Learning with a Hierarchy of Abstract Models 1992 Singh" #cikm13_exp.core.Paper{:title "Reinforcement Learning with a Hierarchy of Abstract Models", :abstract " ModelsSatinder P. SinghDepartment of Computer ScienceUniversity of MassachusettsAmherst, MA 01003singh@cs.umass.eduAbstractReinforcement learning (RL) algorithms have traditionallybeen thought of as trial and error learningmethods that use actual control experience toincrementally improve a control policy. Sutton'sDYNA architecture demonstrated that RL algorithmscan work as well using simulated experiencefrom an environment model, and that the resultingcomputation was similar...", :citation-count 63, :author ({:citation-count 0, :middle-name "P.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Singh", :first-name "Satinder", :native-name nil, :hindex 0, :gindex 0, :id 306978, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 11, :full-version-url (), :year 1992, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "National Conference on Artificial Intelligence", :short-name "AAAI", :cfp nil, :end-year 0, :start-year 0, :id 251, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 315984, :__type "Publication:http://research.microsoft.com"}, "Reinforcement structure/parameter learning for neural-network-based fuzzy logic control systems 1994 Lin, Lee" #cikm13_exp.core.Paper{:title "Reinforcement structure/parameter learning for neural-network-based fuzzy logic control systems", :abstract "This paper proposes a reinforcement neural-network-based fuzzy logic control system (RNN-FLCS) for solving various reinforcement learning problems. The proposed RNN-FLCS is constructed by integrating two neural-network-based fuzzy logic controllers (NN-FLC's), each of which is a connectionist model with a feedforward multilayered network developed for the realization of a fuzzy logic controller. One NN-FLC performs as a fuzzy predictor, and the", :citation-count 167, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Lin", :first-name "Chin-Teng", :native-name nil, :hindex 0, :gindex 0, :id 1115114, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "S. G.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Lee", :first-name "C.", :native-name nil, :hindex 0, :gindex 0, :id 124953, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "IEEE Transactions on Fuzzy Systems", :short-name "TFS", :end-year 0, :issn nil, :start-year 0, :id 669, :__type "Journal:http://research.microsoft.com"}, :reference-count 0, :full-version-url ("http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=273126" "http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=273126"), :year 1994, :doi "10.1109/91.273126", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7139, :name "Computer Simulation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7481, :name "connectionist models", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 9193, :name "Decision Making", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15487, :name "Fuzzy Controller", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15515, :name "Fuzzy Logic Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15516, :name "Fuzzy Logic Controller", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22436, :name "Learning Environment", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22470, :name "Learning Process", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 27453, :name "Network Structure", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 32307, :name "Prediction Method", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 37749, :name "Similarity Measure", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 40471, :name "Structure Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 63512, :name "Neural Network", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 71127, :name "Temporal Difference", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 1684525, :__type "Publication:http://research.microsoft.com"}, "Learning from Demonstration 1996 Schaal" #cikm13_exp.core.Paper{:title "Learning from Demonstration", :abstract "By now it is widely accepted that learning a task from scratch, i.e., without any prior knowledge, is a daunting undertaking. Humans, however, rarely attempt to learn from scratch. They extract initial biases as well as strate- gies how to approach a learning problem from instructions and/or demon- strations of other humans. For learning control, this paper investigates how learning", :citation-count 132, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Schaal", :first-name "Stefan", :native-name nil, :hindex 0, :gindex 0, :id 613248, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 19, :full-version-url ("http://nips.djvuzone.org/djvu/nips09/1040.djvu" "http://www.informatik.uni-trier.de/~ley/db/conf/nips/nipsN1996.html#Schaal96"), :year 1996, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22425, :name "Learning Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22440, :name "Learning From Demonstration", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22967, :name "Linear Quadratic Regulator", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 32566, :name "Prior Knowledge", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 32881, :name "Profitability", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35852, :name "Robot Arm", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35860, :name "Robot Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 37671, :name "Signal Processing", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43806, :name "Value Function", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 60235, :name "Learning Problems", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 561291, :__type "Publication:http://research.microsoft.com"}, "Policy Gradient Methods for Reinforcement Learning with Function Approximation 1999 Sutton, Mcallester, Singh, Mansour" #cikm13_exp.core.Paper{:title "Policy Gradient Methods for Reinforcement Learning with Function Approximation", :abstract "Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and deter- mining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, indepen- dent of the value function, and is updated according to", :citation-count 338, :author ({:citation-count 0, :middle-name "S.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Sutton", :first-name "Richard", :native-name nil, :hindex 0, :gindex 0, :id 359696, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "A.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Mcallester", :first-name "David", :native-name nil, :hindex 0, :gindex 0, :id 1362577, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "P.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Singh", :first-name "Satinder", :native-name nil, :hindex 0, :gindex 0, :id 306978, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Mansour", :first-name "Yishay", :native-name nil, :hindex 0, :gindex 0, :id 2067627, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 23, :full-version-url ("http://nips.djvuzone.org/djvu/nips12/1057.djvu" "http://www.informatik.uni-trier.de/~ley/db/conf/nips/nips1999.html#SuttonMSM99" "http://reference.kfupm.edu.sa/content/p/o/policy_gradient_methods_for_reinforcemen_24372.pdf" "http://webdocs.cs.ualberta.ca/~sutton/papers/SMSM-NIPS99.pdf" "http://www.damas.ift.ulaval.ca/~coursMAS/Presentations-2K8/policy_gradient.pdf"), :year 1999, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 417, :name "Action Selection", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 3256, :name "Best Approximation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11245, :name "Dynamic Program", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15320, :name "Function Approximation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 16725, :name "Gradient Method", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 24623, :name "Mean Square Error", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29374, :name "Optimal Policy", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 31651, :name "Policy Iteration", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43806, :name "Value Function", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 52115, :name "dynamic pro gramming", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 52435, :name "Decision Tree", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 63512, :name "Neural Network", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 71127, :name "Temporal Difference", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 149439, :__type "Publication:http://research.microsoft.com"}, "Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm 1998 Hu, Wellman" #cikm13_exp.core.Paper{:title "Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm", :abstract " In this paper, we adopt general-sum stochasticgames as a framework for multiagent reinforcementlearning. Our work extends previouswork by Littman on zero-sum stochasticgames to a broader framework. We designa multiagent Q-learning method underthis framework, and prove that it convergesto a Nash equilibrium under specified conditions.This algorithm is useful for finding theoptimal strategy when there exists a uniqueNash equilibrium in the game.", :citation-count 345, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Hu", :first-name "Junling", :native-name nil, :hindex 0, :gindex 0, :id 2209189, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "P.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Wellman", :first-name "Michael", :native-name nil, :hindex 0, :gindex 0, :id 544883, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 12, :full-version-url (), :year 1998, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22449, :name "Learning Methods", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 41791, :name "Theoretical Framework", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 62996, :name "Nash Equilibria", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 63004, :name "Nash Equilibrium", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Machine Learning", :short-name "ICML", :cfp nil, :end-year 0, :start-year 0, :id 35, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 186339, :__type "Publication:http://research.microsoft.com"}, "Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning 1996 Asada, Noda, Tawaratsumida, Hosoda" #cikm13_exp.core.Paper{:title "Purposive Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning", :abstract "This paper presents a method of vision-based reinforcement learning by which a robot learns to shoot a ball into a goal. We discuss several issues in applying the reinforcement learning method to a real robot with vision sensor by which the robot can obtain information about the changes in an environment. First, we construct a state space in terms of", :citation-count 150, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Asada", :first-name "Minoru", :native-name nil, :hindex 0, :gindex 0, :id 2181945, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Noda", :first-name "Shoichi", :native-name nil, :hindex 0, :gindex 0, :id 47369611, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Tawaratsumida", :first-name "Sukoya", :native-name nil, :hindex 0, :gindex 0, :id 319589, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Hosoda", :first-name "Koh", :native-name nil, :hindex 0, :gindex 0, :id 87436, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Machine Learning", :short-name "ML", :end-year 0, :issn nil, :start-year 0, :id 157, :__type "Journal:http://research.microsoft.com"}, :reference-count 17, :full-version-url ("http://www.springerlink.com/index/10.1007/BF00117447" "http://www.springerlink.com/index/q2j2558257l0h7r7.pdf" "http://www.springerlink.com/content/q2j2558257l0h7r7" "http://www.springerlink.com/index/pdf/10.1007/BF00117447" "http://www.informatik.uni-trier.de/~ley/db/journals/ml/ml23.html#AsadaNTH96" "http://www.er.ams.eng.osaka-u.ac.jp/Paper/1996/Asada96a.pdf"), :year 1996, :doi "10.1007/BF00117447", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7139, :name "Computer Simulation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 21111, :name "Is Success", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22953, :name "Linear Order", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 25663, :name "Mobile Robot", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35860, :name "Robot Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 37087, :name "Sensors and Actuators", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39753, :name "State Space", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 364456, :__type "Publication:http://research.microsoft.com"}, "A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies 2000 Levin, Pieraccini, Eckert" #cikm13_exp.core.Paper{:title "A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies", :abstract "Abstract, In this paper, we propose a quantitative model for dialog systems that can be used for learning the dialog strategy. We claim that the problem of dialog design can be formalized as an optimization problem with an objective function reflecting different dialog dimensions relevant for a given application. We also show that any dialog system can be formally described", :citation-count 195, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Levin", :first-name "Esther", :native-name nil, :hindex 0, :gindex 0, :id 734565, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Pieraccini", :first-name "Roberto", :native-name nil, :hindex 0, :gindex 0, :id 516773, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Eckert", :first-name "Wieland", :native-name nil, :hindex 0, :gindex 0, :id 2373253, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "IEEE Transactions on Speech and Audio Processing", :short-name "IEEE SAP", :end-year 0, :issn nil, :start-year 0, :id 1070, :__type "Journal:http://research.microsoft.com"}, :reference-count 30, :full-version-url ("http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=817450" "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=00817450" "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=817450"), :year 2000, :doi "10.1109/89.817450", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 9202, :name "Decision Process", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 18622, :name "Human Machine Interaction", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 19482, :name "Indexing Terms", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 28790, :name "Objective Function", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29413, :name "Optimization Problem", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39753, :name "State Space", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39760, :name "State Space Representation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39761, :name "State Transition", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 40076, :name "Stochastic Model", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 40796, :name "Supervised Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 47515, :name "Air Travel Information System", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 61196, :name "Markov Decision Process", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 67614, :name "rein forcement learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 69570, :name "Spoken Language Systems", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 1642746, :__type "Publication:http://research.microsoft.com"}, "Reinforcement Learning in the Multi-Robot Domain 1997 Mataric" #cikm13_exp.core.Paper{:title "Reinforcement Learning in the Multi-Robot Domain", :abstract "", :citation-count 210, :author ({:citation-count 0, :middle-name "J.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Mataric", :first-name "Maja", :native-name nil, :hindex 0, :gindex 0, :id 251027, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Autonomous Robots", :short-name "AROBOTS", :end-year 0, :issn nil, :start-year 0, :id 208, :__type "Journal:http://research.microsoft.com"}, :reference-count 24, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/journals/arobots/arobots4.html#Matari97"), :year 1997, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 2221, :name "Assignment Problem", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11182, :name "Dynamic Environment", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 13361, :name "Experimental Validation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 17057, :name "Group Behavior", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 25663, :name "Mobile Robot", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35860, :name "Robot Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 60845, :name "Multi Agent System", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 858559, :__type "Publication:http://research.microsoft.com"}, "Efficient Exploration In Reinforcement Learning 1992 Thrun" #cikm13_exp.core.Paper{:title "Efficient Exploration In Reinforcement Learning", :abstract " Exploration plays a fundamental role in any active learning system. This study evaluates the role of explorationin active learning and describes several local techniques for exploration in finite, discrete domains,embedded in a reinforcement learning framework (delayed reinforcement).This paper distinguishes between two families of exploration schemes: undirected and directed exploration.While the former family is closely related to random walk exploration, directed", :citation-count 114, :author ({:citation-count 0, :middle-name "B.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Thrun", :first-name "Sebastian", :native-name nil, :hindex 0, :gindex 0, :id 361346, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 11, :full-version-url (), :year 1992, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 473, :name "Active Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 68242, :name "Random Walk", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 349436, :__type "Publication:http://research.microsoft.com"}, "The MAXQ Method for Hierarchical Reinforcement Learning 1998 Dietterich" #cikm13_exp.core.Paper{:title "The MAXQ Method for Hierarchical Reinforcement Learning", :abstract "This paper presents a new approach to hier- archical reinforcement learning based on the MAXQ decomposition of the value function. The MAXQ decomposition has both a procedu- ral semantics—as a subroutine hierarchy—and a declarative semantics—as a representation of the value function of a hierarchical policy. MAXQ unifies and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan", :citation-count 111, :author ({:citation-count 0, :middle-name "G.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Dietterich", :first-name "Thomas", :native-name nil, :hindex 0, :gindex 0, :id 1622575, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 13, :full-version-url (), :year 1998, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22409, :name "Learning Algorithm", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29398, :name "Optimal Value Function", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43806, :name "Value Function", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Machine Learning", :short-name "ICML", :cfp nil, :end-year 0, :start-year 0, :id 35, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 392315, :__type "Publication:http://research.microsoft.com"}, "Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons 1991 Chapman, Kaelbling" #cikm13_exp.core.Paper{:title "Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons", :abstract "Delayed reinforcement learning is an attrac­ tive framework for the unsupervised learning of action policies for autonomous agents. Some existing delayed reinforcement learning tech­ niques have shown promise in simple domains. However, a number of hurdles must be passed before they are applicable to realistic problems. This paper describes one such difficulty, the in­ put generalization problem (whereby the system", :citation-count 148, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Chapman", :first-name "David", :native-name nil, :hindex 0, :gindex 0, :id 12718335, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "Pack", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Kaelbling", :first-name "Leslie", :native-name nil, :hindex 0, :gindex 0, :id 289339, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 13, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/conf/ijcai/ijcai91.html#ChapmanK91"), :year 1991, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 2673, :name "Autonomous Agent", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 2785, :name "Back Propagation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29374, :name "Optimal Policy", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 30629, :name "Performance Comparison", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39753, :name "State Space", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43555, :name "Unsupervised Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Joint Conference on Artificial Intelligence", :short-name "IJCAI", :cfp nil, :end-year 0, :start-year 0, :id 64, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 613921, :__type "Publication:http://research.microsoft.com"}, "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning 1999 Sutton, Precup, Singh" #cikm13_exp.core.Paper{:title "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning", :abstract "Learning, planning, and representing knowledge at multiple levels of temporal ab- straction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforce- ment learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking ac- tion", :citation-count 504, :author ({:citation-count 0, :middle-name "S.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Sutton", :first-name "Richard", :native-name nil, :hindex 0, :gindex 0, :id 359696, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Precup", :first-name "Doina", :native-name nil, :hindex 0, :gindex 0, :id 290161, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "P.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Singh", :first-name "Satinder", :native-name nil, :hindex 0, :gindex 0, :id 306978, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Artificial Intelligence", :short-name "AI", :end-year 0, :issn nil, :start-year 0, :id 242, :__type "Journal:http://research.microsoft.com"}, :reference-count 93, :full-version-url ("http://linkinghub.elsevier.com/retrieve/pii/S0004370299000521" "http://www.sciencedirect.com/science/article/pii/S0004370299000521" "http://www.cs.ualberta.ca/~sutton/papers/SPS-aij.pdf" "http://www-all.cs.umass.edu/pubs/1999/sutton_ps_AI99.pdf" "http://webdocs.cs.ualberta.ca/~papersdb/uploaded_files/470/paper_SPS-aij.pdf" "http://www.cs.ualberta.ca/~papersdb/uploaded_files/470/paper_SPS-aij.pdf" "http://webdocs.cs.ualberta.ca/~sutton/papers/SPS-aij.pdf"), :year 1999, :doi "10.1016/S0004-3702(99)00052-1", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15320, :name "Function Approximation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22449, :name "Learning Methods", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 36950, :name "Semi Markov Decision Process", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 41579, :name "Temporal Abstraction", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 46486, :name "Abstraction Hierarchy", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 52115, :name "dynamic pro gramming", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 61196, :name "Markov Decision Process", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 67617, :name "reinforce ment learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 227857, :__type "Publication:http://research.microsoft.com"}, "Vision-Based Reinforcement Learning for Purposive Behavior Acquisition 1995 Asada, Noda, Tawaratsumida, Hosoda" #cikm13_exp.core.Paper{:title "Vision-Based Reinforcement Learning for Purposive Behavior Acquisition", :abstract "This paper presents a method of vision-based rein- forcement learning by which a robot learns to shoot a ball into a goal, and discusses several issues in applying the reinforcement learning method to a real robot with vision sensor. First, a \\state-action deviation\" prob- lem is found as a form of perceptual aliasing in con- structing the state and action", :citation-count 59, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Asada", :first-name "Minoru", :native-name nil, :hindex 0, :gindex 0, :id 2181945, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Noda", :first-name "Shoichi", :native-name nil, :hindex 0, :gindex 0, :id 47369611, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Tawaratsumida", :first-name "Sukoya", :native-name nil, :hindex 0, :gindex 0, :id 319589, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Hosoda", :first-name "Koh", :native-name nil, :hindex 0, :gindex 0, :id 87436, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 13, :full-version-url ("http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=525277" "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=00525277" "http://reference.kfupm.edu.sa/content/v/i/vision_based_reinforcement_learning_for__1271168.pdf"), :year 1995, :doi "10.1109/ROBOT.1995.525277", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7139, :name "Computer Simulation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 21111, :name "Is Success", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22953, :name "Linear Order", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35860, :name "Robot Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 37087, :name "Sensors and Actuators", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39753, :name "State Space", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 67614, :name "rein forcement learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Robotics and Automation", :short-name "ICRA", :cfp nil, :end-year 0, :start-year 0, :id 38, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 342686, :__type "Publication:http://research.microsoft.com"}, "Prioritized Sweeping: Reinforcement Learning With Less Data and Less Time 1993 Moore, Atkeson" #cikm13_exp.core.Paper{:title "Prioritized Sweeping: Reinforcement Learning With Less Data and Less Time", :abstract " We present a new algorithm, Prioritized Sweeping, for efficient prediction and control of stochasticMarkov systems. Incremental learning methods such as Temporal Differencing and Qlearninghave fast real time performance. Classical methods are slower, but more accurate,because they make full use of the observations. Prioritized Sweeping aims for the best of bothworlds. It uses all previous experiences both to prioritize important dynamic", :citation-count 327, :author ({:citation-count 0, :middle-name "W.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Moore", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 1802181, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "G.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Atkeson", :first-name "Christopher", :native-name nil, :hindex 0, :gindex 0, :id 2262019, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Machine Learning", :short-name "ML", :end-year 0, :issn nil, :start-year 0, :id 157, :__type "Journal:http://research.microsoft.com"}, :reference-count 36, :full-version-url ("http://www.springerlink.com/index/r0t830035u571jt7.pdf" "http://www.springerlink.com/content/r0t830035u571jt7" "http://www.springerlink.com/index/10.1007/BF00993104" "http://www.springerlink.com/index/pdf/10.1007/BF00993104" "http://www.informatik.uni-trier.de/~ley/db/journals/ml/ml13.html#MooreA93"), :year 1993, :doi "10.1007/BF00993104", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 2344, :name "Asynchronous Dynamic Programming", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 17969, :name "Heuristic Search", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 19440, :name "Incremental Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22425, :name "Learning Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 24890, :name "Memory Based Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 68078, :name "Real Time", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 384201, :__type "Publication:http://research.microsoft.com"}, "Using reinforcement learning to spider the Web efficiently 1999 Rennie, McCallum" #cikm13_exp.core.Paper{:title "Using reinforcement learning to spider the Web efficiently", :abstract "Consider the task of exploring the Web in order to find\n                   pages of a particular kind or on a particular topic. This\n                   task arises in the construction of search engines and Web\n                   knowledge bases. The paper argues that the creation of\n                   efficient Web spiders is best framed and solved by\n                   reinforcement learning, a branch of machine learning that\n                   concerns itself", :citation-count 143, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Rennie", :first-name "Jason", :native-name nil, :hindex 0, :gindex 0, :id 172326, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "K.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "McCallum", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 2232623, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 13, :full-version-url (), :year 1999, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 21646, :name "Knowledge Base", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 23782, :name "Machine Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 27037, :name "Naive Bayes", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 36510, :name "Search Engine", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 37152, :name "Sequential Decision Making", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43806, :name "Value Function", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 48078, :name "Breadth First Search", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Machine Learning", :short-name "ICML", :cfp nil, :end-year 0, :start-year 0, :id 35, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 384352, :__type "Publication:http://research.microsoft.com"}, "Gradient Descent for General Reinforcement Learning 1998 III, Moore" #cikm13_exp.core.Paper{:title "Gradient Descent for General Reinforcement Learning", :abstract " A simple learning rule is derived, the VAPS algorithm, which canbe instantiated to generate a wide range of new reinforcementlearningalgorithms. These algorithms solve a number of openproblems, define several new approaches to reinforcement learning,and unify different approaches to reinforcement learning under asingle theory. These algorithms all have guaranteed convergence,and include modifications of several existing algorithms that wereknown to fail to", :citation-count 126, :author ({:citation-count 0, :middle-name "C. Baird", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "III", :first-name "Leemon", :native-name nil, :hindex 0, :gindex 0, :id 6826440, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "W.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Moore", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 1802181, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 9, :full-version-url (), :year 1998, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 55367, :name "Gradient Descent", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 391460, :__type "Publication:http://research.microsoft.com"}, "Tree-Based Batch Mode Reinforcement Learning 2005 Ernst, Geurts, Wehenkel" #cikm13_exp.core.Paper{:title "Tree-Based Batch Mode Reinforcement Learning", :abstract "", :citation-count 129, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Ernst", :first-name "Damien", :native-name nil, :hindex 0, :gindex 0, :id 163800, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Geurts", :first-name "Pierre", :native-name nil, :hindex 0, :gindex 0, :id 982292, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Wehenkel", :first-name "Louis", :native-name nil, :hindex 0, :gindex 0, :id 104109, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Journal of Machine Learning Research", :short-name "JMLR", :end-year 0, :issn nil, :start-year 0, :id 126, :__type "Journal:http://research.microsoft.com"}, :reference-count 42, :full-version-url ("http://www.jmlr.org/papers/v6/ernst05a.html" "http://www.informatik.uni-trier.de/~ley/db/journals/jmlr/jmlr6.html#ErnstGW05"), :year 2005, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 12469, :name "Ensemble Method", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29345, :name "Optimal Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 34290, :name "Random Tree", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 34947, :name "Regression Tree", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 40796, :name "Supervised Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43808, :name "Value Iteration", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 2384668, :__type "Publication:http://research.microsoft.com"}, "Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley, Dominic, Das, Anderson" #cikm13_exp.core.Paper{:title "Genetic Reinforcement Learning for Neurocontrol Problems", :abstract "Empirical tests indicate that at least one class of genetic algorithms yields good performance for neural network weight optimization in terms of learning rates and scalability. The successful application of these genetic algorithms to supervised learning problems sets the stage for the use of genetic algorithms in reinforcement learning problems. On a simulated inverted-pendulum control problem, “genetic reinforcement learning” produces", :citation-count 84, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Whitley", :first-name "Darrell", :native-name nil, :hindex 0, :gindex 0, :id 1328499, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Dominic", :first-name "Stephen", :native-name nil, :hindex 0, :gindex 0, :id 1990832, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Das", :first-name "Rajarshi", :native-name nil, :hindex 0, :gindex 0, :id 872956, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "W.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Anderson", :first-name "Charles", :native-name nil, :hindex 0, :gindex 0, :id 951737, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Machine Learning", :short-name "ML", :end-year 0, :issn nil, :start-year 0, :id 157, :__type "Journal:http://research.microsoft.com"}, :reference-count 36, :full-version-url ("http://www.springerlink.com/content/vjg8418t61v58882" "http://www.springerlink.com/index/vjg8418t61v58882.pdf" "http://www.springerlink.com/openurl.asp?id=doi:10.1023/A:1022674030396"), :year 1993, :doi "10.1023/A:1022674030396", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 637, :name "Adaptive Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7884, :name "Control Problem", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 16135, :name "Genetic Algorithm", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 16236, :name "Genetics", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 20986, :name "Inverted Pendulum", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22472, :name "Learning Rate", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 40796, :name "Supervised Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 63512, :name "Neural Network", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 71127, :name "Temporal Difference", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 15222079, :__type "Publication:http://research.microsoft.com"}, "Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems 1997 Bertsekas, Singh" #cikm13_exp.core.Paper{:title "Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems", :abstract " In cellular telephone systems, an important problem is to dynamically allocatethe communication resource (channels) so as to maximize service ina stochastic caller environment. This problem is naturally formulated as adynamic programming problem and we use a reinforcement learning (RL)method to find dynamic channel allocation policies that are better thanprevious heuristic solutions. The policies obtained perform well for a broadvariety of", :citation-count 144, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Bertsekas", :first-name "Dimitri", :native-name nil, :hindex 0, :gindex 0, :id 2167725, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Singh", :first-name "Satinder", :native-name nil, :hindex 0, :gindex 0, :id 306978, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 7, :full-version-url (), :year 1997, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11157, :name "Dynamic Channel Allocation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 333029, :__type "Publication:http://research.microsoft.com"}, "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding 1995 Sutton" #cikm13_exp.core.Paper{:title "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding", :abstract "Abstract On large problems, reinforcement learning systems must use parameterized function approximators such as neural networks in order to generalize between similar situations and actions. In these cases there are no strong theoretical results on the accuracy of convergence, and computational results have been mixed. In particular, Boyan and Moore reported at last year’s meeting a series of negative results", :citation-count 432, :author ({:citation-count 0, :middle-name "S.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Sutton", :first-name "Richard", :native-name nil, :hindex 0, :gindex 0, :id 359696, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 24, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/conf/nips/nips1995.html#Sutton95" "http://nips.djvuzone.org/djvu/nips08/1038.djvu" "http://webdocs.cs.ualberta.ca/~sutton/papers/sutton-96.pdf"), :year 1995, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7884, :name "Control Problem", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11245, :name "Dynamic Program", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15320, :name "Function Approximation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 26096, :name "Monte Carlo Method", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39753, :name "State Space", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 63512, :name "Neural Network", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 561015, :__type "Publication:http://research.microsoft.com"}, "Active Perception and Reinforcement Learning 1990 Whitehead, Ballard" #cikm13_exp.core.Paper{:title "Active Perception and Reinforcement Learning", :abstract "This paper considers adaptive control architectures that integrate active sensorimotor systems with decision systems based on reinforcement learning. One unavoidable consequence of active perception is that the agent's internal representation often confounds external world states. We call this phenomenon perceptual aliasing and show that it destabilizes existing reinforcement learning algorithms with respect to the optimal decision policy. A new decision", :citation-count 106, :author ({:citation-count 0, :middle-name "D.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Whitehead", :first-name "Steven", :native-name nil, :hindex 0, :gindex 0, :id 49310, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "H.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Ballard", :first-name "Dana", :native-name nil, :hindex 0, :gindex 0, :id 1853352, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Computation", :short-name "NECO", :end-year 0, :issn nil, :start-year 0, :id 183, :__type "Journal:http://research.microsoft.com"}, :reference-count 9, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/conf/icml/ml1990.html#WhiteheadB90" "http://www.mitpressjournals.org/doi/abs/10.1162/neco.1990.2.4.409" "http://dx.doi.org/10.1162/neco.1990.2.4.409"), :year 1990, :doi "10.1162/neco.1990.2.4.409", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 637, :name "Adaptive Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7864, :name "Control Architecture", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22409, :name "Learning Algorithm", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 531216, :__type "Publication:http://research.microsoft.com"}, "The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems 1998 Claus, Boutilier" #cikm13_exp.core.Paper{:title "The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems", :abstract "Abstract Reinforcement   learning   can   provide   a   robust   and   natural means   for   agents   to   learn   how   to   coordinate   their   action choices in  multiagent  systems We  examine some of  the  fac - tors that can influence the dynamics of the learning process in such a setting We first distinguish reinforcement learners that are  unaware of  (or  ignore)  the  presence of  other  agents", :citation-count 362, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Claus", :first-name "Caroline", :native-name nil, :hindex 0, :gindex 0, :id 51376251, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Boutilier", :first-name "Craig", :native-name nil, :hindex 0, :gindex 0, :id 927981, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 33, :full-version-url ("http://www.aaai.org/Papers/AAAI/1998/AAAI98-106.pdf" "https://www.aaai.org/Papers/AAAI/1998/AAAI98-106.pdf" "http://www.informatik.uni-trier.de/~ley/db/conf/aaai/aaai98.html#ClausB98" "http://www.aaai.org/Papers/Workshops/1997/WS-97-03/WS97-03-003.pdf" "http://www.eecs.harvard.edu/~parkes/cs286r/spring06/papers/clausbout98.pdf" "http://opim.wharton.upenn.edu/~sok/papers/c/claus98dynamics.pdf" "http://www.cs.toronto.edu/kr/publications/multirl2.pdf"), :year 1998, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 21359, :name "Joint Action", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22470, :name "Learning Process", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 26421, :name "multiagent system", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 62996, :name "Nash Equilibria", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "National Conference on Artificial Intelligence", :short-name "AAAI", :cfp nil, :end-year 0, :start-year 0, :id 251, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 491212, :__type "Publication:http://research.microsoft.com"}, "Associative search network: A reinforcement learning associative memory 1981 Barto, Sutton, Brouwer" #cikm13_exp.core.Paper{:title "Associative search network: A reinforcement learning associative memory", :abstract "An associative memory system is presented which does not require a “teacher” to provide the desired associations. For each input key it conducts a search for the output pattern which optimizes an external payoff or reinforcement signal. The associative search network (ASN) combines pattern recognition and function optimization capabilities in a simple and effective way. We define the associative search", :citation-count 85, :author ({:citation-count 0, :middle-name "G.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Barto", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 633931, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "S.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Sutton", :first-name "Richard", :native-name nil, :hindex 0, :gindex 0, :id 359696, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "S.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Brouwer", :first-name "Peter", :native-name nil, :hindex 0, :gindex 0, :id 48717816, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Biological Cybernetics", :short-name "", :end-year 0, :issn nil, :start-year 0, :id 802, :__type "Journal:http://research.microsoft.com"}, :reference-count 24, :full-version-url ("http://www.springerlink.com/content/h6628843250k4p78" "http://www.springerlink.com/index/h6628843250k4p78.pdf" "http://www.springerlink.com/index/10.1007/BF00453370" "http://www.springerlink.com/index/pdf/10.1007/BF00453370" "http://webdocs.cs.ualberta.ca/~sutton/papers/BSB-81.pdf"), :year 1981, :doi "10.1007/BF00453370", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 2258, :name "Associative Memory", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7139, :name "Computer Simulation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15322, :name "Function Optimization", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 26203, :name "Motor Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 30407, :name "Pattern Recognition", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 1319782, :__type "Publication:http://research.microsoft.com"}, "Reinforcement Learning with Soft State Aggregation 1995 Jordan, Singh, Jaakkola" #cikm13_exp.core.Paper{:title "Reinforcement Learning with Soft State Aggregation", :abstract " It is widely accepted that the use of more compact representationsthan lookup tables is crucial to scaling reinforcement learning (RL)algorithms to real-world problems. Unfortunately almost all of thetheory of reinforcement learning assumes lookup table representations.In this paper we address the pressing issue of combiningfunction approximation and RL, and present 1) a function approximatorbased on a simple extension to state aggregation", :citation-count 133, :author ({:citation-count 0, :middle-name "I.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Jordan", :first-name "Michael", :native-name nil, :hindex 0, :gindex 0, :id 1135555, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "P.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Singh", :first-name "Satinder", :native-name nil, :hindex 0, :gindex 0, :id 306978, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Jaakkola", :first-name "Tommi", :native-name nil, :hindex 0, :gindex 0, :id 1820153, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 5, :full-version-url (), :year 1995, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 23488, :name "Lookup Table", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 258046, :__type "Publication:http://research.microsoft.com"}, "A Graph-Based Evolutionary Algorithm: Genetic Network Programming (GNP) and Its Extension Using Reinforcement Learning 2007 Mabu, Hirasawa, Hu" #cikm13_exp.core.Paper{:title "A Graph-Based Evolutionary Algorithm: Genetic Network Programming (GNP) and Its Extension Using Reinforcement Learning", :abstract "This paper proposes a graph-based evolutionary algorithm called Genetic Network Programming (GNP). Our goal is to develop GNP, which can deal with dynamic environments efficiently and effectively, based on the distinguished expression ability of the graph (network) structure. The characteristics of GNP are as follows. 1) GNP programs are composed of a number of nodes which execute simple judgment/processing, and", :citation-count 119, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Mabu", :first-name "Shingo", :native-name nil, :hindex 0, :gindex 0, :id 1534268, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Hirasawa", :first-name "Kotaro", :native-name nil, :hindex 0, :gindex 0, :id 3126948, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Hu", :first-name "Jinglu", :native-name nil, :hindex 0, :gindex 0, :id 2976412, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Evolutionary Computation", :short-name "EC", :end-year 0, :issn nil, :start-year 0, :id 325, :__type "Journal:http://research.microsoft.com"}, :reference-count 5, :full-version-url ("http://www.mitpressjournals.org/doi/abs/10.1162/evco.2007.15.3.369" "http://dx.doi.org/10.1162/evco.2007.15.3.369" "http://www.informatik.uni-trier.de/~ley/db/journals/ec/ec15.html#MabuHH07"), :year 2007, :doi "10.1162/evco.2007.15.3.369", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11182, :name "Dynamic Environment", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 13120, :name "Evolutionary Algorithm", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 13128, :name "Evolutionary Computing", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 19270, :name "Implicit Memory", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 27453, :name "Network Structure", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 37812, :name "Simulation Environment", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 55842, :name "Genetic Network Programming", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 4377625, :__type "Publication:http://research.microsoft.com"}, "Feudal Reinforcement Learning 1992 Dayan, Hinton" #cikm13_exp.core.Paper{:title "Feudal Reinforcement Learning", :abstract "One way to speed up reinforcement learning is to enable learn- ing to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a -learning managerial hierarchy in which high level managers learn how to set tasks to their sub-managers who, in turn, learn how to satisfy them. Sub-managers need not initially understand their managers'", :citation-count 133, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Dayan", :first-name "Peter", :native-name nil, :hindex 0, :gindex 0, :id 1576418, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "E.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Hinton", :first-name "Geoffrey", :native-name nil, :hindex 0, :gindex 0, :id 1143392, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 13, :full-version-url ("http://www.cs.utoronto.ca/%7Ehinton/absps/dh93.pdf" "http://nips.djvuzone.org/djvu/nips05/0271.djvu" "http://www.informatik.uni-trier.de/~ley/db/conf/nips/nips1992.html#DayanH92"), :year 1992, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 36247, :name "Satisfiability", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 561130, :__type "Publication:http://research.microsoft.com"}, "Generalization in Reinforcement Learning: Safely Approximating the Value Function 1994 Boyan, Moore" #cikm13_exp.core.Paper{:title "Generalization in Reinforcement Learning: Safely Approximating the Value Function", :abstract "", :citation-count 176, :author ({:citation-count 0, :middle-name "A.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Boyan", :first-name "Justin", :native-name nil, :hindex 0, :gindex 0, :id 1042087, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "W.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Moore", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 1802181, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 14, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/conf/nips/nips1994.html#BoyanM94" "http://nips.djvuzone.org/djvu/nips07/0369.djvu"), :year 1994, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43806, :name "Value Function", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 560893, :__type "Publication:http://research.microsoft.com"}, "Residual Algorithms: Reinforcement Learning with Function Approximation 1995 III" #cikm13_exp.core.Paper{:title "Residual Algorithms: Reinforcement Learning with Function Approximation", :abstract "A number of reinforcement learning algorithms have been developed that are guaranteed to converge to the optimal solution when used with lookup tables. It is shown, however, that these algorithms can easily become unstable when implemented directly with a general function-approximation system, such as a sigmoidal multilayer perceptron, a radial-basis- function system, a memory-based learning system, or even a linear", :citation-count 267, :author ({:citation-count 0, :middle-name "C. Baird", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "III", :first-name "Leemon", :native-name nil, :hindex 0, :gindex 0, :id 6826440, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 10, :full-version-url ("http://reference.kfupm.edu.sa/content/r/e/residual_algorithms__reinforcement_learn_572422.pdf"), :year 1995, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15320, :name "Function Approximation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 16116, :name "Generating Function", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22924, :name "linear functionals", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 23488, :name "Lookup Table", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 24890, :name "Memory Based Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 26507, :name "Multilayer Perceptron", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29387, :name "Optimal Solution", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 34031, :name "Radial Basis Function", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 41788, :name "Theoretical Analysis", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43808, :name "Value Iteration", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 55367, :name "Gradient Descent", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 62234, :name "Mean Square", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Machine Learning", :short-name "ICML", :cfp nil, :end-year 0, :start-year 0, :id 35, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 307459, :__type "Publication:http://research.microsoft.com"}, "Introduction to Reinforcement Learning 1998 Sutton, Barto" #cikm13_exp.core.Paper{:title "Introduction to Reinforcement Learning", :abstract "", :citation-count 189, :author ({:citation-count 0, :middle-name "S.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Sutton", :first-name "R.", :native-name nil, :hindex 0, :gindex 0, :id 359696, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "G.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Barto", :first-name "A.", :native-name nil, :hindex 0, :gindex 0, :id 633931, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 0, :full-version-url (), :year 1998, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 67617, :name "reinforce ment learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 1993401, :__type "Publication:http://research.microsoft.com"}, "Apprenticeship learning via inverse reinforcement learning 2004 Abbeel, Ng" #cikm13_exp.core.Paper{:title "Apprenticeship learning via inverse reinforcement learning", :abstract "We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying", :citation-count 118, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Abbeel", :first-name "Pieter", :native-name nil, :hindex 0, :gindex 0, :id 504868, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "Y.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Ng", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 838823, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 14, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/conf/icml/icml2004.html#PieterN04" "http://portal.acm.org/citation.cfm?id=1015430" "http://doi.acm.org/10.1145/1015330.1015430" "http://portal.acm.org/citation.cfm?id=1015330.1015430"), :year 2004, :doi "10.1145/1015330.1015430", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 58516, :name "Inverse Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 61196, :name "Markov Decision Process", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Machine Learning", :short-name "ICML", :cfp nil, :end-year 0, :start-year 0, :id 35, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 908346, :__type "Publication:http://research.microsoft.com"}, "Effective Reinforcement Learning for Mobile Robots 2002 Smart, Kaelbling" #cikm13_exp.core.Paper{:title "Effective Reinforcement Learning for Mobile Robots", :abstract "Programming mobile robots can be a long, time-consuming process. Specifying the low-level map- ping from sensors to actuators is prone to programmer misconceptions, and debugging such a mapping can be tedious. The idea of having a robot learn how to ac- complish a task, rather than being told explicitly is an appealing one. It seems easier and much more intuitive", :citation-count 123, :author ({:citation-count 0, :middle-name "D.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Smart", :first-name "William", :native-name nil, :hindex 0, :gindex 0, :id 613568, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "Pack", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Kaelbling", :first-name "Leslie", :native-name nil, :hindex 0, :gindex 0, :id 289339, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 16, :full-version-url ("http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=01014237" "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1014237" "http://www.cse.wustl.edu/~wds/papers/2002/icra2002.pdf" "http://www.cs.wustl.edu/~wds/papers/2002/icra2002.pdf" "http://people.csail.mit.edu/lpk/papers/2002/SmartKaelbling-ICRA2002.pdf" "http://www.informatik.uni-trier.de/~ley/db/conf/icra/icra2002.html#SmartK02" "http://people.csail.mit.edu/lpk/papers/icra2002.pdf" "http://www.sci.brooklyn.cuny.edu/~sklar/teaching/f06/air/papers/smart-icra2002.pdf" "http://www.cs.uml.edu/%7eholly/91.549/readings/smart-icra2002.pdf"), :year 2002, :doi "10.1109/ROBOT.2002.1014237", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22440, :name "Learning From Demonstration", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 23782, :name "Machine Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 25663, :name "Mobile Robot", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35860, :name "Robot Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 67614, :name "rein forcement learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Robotics and Automation", :short-name "ICRA", :cfp nil, :end-year 0, :start-year 0, :id 38, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 536236, :__type "Publication:http://research.microsoft.com"}, "Reinforcement, expectancy, and learning 1972 Bolles" #cikm13_exp.core.Paper{:title "Reinforcement, expectancy, and learning", :abstract "A review of recent evidence indicates that contingent reinforcement is neither a necessary nor a sufficient condition for operant learning. This dilemma reopens the old question of \"what is learned.\" It is proposed that what laboratory Ss characteristically learn is not a response to a stimulus, but rather 2 kinds of expectancies. 1 kind of expectancy corresponds rather accurately with", :citation-count 305, :author ({:citation-count 0, :middle-name "C.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Bolles", :first-name "Robert", :native-name nil, :hindex 0, :gindex 0, :id 448501, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Psychological Review", :short-name "PSYCHOL REV", :end-year 0, :issn nil, :start-year 0, :id 9936, :__type "Journal:http://research.microsoft.com"}, :reference-count 0, :full-version-url ("http://content.apa.org/journals/rev/79/5/394" "http://dx.doi.org/10.1037/h0033120"), :year 1972, :doi "10.1037/h0033120", :keyword (), :conference nil, :type 1, :citation-context (), :id 36922105, :__type "Publication:http://research.microsoft.com"}, "Programming Robots Using Reinforcement Learning and Teaching 1991 Lin" #cikm13_exp.core.Paper{:title "Programming Robots Using Reinforcement Learning and Teaching", :abstract "", :citation-count 75, :author ({:citation-count 0, :middle-name "Ji", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Lin", :first-name "Long", :native-name nil, :hindex 0, :gindex 0, :id 23911791, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 0, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/conf/aaai/aaai91-2.html#Lin91"), :year 1991, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "National Conference on Artificial Intelligence", :short-name "AAAI", :cfp nil, :end-year 0, :start-year 0, :id 251, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 490136, :__type "Publication:http://research.microsoft.com"}, "Near-Optimal Reinforcement Learning in Polynominal Time 1998 Kearns, Singh" #cikm13_exp.core.Paper{:title "Near-Optimal Reinforcement Learning in Polynominal Time", :abstract " We present new algorithms for reinforcement learning, and prove that they have polynomial bounds on the resources required to achieve near-optimal return in general Markov decision processes. After observing that the number of actions required to approach the optimal return is lower bounded by the mixing time T of the optimal policy (in the undiscounted case) or by the horizon", :citation-count 171, :author ({:citation-count 0, :middle-name "J.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Kearns", :first-name "Michael", :native-name nil, :hindex 0, :gindex 0, :id 299153, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "P.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Singh", :first-name "Satinder", :native-name nil, :hindex 0, :gindex 0, :id 306978, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 14, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/conf/icml/icml1998.html#KearnsS98"), :year 1998, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 25565, :name "Mixing Time", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29374, :name "Optimal Policy", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 59469, :name "Lower Bound", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 61196, :name "Markov Decision Process", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Machine Learning", :short-name "ICML", :cfp nil, :end-year 0, :start-year 0, :id 35, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 531231, :__type "Publication:http://research.microsoft.com"}, "Moore Reinforcement Learning: A Survey 1996 Kaelbling, Littman" #cikm13_exp.core.Paper{:title "Moore Reinforcement Learning: A Survey", :abstract "", :citation-count 70, :author ({:citation-count 0, :middle-name "P.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Kaelbling", :first-name "L.", :native-name nil, :hindex 0, :gindex 0, :id 289339, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "L.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Littman", :first-name "M.", :native-name nil, :hindex 0, :gindex 0, :id 171434, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Journal of Artificial Intelligence Research", :short-name "JAIR", :end-year 0, :issn nil, :start-year 0, :id 93, :__type "Journal:http://research.microsoft.com"}, :reference-count 0, :full-version-url (), :year 1996, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 2093585, :__type "Publication:http://research.microsoft.com"}, "Reinforcement learning is direct adaptive optimal control 1992 Sutton, Barto, Williams" #cikm13_exp.core.Paper{:title "Reinforcement learning is direct adaptive optimal control", :abstract "Neural network reinforcement learning methods are described and considered as a direct approach to adaptive optimal control of nonlinear systems. These methods have their roots in studies of animal learning and in early learning control work. An emerging deeper understanding of these methods is summarized that is obtained by viewing them as a synthesis of dynamic programming and stochastic approximation", :citation-count 89, :author ({:citation-count 0, :middle-name "S.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Sutton", :first-name "R.", :native-name nil, :hindex 0, :gindex 0, :id 359696, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "G.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Barto", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 633931, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "J.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Williams", :first-name "Ronald", :native-name nil, :hindex 0, :gindex 0, :id 827014, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "IEEE Control Systems Magazine", :short-name "IEEE CONTROL SYST MAG", :end-year 0, :issn nil, :start-year 0, :id 5310, :__type "Journal:http://research.microsoft.com"}, :reference-count 36, :full-version-url ("http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=126844" "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=00126844" "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4791776" "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04791776" "http://research.microsoft.com/apps/pubs/default.aspx?id=66644"), :year 1992, :doi "10.1109/37.126844", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 704, :name "Adaptive Optimization", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11245, :name "Dynamic Program", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22425, :name "Learning Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22481, :name "Learning System", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 28257, :name "Nonlinear System", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39996, :name "Stochastic Approximation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 63512, :name "Neural Network", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 689, :name "Adaptive Method", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7878, :name "Control Method", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7884, :name "Control Problem", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29345, :name "Optimal Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29347, :name "Optimal Control Problem", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 41216, :name "System Modeling", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 70601, :name "Self Tuning Regulator", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 153660, :__type "Publication:http://research.microsoft.com"}, "Automatic Programming of Behavior-Based Robots Using Reinforcement Learning 1991 Mahadevan, Connell" #cikm13_exp.core.Paper{:title "Automatic Programming of Behavior-Based Robots Using Reinforcement Learning", :abstract " This paper describes a general approach for automatically programming a behavior-based robot. New behaviors are learned by trial and error using a performance feedback function as reinforcement. Two algorithms for behavior learning are described that combine Q learning, a well known scheme for propagating reinforcement values temporally across actions, with statistical clustering and Hamming distance, two ways of propagating reinforcement", :citation-count 431, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Mahadevan", :first-name "Sridhar", :native-name nil, :hindex 0, :gindex 0, :id 905886, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Connell", :first-name "Jonathan", :native-name nil, :hindex 0, :gindex 0, :id 706455, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 2, :full-version-url ("http://linkinghub.elsevier.com/retrieve/pii/0004370292900586" "http://www.informatik.uni-trier.de/~ley/db/journals/ai/ai55.html#MahadevanC92"), :year 1991, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 2631, :name "Automatic Programming", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 3148, :name "behavior-based robotics", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 17351, :name "Hamming Distance", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "National Conference on Artificial Intelligence", :short-name "AAAI", :cfp nil, :end-year 0, :start-year 0, :id 251, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 66235, :__type "Publication:http://research.microsoft.com"}, "The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-spaces 1995 Moore, Atkeson" #cikm13_exp.core.Paper{:title "The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-spaces", :abstract " . Parti-game is a new algorithm for learning feasible trajectories to goal regions inhigh dimensional continuous state-spaces. In high dimensions it is essential that learning does notplan uniformly over a state-space. Parti-game maintains a decision-tree partitioning of state-spaceand applies techniques from game-theoryand computational geometry to efficiently and adaptivelyconcentrate high resolution only on critical areas. The current version of the algorithm", :citation-count 143, :author ({:citation-count 0, :middle-name "W.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Moore", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 1802181, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "G.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Atkeson", :first-name "Christopher", :native-name nil, :hindex 0, :gindex 0, :id 2262019, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Machine Learning", :short-name "ML", :end-year 0, :issn nil, :start-year 0, :id 157, :__type "Journal:http://research.microsoft.com"}, :reference-count 30, :full-version-url ("http://www.springerlink.com/index/10.1007/BF00993591" "http://www.springerlink.com/index/t030283843228663.pdf" "http://www.springerlink.com/content/t030283843228663" "http://www.springerlink.com/index/pdf/10.1007/BF00993591"), :year 1995, :doi "10.1007/BF00993591", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7006, :name "Computational Geometry", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 8755, :name "Curse of Dimensionality", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 18065, :name "High Dimension", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 18117, :name "High Resolution", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 21493, :name "kd tree", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39753, :name "State Space", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 52435, :name "Decision Tree", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 364074, :__type "Publication:http://research.microsoft.com"}, "Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents 1993 Tan" #cikm13_exp.core.Paper{:title "Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents", :abstract " Intelligent human agents exist in a cooperativesocial environment that facilitateslearning. They learn not only by trialand-error, but also through cooperation bysharing instantaneous information, episodicexperience, and learned knowledge. Thekey investigations of this paper are, &amp;quot;Giventhe same number of reinforcement learningagents, will cooperative agents outperformindependent agents who do not communicateduring learning?&amp;quot; and &amp;quot;What is the pricefor such cooperation?&amp;quot; Using...", :citation-count 344, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Tan", :first-name "Ming", :native-name nil, :hindex 0, :gindex 0, :id 55032274, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 3, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/conf/icml/icml1993.html#Tan93"), :year 1993, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 8006, :name "Cooperative Agents", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 60841, :name "Multi Agent Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Machine Learning", :short-name "ICML", :cfp nil, :end-year 0, :start-year 0, :id 35, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 530779, :__type "Publication:http://research.microsoft.com"}, "Improving Elevator Performance Using Reinforcement Learning 1995 Crites, Barto" #cikm13_exp.core.Paper{:title "Improving Elevator Performance Using Reinforcement Learning", :abstract " This paper describes the application of reinforcement learning (RL)to the difficult real world problem of elevator dispatching. The elevatordomain poses a combination of challenges not seen in mostRL research to date. Elevator systems operate in continuous statespaces and in continuous time as discrete event dynamic systems.Their states are not fully observable and they are nonstationarydue to changing passenger arrival rates.", :citation-count 312, :author ({:citation-count 0, :middle-name "H.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Crites", :first-name "Robert", :native-name nil, :hindex 0, :gindex 0, :id 1003196, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "G.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Barto", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 633931, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 2, :full-version-url (), :year 1995, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7806, :name "Continuous Time", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 10298, :name "Discrete Event Dynamic System", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 362822, :__type "Publication:http://research.microsoft.com"}, "Reinforcement Learning for Robots Using Neural Networks 1993 Lin" #cikm13_exp.core.Paper{:title "Reinforcement Learning for Robots Using Neural Networks", :abstract "", :citation-count 149, :author ({:citation-count 0, :middle-name "J", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Lin", :first-name "L.", :native-name nil, :hindex 0, :gindex 0, :id 23911791, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 0, :full-version-url (), :year 1993, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 63512, :name "Neural Network", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 1973060, :__type "Publication:http://research.microsoft.com"}, "Multiagent Reinforcement Learning in the Iterated Prisoner's Dilemma 1995 Sandholm, Crites" #cikm13_exp.core.Paper{:title "Multiagent Reinforcement Learning in the Iterated Prisoner's Dilemma", :abstract " Reinforcement learning (RL) is based on the idea that the tendency to producean action should be strengthened (reinforced) if it produces favorable results, andweakened if it produces unfavorable results. Q-learning is a recent RL algorithmthat does not need a model of its environment and can be used on-line. Thereforeit is well-suited for use in repeated games against an unknown opponent.", :citation-count 135, :author ({:citation-count 0, :middle-name "W.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Sandholm", :first-name "Tuomas", :native-name nil, :hindex 0, :gindex 0, :id 2502031, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "H.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Crites", :first-name "Robert", :native-name nil, :hindex 0, :gindex 0, :id 42194166, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Biosystems", :short-name "", :end-year 0, :issn nil, :start-year 0, :id 803, :__type "Journal:http://research.microsoft.com"}, :reference-count 19, :full-version-url (), :year 1995, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 21232, :name "iterated prisoner's dilemma", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35184, :name "Repeated Game", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 207789, :__type "Publication:http://research.microsoft.com"}, "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems 1995 Jordan, Singh, Jaakkola" #cikm13_exp.core.Paper{:title "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems", :abstract " Increasing attention has been paid to reinforcement learning algorithmsin recent years, partly due to successes in the theoreticalanalysis of their behavior in Markov environments. If the Markovassumption is removed, however, neither generally the algorithmsnor the analyses continue to be usable. We propose and analyzea new learning algorithm to solve a certain class of non-Markovdecision problems. Our algorithm applies to problems", :citation-count 145, :author ({:citation-count 0, :middle-name "I.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Jordan", :first-name "Michael", :native-name nil, :hindex 0, :gindex 0, :id 1135555, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "P.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Singh", :first-name "Satinder", :native-name nil, :hindex 0, :gindex 0, :id 306978, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Jaakkola", :first-name "Tommi", :native-name nil, :hindex 0, :gindex 0, :id 1820153, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 1, :full-version-url (), :year 1995, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22409, :name "Learning Algorithm", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 61195, :name "Markov Decision Problem", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 321348, :__type "Publication:http://research.microsoft.com"}, "On the Computational Economics of Reinforcement Learning 1990 Barto" #cikm13_exp.core.Paper{:title "On the Computational Economics of Reinforcement Learning", :abstract " Following terminology used in adaptive control,we distinguish between indirect learningmethods, which learn explicit models of thedynamic structure of the system to be controlled,and direct learning methods, whichdo not. We compare an existing indirectmethod, which uses a conventional dynamicprogramming algorithm, with a closely relateddirect reinforcement learning methodby applying both methods to an infinite horizonMarkov decision problem with unknownstate-transition...", :citation-count 23, :author ({:citation-count 0, :middle-name "G.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Barto", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 633931, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 1, :full-version-url (), :year 1990, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 637, :name "Adaptive Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 6996, :name "Computational Economics", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 9198, :name "Decision Problem", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22449, :name "Learning Methods", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 175080, :__type "Publication:http://research.microsoft.com"}, "Genetic reinforcement learning through symbiotic evolution for fuzzy controller design 2000 Juang, Lin, Lin" #cikm13_exp.core.Paper{:title "Genetic reinforcement learning through symbiotic evolution for fuzzy controller design", :abstract "An efficient genetic reinforcement learning algorithm for designing fuzzy controllers is proposed in this paper. The ge- netic algorithm (GA) adopted in this paper is based upon symbiotic evolution which, when applied to fuzzy controller design, comple- ments the local mapping property of a fuzzy rule. Using this Symbi- otic-Evolution-based Fuzzy Controller (SEFC) design method, the number of control trials,", :citation-count 121, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Juang", :first-name "Chia-feng", :native-name nil, :hindex 0, :gindex 0, :id 264968, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Lin", :first-name "Jiann-yow", :native-name nil, :hindex 0, :gindex 0, :id 2463404, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Lin", :first-name "Chin-teng", :native-name nil, :hindex 0, :gindex 0, :id 1115114, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "IEEE Transactions on Systems, Man, and Cybernetics", :short-name "TSMC", :end-year 0, :issn nil, :start-year 0, :id 28, :__type "Journal:http://research.microsoft.com"}, :reference-count 33, :full-version-url ("http://ir.lib.nctu.edu.tw/bitstream/987654321/1550/1/020109072.pdf" "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=00836377" "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=836377" "http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=836377" "http://www.informatik.uni-trier.de/~ley/db/journals/tsmc/tsmcb30.html#JuangLL00"), :year 2000, :doi "10.1109/3477.836377", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7884, :name "Control Problem", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 9584, :name "Design Method", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15487, :name "Fuzzy Controller", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15557, :name "Fuzzy Rules", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15559, :name "Fuzzy Set", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15563, :name "Fuzzy System", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 16236, :name "Genetics", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 19482, :name "Indexing Terms", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22409, :name "Learning Algorithm", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22913, :name "Linear Equations", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 23859, :name "Magnetic Levitation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 24831, :name "Membership Function", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 41552, :name "Temperature Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 55189, :name "ge netic algorithm", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 806965, :__type "Publication:http://research.microsoft.com"}, "Reinforcement Learning: A Survey 1996 Kaelbling, Littman, Moore" #cikm13_exp.core.Paper{:title "Reinforcement Learning: A Survey", :abstract "This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the eld and a broad selection of current work are summarized.Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work", :citation-count 2060, :author ({:citation-count 0, :middle-name "Pack", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Kaelbling", :first-name "Leslie", :native-name nil, :hindex 0, :gindex 0, :id 289339, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "L.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Littman", :first-name "Michael", :native-name nil, :hindex 0, :gindex 0, :id 171434, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "W.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Moore", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 1802181, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Journal of Artificial Intelligence Research", :short-name "JAIR", :end-year 0, :issn nil, :start-year 0, :id 93, :__type "Journal:http://research.microsoft.com"}, :reference-count 122, :full-version-url ("http://arxiv.org/abs/cs/9605103" "http://arxiv.org/abs/cs.AI/9605103" "http://www.informatik.uni-trier.de/~ley/db/journals/corr/corr9605.html#cs-AI-9605103"), :year 1996, :doi "10.1613/jair.301", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 9218, :name "Decision Theory", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11182, :name "Dynamic Environment", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 12224, :name "Empirical Model", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 23782, :name "Machine Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 137, :__type "Publication:http://research.microsoft.com"}, "Genetic Reinforcement Learning for Neurocontrol Problems 1993 Whitley" #cikm13_exp.core.Paper{:title "Genetic Reinforcement Learning for Neurocontrol Problems", :abstract "Empirical tests indicate that at least one class of genetic algorithms yields good performance for neural network weight optimization in terms of learning rates and scalability. The successful application of these genetic algorithms to supervised learning problems sets the stage for the use of genetic algorithms in reinforcement learn- ing problems. On a simulated inverted-pendulum control problem, \"genetic reinforcement learning\"", :citation-count 100, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Whitley", :first-name "Darrell", :native-name nil, :hindex 0, :gindex 0, :id 1328499, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Machine Learning", :short-name "ML", :end-year 0, :issn nil, :start-year 0, :id 157, :__type "Journal:http://research.microsoft.com"}, :reference-count 37, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/journals/ml/ml13.html#Whitleya93" "http://maircrosoft.com/moofle/papers/genetic.pdf"), :year 1993, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 637, :name "Adaptive Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7884, :name "Control Problem", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 16135, :name "Genetic Algorithm", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 16236, :name "Genetics", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 20986, :name "Inverted Pendulum", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22472, :name "Learning Rate", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 40796, :name "Supervised Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 63512, :name "Neural Network", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 67615, :name "Reinforcement Learn Ing", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 71127, :name "Temporal Difference", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 709030, :__type "Publication:http://research.microsoft.com"}, "Finding Structure in Reinforcement Learning 1995 Thrun" #cikm13_exp.core.Paper{:title "Finding Structure in Reinforcement Learning", :abstract " Reinforcement learning addresses the problem of learning to select actions in order tomaximize one's performance in unknown environments. To scale reinforcement learningto complex real-world tasks, such as typically studied in AI, one must ultimately be ableto discover the structure in the world, in order to abstract away the myriad of details andto operate in more tractable problem spaces.This paper presents", :citation-count 100, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Thrun", :first-name "Sebastian", :native-name nil, :hindex 0, :gindex 0, :id 361346, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 14, :full-version-url (), :year 1995, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 331431, :__type "Publication:http://research.microsoft.com"}, "Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching 1992 Lin" #cikm13_exp.core.Paper{:title "Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching", :abstract "To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two- fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up", :citation-count 317, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Lin", :first-name "Long-Ji", :native-name nil, :hindex 0, :gindex 0, :id 23911791, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Machine Learning", :short-name "ML", :end-year 0, :issn nil, :start-year 0, :id 157, :__type "Journal:http://research.microsoft.com"}, :reference-count 31, :full-version-url ("http://www.springerlink.com/content/w080373g15u2g84v" "http://www.springerlink.com/index/w080373g15u2g84v.pdf" "http://webdocs.cs.ualberta.ca/~sutton/lin-92.pdf" "http://www.springerlink.com/index/10.1007/BF00992699" "http://www.springerlink.com/index/pdf/10.1007/BF00992699" "http://www.informatik.uni-trier.de/~ley/db/journals/ml/ml8.html#Lin92"), :year 1992, :doi "10.1007/BF00992699", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11182, :name "Dynamic Environment", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 12217, :name "Empirical Evaluation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 45886, :name "Agent Based", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 46494, :name "Adaptive Heuristic Critic", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 708844, :__type "Publication:http://research.microsoft.com"}, "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning 2002 Brafman, Tennenholtz" #cikm13_exp.core.Paper{:title "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning", :abstract "", :citation-count 117, :author ({:citation-count 0, :middle-name "I.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Brafman", :first-name "Ronen", :native-name nil, :hindex 0, :gindex 0, :id 732881, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Tennenholtz", :first-name "Moshe", :native-name nil, :hindex 0, :gindex 0, :id 146116, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Journal of Machine Learning Research", :short-name "JMLR", :end-year 0, :issn nil, :start-year 0, :id 126, :__type "Journal:http://research.microsoft.com"}, :reference-count 0, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/journals/jmlr/jmlr3.html#BrafmanT02" "http://www.jmlr.org/papers/v3/brafman02a.html"), :year 2002, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 31835, :name "Polynomial Time Algorithm", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 794538, :__type "Publication:http://research.microsoft.com"}, "A Reinforcement Learning Approach to Job-Shop Scheduling 1995 Zhang, Dietterich" #cikm13_exp.core.Paper{:title "A Reinforcement Learning Approach to Job-Shop Scheduling", :abstract " We apply reinforcement learning methods tolearn domain-specific heuristics for job shopscheduling. A repair-based scheduler startswith a critical-path schedule and incrementallyrepairs constraint violations with the goal offinding a short conflict-free schedule. The temporaldifference algorithm TD() is appliedto train a neural network to learn a heuristicevaluation function over states. This evaluationfunction is used by a one-step lookaheadsearch procedure to find good solutions", :citation-count 167, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Zhang", :first-name "Wei", :native-name nil, :hindex 0, :gindex 0, :id 21638717, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "G.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Dietterich", :first-name "Thomas", :native-name nil, :hindex 0, :gindex 0, :id 1622575, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 6, :full-version-url (), :year 1995, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 8498, :name "Critical Path", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 10865, :name "Domain Specificity", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 21353, :name "Job Shop Scheduling", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 63512, :name "Neural Network", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Joint Conference on Artificial Intelligence", :short-name "IJCAI", :cfp nil, :end-year 0, :start-year 0, :id 64, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 328437, :__type "Publication:http://research.microsoft.com"}, "Instance-Based Utile Distinctions for Reinforcement Learning with Hidden State 1995 Mccallum" #cikm13_exp.core.Paper{:title "Instance-Based Utile Distinctions for Reinforcement Learning with Hidden State", :abstract "We present Utile Suffix Memory , a reinforcement learning algorithm that uses short-term memory to overcome the state aliasing that results from hidden state. By combining the advantages of previous work in instance-based (or \"memory- based\") learning and previous work with statisti- cal tests for separating noise from task structure, the method learns quickly, creates only as much memory as", :citation-count 77, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Mccallum", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 2232623, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 24, :full-version-url ("http://reference.kfupm.edu.sa/content/i/n/instance_based_utile_distinctions_for_re_1525.pdf"), :year 1995, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11245, :name "Dynamic Program", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 24890, :name "Memory Based Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 37586, :name "Short Term Memory", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 40676, :name "Suffix Tree", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 42903, :name "Tree Structure", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Machine Learning", :short-name "ICML", :cfp nil, :end-year 0, :start-year 0, :id 35, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 340544, :__type "Publication:http://research.microsoft.com"}, "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition 2000 Dietterich" #cikm13_exp.core.Paper{:title "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition", :abstract "This paper presents a new approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The decomposition, known as the MAXQ decomposition, has both a procedural semantics—as a subroutine", :citation-count 380, :author ({:citation-count 0, :middle-name "G.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Dietterich", :first-name "Thomas", :native-name nil, :hindex 0, :gindex 0, :id 1622575, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Journal of Artificial Intelligence Research", :short-name "JAIR", :end-year 0, :issn nil, :start-year 0, :id 93, :__type "Journal:http://research.microsoft.com"}, :reference-count 36, :full-version-url ("http://www.cs.washington.edu/research/jair/abstracts/dietterich00a.html" "http://arxiv.org/abs/cs/9905014"), :year 2000, :doi "10.1613/jair.639", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22409, :name "Learning Algorithm", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29374, :name "Optimal Policy", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 31651, :name "Policy Iteration", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39753, :name "State Space", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43806, :name "Value Function", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 61196, :name "Markov Decision Process", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 1911, :__type "Publication:http://research.microsoft.com"}, "Relational Reinforcement Learning 2001 Driessens, Raedt" #cikm13_exp.core.Paper{:title "Relational Reinforcement Learning", :abstract "Relational reinforcement learning is presented, a learning technique that combines reinforcement learning with relational learning or inductive logic programming. Due to the use of a more expressive representation language to represent states, actions and Q-functions, relational reinforcement learning can be potentially applied to a new range of learning tasks. One such task that we investigate is planning in the blocks", :citation-count 112, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Driessens", :first-name "Kurt", :native-name nil, :hindex 0, :gindex 0, :id 1410464, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "De", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Raedt", :first-name "Luc", :native-name nil, :hindex 0, :gindex 0, :id 1195621, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Machine Learning", :short-name "ML", :end-year 0, :issn nil, :start-year 0, :id 157, :__type "Journal:http://research.microsoft.com"}, :reference-count 36, :full-version-url ("http://www-ai.ijs.si/SasoDzeroski/files/2001_DRD_RelationalReinforcementLearning.pdf"), :year 2001, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 19633, :name "Inductive Logic Programming", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35020, :name "Relational Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 67950, :name "Relational Reinforcement Learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 86345, :__type "Publication:http://research.microsoft.com"}, "The Parti-Game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-Spaces 1993 Moore" #cikm13_exp.core.Paper{:title "The Parti-Game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-Spaces", :abstract " Parti-game is a new algorithm for learning from delayed rewards in highdimensional real-valued state-spaces. In high dimensions it is essential thatlearning does not explore or plan over state space uniformly. Parti-gamemaintains a decision-tree partitioning of state-space and applies game-theoryand computational geometry techniques to efficiently and reactively concentratehigh resolution only on critical areas. Many simulated problems havebeen tested, ranging from 2-dimensional", :citation-count 119, :author ({:citation-count 0, :middle-name "W.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Moore", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 1802181, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 22, :full-version-url (), :year 1993, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7006, :name "Computational Geometry", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 18065, :name "High Dimension", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39753, :name "State Space", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 45736, :name "2 dimensional", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 52435, :name "Decision Tree", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 255663, :__type "Publication:http://research.microsoft.com"}, "Behavior coordination for a mobile robot using modular reinforcement learning 1996 Uchibe, Asada, Hosoda" #cikm13_exp.core.Paper{:title "Behavior coordination for a mobile robot using modular reinforcement learning", :abstract "Coordination of multiple behaviors independently obtained by a reinforcement learning method is one of the issues in order for the method to be scaled to larger and more complex robot learning tasks. Direct combination of all the state spaces for individual modules (subtasks) needs enormous learning time, and it causes hidden states. This paper presents a method of modular learning", :citation-count 57, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Uchibe", :first-name "Eiji", :native-name nil, :hindex 0, :gindex 0, :id 762002, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Asada", :first-name "Minoru", :native-name nil, :hindex 0, :gindex 0, :id 2181945, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Hosoda", :first-name "Koh", :native-name nil, :hindex 0, :gindex 0, :id 87436, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 12, :full-version-url ("http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=00568989" "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=568989"), :year 1996, :doi "10.1109/IROS.1996.568989", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7139, :name "Computer Simulation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 19836, :name "Information Criterion", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 25663, :name "Mobile Robot", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 25742, :name "Model Fitting", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35860, :name "Robot Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39753, :name "State Space", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Intelligent RObots and Systems - IROS", :short-name "IROS", :cfp nil, :end-year 0, :start-year 0, :id 2821, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 299299, :__type "Publication:http://research.microsoft.com"}, "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning 2001 Brafman, Tennenholtz" #cikm13_exp.core.Paper{:title "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning", :abstract "R-max is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-max, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal", :citation-count 166, :author ({:citation-count 0, :middle-name "I.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Brafman", :first-name "Ronen", :native-name nil, :hindex 0, :gindex 0, :id 732881, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Tennenholtz", :first-name "Moshe", :native-name nil, :hindex 0, :gindex 0, :id 146116, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 22, :full-version-url ("http://iew3.technion.ac.il/~moshet/brafman02a.pdf" "http://iew3.technion.ac.il:8080/~moshet/brafman02a.pdf" "http://www.sci.brooklyn.cuny.edu/~parsons/courses/790-spring-2004/notes/rmax.pdf" "http://jmlr.csail.mit.edu/papers/volume3/brafman02a/brafman02a.pdf" "http://ie.technion.ac.il/~moshet/brafman02a.pdf" "http://www.cs.tufts.edu/~roni/Teaching/RL/Papers/RMAX.pdf"), :year 2001, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22444, :name "Learning In Games", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29374, :name "Optimal Policy", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29419, :name "Optimization Under Uncertainty", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 31834, :name "Polynomial Time", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 31835, :name "Polynomial Time Algorithm", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35184, :name "Repeated Game", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 40052, :name "Stochastic Games", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 61196, :name "Markov Decision Process", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Joint Conference on Artificial Intelligence", :short-name "IJCAI", :cfp nil, :end-year 0, :start-year 0, :id 64, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 132063, :__type "Publication:http://research.microsoft.com"}, "The neural basis of human error processing: Reinforcement learning, dopamine, and the error-related negativity 2002 Holroyd, Coles" #cikm13_exp.core.Paper{:title "The neural basis of human error processing: Reinforcement learning, dopamine, and the error-related negativity", :abstract "The authors present a unified account of 2 neural systems concerned with the development and expression of adaptive behaviors: a mesencephalic dopamine system for reinforcement learning and a \"generic\" error-processing system associated with the anterior cingulate cortex. The existence of the error-processing system has been inferred from the error-related negativity (ERN), a component of the event-related brain potential elicited when", :citation-count 832, :author ({:citation-count 0, :middle-name "B.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Holroyd", :first-name "Clay", :native-name nil, :hindex 0, :gindex 0, :id 1894517, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "G. H.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Coles", :first-name "Michael", :native-name nil, :hindex 0, :gindex 0, :id 2664174, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Psychological Review", :short-name "PSYCHOL REV", :end-year 0, :issn nil, :start-year 0, :id 9936, :__type "Journal:http://research.microsoft.com"}, :reference-count 208, :full-version-url ("http://dionysus.psych.wisc.edu/lit/articles/holroydc2002a.pdf" "http://web.uvic.ca/psyc/braincoglab/papers/2002_HolroydColes.pdf" "http://web.uvic.ca/psyc/grad.html/braincoglab/papers/2002_HolroydColes.pdf" "http://brainvitge.org/papers/Holroyd_&_Coles_2002.pdf" "http://www.cnbc.pitt.edu/ibsc/papers/HolroydColes2002.pdf" "http://cognitrn.psych.indiana.edu/busey/forjp/locid2004/errorrelatednegativity.pdf" "http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.109.4.679"), :year 2002, :doi "10.1037//0033-295X.109.4.679", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 631, :name "Adaptive Behavior", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 1709, :name "Anterior Cingulate Cortex", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 10898, :name "Dopamine", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 12835, :name "Error Processing", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 12847, :name "Error Related Negativity", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 13060, :name "Event Related Brain Potential", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 16020, :name "Generalization Error", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 18585, :name "Human Error", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 27526, :name "Neural System", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 68073, :name "Reaction Time", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 10799599, :__type "Publication:http://research.microsoft.com"}, "Policy Gradient Reinforcement Learning for Fast Quadrupedal Locomotion 2004 Kohl, Stone" #cikm13_exp.core.Paper{:title "Policy Gradient Reinforcement Learning for Fast Quadrupedal Locomotion", :abstract "This paper presents a machine learning approach to optimizing a quadrupedal trot gait for forward speed. Given a parameterized walk designed for a specific robot, we propose using a form of policy gradient reinforcement learning to automatically search the set of possible parameters with the goal of finding the fastest possible walk. We implement and test our approach on a", :citation-count 122, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Kohl", :first-name "Nate", :native-name nil, :hindex 0, :gindex 0, :id 1251724, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Stone", :first-name "Peter", :native-name nil, :hindex 0, :gindex 0, :id 563419, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 15, :full-version-url ("http://www.cs.utexas.edu/users/nate/pubs/papers/kohlicra04.pdf" "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1307456" "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=01307456" "http://www.cs.utexas.edu/~pstone/Papers/bib2html-links/icra04.pdf" "http://www.cs.utexas.edu/users/pstone/Papers/bib2html-links/icra04.pdf" "http://dx.doi.org/10.1109/ROBOT.2004.1307456" "http://www.sci.brooklyn.cuny.edu/~sklar/teaching/f06/air/papers/stone-learning.pdf" "http://www.cs.utexas.edu/~nate/pubs/papers/kohlicra04.pdf" "http://www.informatik.uni-trier.de/~ley/db/conf/icra/icra2004-3.html#KohlS04"), :year 2004, :doi "10.1109/ROBOT.2004.1307456", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22425, :name "Learning Control", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22577, :name "Legged Robot", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 23782, :name "Machine Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 44684, :name "Walking Robot", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Robotics and Automation", :short-name "ICRA", :cfp nil, :end-year 0, :start-year 0, :id 38, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 1967108, :__type "Publication:http://research.microsoft.com"}, "Vicarious reinforcement and imitative learning 1963 Bandura, Ross, Ross" #cikm13_exp.core.Paper{:title "Vicarious reinforcement and imitative learning", :abstract "The present experiment was designed to study the influence of response-consequences to the model on the imitative learning of aggression. Nursery school children were assigned randomly to 1 of the following groups: aggressive model-rewarded; aggressive model-punished; a control group shown highly expressive but nonaggressive models; and a 2nd control group which had no exposure to models. The children were then", :citation-count 126, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Bandura", :first-name "Albert", :native-name nil, :hindex 0, :gindex 0, :id 1464270, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Ross", :first-name "Dorothea", :native-name nil, :hindex 0, :gindex 0, :id 24401016, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "A.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Ross", :first-name "Sheila", :native-name nil, :hindex 0, :gindex 0, :id 53061460, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "The Journal of Abnormal and Social Psychology", :short-name "", :end-year 0, :issn nil, :start-year 0, :id 15271, :__type "Journal:http://research.microsoft.com"}, :reference-count 0, :full-version-url ("http://doi.apa.org/getdoi.cfm?doi=10.1037/h0045550" "http://dx.doi.org/10.1037/h0045550"), :year 1963, :doi "10.1037/h0045550", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 19172, :name "Imitation Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 36378, :name "School Children", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 49663, :name "Control Group", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 37031847, :__type "Publication:http://research.microsoft.com"}, "Ants and Reinforcement Learning: A Case Study in Routing in Dynamic Networks 1997 Subramanian, Druschel, Chen" #cikm13_exp.core.Paper{:title "Ants and Reinforcement Learning: A Case Study in Routing in Dynamic Networks", :abstract "We investigate two new distributed routing al­ gorithms for data networks based on simple bi­ ological \"ants\" that explore the network and rapidly learn good routes, using a novel varia­ tion of reinforcement learning. These two algo- rithms are fully adaptive to topology changes and changes in link costs in the network, and have space and computational overheads that are", :citation-count 134, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Subramanian", :first-name "Devika", :native-name nil, :hindex 0, :gindex 0, :id 1721901, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Druschel", :first-name "Peter", :native-name nil, :hindex 0, :gindex 0, :id 180454, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Chen", :first-name "Johnny", :native-name nil, :hindex 0, :gindex 0, :id 591035, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 4, :full-version-url ("http://dli.iiit.ac.in/ijcai/IJCAI-97-VOL2/PDF/006.pdf"), :year 1997, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 4931, :name "Case Study", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 7943, :name "Convergence Theorem", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 10371, :name "Discrete Time Markov Chain", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11224, :name "Dynamic Networks", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 12217, :name "Empirical Evaluation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 13644, :name "Failure Rate", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29870, :name "Packet Routing", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 36014, :name "Routing Algorithm", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Joint Conference on Artificial Intelligence", :short-name "IJCAI", :cfp nil, :end-year 0, :start-year 0, :id 64, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 362546, :__type "Publication:http://research.microsoft.com"}, "Temporal credit assignment in reinforcement learning 1984 Sutton" #cikm13_exp.core.Paper{:title "Temporal credit assignment in reinforcement learning", :abstract "", :citation-count 276, :author ({:citation-count 0, :middle-name "S.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Sutton", :first-name "R.", :native-name nil, :hindex 0, :gindex 0, :id 359696, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 0, :full-version-url (), :year 1984, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 1274763, :__type "Publication:http://research.microsoft.com"}, "Reinforcement Learning with Replacing Eligibility Traces 1996 Singh, Sutton" #cikm13_exp.core.Paper{:title "Reinforcement Learning with Replacing Eligibility Traces", :abstract " . The eligibility trace is one of the basic mechanisms used in reinforcement learningto handle delayed reward. In this paper we introduce a new kind of eligibility trace, the replacing trace, analyze it theoretically, and show that it results in faster, more reliable learningthan the conventional trace. Both kinds of trace assign credit to prior events according to howrecently they", :citation-count 227, :author ({:citation-count 0, :middle-name "P.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Singh", :first-name "Satinder", :native-name nil, :hindex 0, :gindex 0, :id 306978, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "S.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Sutton", :first-name "Richard", :native-name nil, :hindex 0, :gindex 0, :id 359696, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Machine Learning", :short-name "ML", :end-year 0, :issn nil, :start-year 0, :id 157, :__type "Journal:http://research.microsoft.com"}, :reference-count 37, :full-version-url ("http://www.springerlink.com/content/kq74821757xl4w4m" "http://www.springerlink.com/index/kq74821757xl4w4m.pdf" "http://www.springerlink.com/index/10.1007/BF00114726" "http://www.springerlink.com/index/pdf/10.1007/BF00114726" "http://www.informatik.uni-trier.de/~ley/db/journals/ml/ml22.html#SinghS96"), :year 1996, :doi "10.1007/BF00114726", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 26096, :name "Monte Carlo Method", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 41597, :name "Temporal Difference Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 60967, :name "Markov Chain", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 709250, :__type "Publication:http://research.microsoft.com"}, "A sensor-based navigation for a mobile robot using fuzzy logic and reinforcement learning 1995 Beom, Cho" #cikm13_exp.core.Paper{:title "A sensor-based navigation for a mobile robot using fuzzy logic and reinforcement learning", :abstract "The proposed navigator consists of an avoidance behavior and goal-seeking behavior. Two behaviors are independently designed at the design stage and then combined them by a behavior selector at the running stage. A behavior selector using a bistable switching function chooses a behavior at each action step so that the mobile robot can go for the goal position without colliding", :citation-count 134, :author ({:citation-count 0, :middle-name "Rak", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Beom", :first-name "Hee", :native-name nil, :hindex 0, :gindex 0, :id 2014444, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "Suck", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Cho", :first-name "Hyung", :native-name nil, :hindex 0, :gindex 0, :id 34116906, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "IEEE Transactions on Systems, Man, and Cybernetics", :short-name "TSMC", :end-year 0, :issn nil, :start-year 0, :id 28, :__type "Journal:http://research.microsoft.com"}, :reference-count 12, :full-version-url ("http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=364859" "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=364859" "http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=00364859"), :year 1995, :doi "10.1109/21.364859", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15513, :name "Fuzzy Logic", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15553, :name "Fuzzy Rule Base", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15557, :name "Fuzzy Rules", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15559, :name "Fuzzy Set", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 25663, :name "Mobile Robot", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 39753, :name "State Space", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43296, :name "Ultrasonic Sensor", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 58281, :name "Input Output", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 1496186, :__type "Publication:http://research.microsoft.com"}, "Generalization in Reinforcement Learning: Safely Approximating theValue Function 1995 Boyan, Moore" #cikm13_exp.core.Paper{:title "Generalization in Reinforcement Learning: Safely Approximating theValue Function", :abstract "", :citation-count 181, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Boyan", :first-name "Justin", :native-name nil, :hindex 0, :gindex 0, :id 1042087, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Moore", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 34066354, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 0, :full-version-url (), :year 1995, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43806, :name "Value Function", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 27812607, :__type "Publication:http://research.microsoft.com"}, "Reinforcement Learning with Selective Perception and Hidden State 1995 Mccallum" #cikm13_exp.core.Paper{:title "Reinforcement Learning with Selective Perception and Hidden State", :abstract "", :citation-count 213, :author ({:citation-count 0, :middle-name "K.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Mccallum", :first-name "A.", :native-name nil, :hindex 0, :gindex 0, :id 2232623, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 0, :full-version-url (), :year 1995, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 1973221, :__type "Publication:http://research.microsoft.com"}, "Generalization in Reinforcement Learning: Safely Approximating the Value Function 1995 " #cikm13_exp.core.Paper{:title "Generalization in Reinforcement Learning: Safely Approximating the Value Function", :abstract " To appear in: G. Tesauro, D. S. Touretzky and T. K. Leen, eds., Advances in NeuralInformation Processing Systems 7, MIT Press, Cambridge MA, 1995.A straightforward approach to the curse of dimensionality in reinforcementlearning and dynamic programming is to replace thelookup table with a generalizing function approximator such as a neuralnet. Although this has been successful in the domain of backgammon,there", :citation-count 193, :author (), :journal nil, :reference-count 15, :full-version-url (), :year 1995, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 8755, :name "Curse of Dimensionality", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11245, :name "Dynamic Program", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 16116, :name "Generating Function", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43806, :name "Value Function", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 348778, :__type "Publication:http://research.microsoft.com"}, "Gradient Descent fo r General Reinf orcement Learning 0 Baird, Moore" #cikm13_exp.core.Paper{:title "Gradient Descent fo r General Reinf orcement Learning", :abstract "A simple learning rule is derived, the VAPS algorithm, which can be instantiated to generate a wide range of new reinforcement- learning algorithms. These algorithms solve a number of open problems, define several new approaches to reinforcement learning, and unify different approaches to reinforcement learning under a single theory. These algorithms all have guaranteed convergence, and include modifications of several", :citation-count 93, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Baird", :first-name "Leemon", :native-name nil, :hindex 0, :gindex 0, :id 6826440, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Moore", :first-name "Andrew", :native-name nil, :hindex 0, :gindex 0, :id 1802181, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 7, :full-version-url ("http://www.ri.cmu.edu/pub_files/pub1/baird_leemon_1999_1/baird_leemon_1999_1.pdf" "http://reference.kfupm.edu.sa/content/g/r/gradient_descent_for_general_reinforceme_85333.pdf"), :year 0, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29374, :name "Optimal Policy", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 43806, :name "Value Function", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 55367, :name "Gradient Descent", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 1981454, :__type "Publication:http://research.microsoft.com"}, "Reinforcement Learning Applied to Linear Quadratic Regulation 1992 Bradtke" #cikm13_exp.core.Paper{:title "Reinforcement Learning Applied to Linear Quadratic Regulation", :abstract "", :citation-count 58, :author ({:citation-count 0, :middle-name "J.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Bradtke", :first-name "Steven", :native-name nil, :hindex 0, :gindex 0, :id 758092, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 4, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/conf/nips/nips1992.html#Bradtke92"), :year 1992, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11245, :name "Dynamic Program", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11283, :name "Dynamic System", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22924, :name "linear functionals", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 22967, :name "Linear Quadratic Regulator", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 560145, :__type "Publication:http://research.microsoft.com"}, "Packet Routing in Dynamically Changing Networks: A Reinforcement Learning Approach 1993 Boyan, Littman" #cikm13_exp.core.Paper{:title "Packet Routing in Dynamically Changing Networks: A Reinforcement Learning Approach", :abstract "This paper describes the Q-routing algorithm for packet routing, in which a reinforcement learning module is embedded into each node of a switching network. Only local communication is used by each node to keep accurate statistics on which routing decisions lead to minimal delivery times. In simple experiments involving a 36-node, irregularly connected network, Q-routing proves supe-rior to a nonadaptive", :citation-count 222, :author ({:citation-count 0, :middle-name "A.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Boyan", :first-name "Justin", :native-name nil, :hindex 0, :gindex 0, :id 1042087, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "L.", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Littman", :first-name "Michael", :native-name nil, :hindex 0, :gindex 0, :id 171434, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 7, :full-version-url ("http://www.informatik.uni-trier.de/~ley/db/conf/nips/nips1993.html#BoyanL93" "http://nips.djvuzone.org/djvu/nips06/0671.djvu"), :year 1993, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11155, :name "Dynamic Change", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 29870, :name "Packet Routing", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 23213, :name "Local Community", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 36014, :name "Routing Algorithm", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 41052, :name "Switching Network", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 69965, :name "Shortest Path", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "Neural Information Processing Systems", :short-name "NIPS", :cfp nil, :end-year 0, :start-year 0, :id 187, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 560892, :__type "Publication:http://research.microsoft.com"}, "Ant-Q: A Reinforcement Learning Approach to the Traveling Salesman Problem 1995 Gambardella, Dorigo" #cikm13_exp.core.Paper{:title "Ant-Q: A Reinforcement Learning Approach to the Traveling Salesman Problem", :abstract "In this paper we introduce Ant-Q, a family of algorithms which present many similarities with Q-learning (Watkins, 1989), and which we apply to the solution of symmetric and asym- metric instances of the traveling salesman prob- lem (TSP). Ant-Q algorithms were inspired by work on the ant system (AS), a distributed algo- rithm for combinatorial optimization based on the metaphor", :citation-count 191, :author ({:citation-count 0, :middle-name "Maria", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Gambardella", :first-name "Luca", :native-name nil, :hindex 0, :gindex 0, :id 1088591, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Dorigo", :first-name "Marco", :native-name nil, :hindex 0, :gindex 0, :id 1554161, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 19, :full-version-url ("https://eprints.kfupm.edu.sa/25980/1/25980.pdf" "http://www.informatik.uni-trier.de/~ley/db/conf/icml/icml1995.html#GambardellaD95" "http://www.idsia.ch/~luca/ml95.pdf" "http://staff.washington.edu/paymana/swarm/gambardella95-icml.pdf"), :year 1995, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 1680, :name "Ant Colony", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 6536, :name "Combinatorial Optimization", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 42855, :name "Traveling Salesman", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 42856, :name "Traveling Salesman Problem", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 47285, :name "Ant System", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 60379, :name "Local Search", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 63512, :name "Neural Network", :publication-count 0}), :conference {:citation-count 0, :publication-count 0, :research-interest-domain nil, :homepage-url nil, :full-name "International Conference on Machine Learning", :short-name "ICML", :cfp nil, :end-year 0, :start-year 0, :id 35, :__type "Conference:http://research.microsoft.com"}, :type 1, :citation-context (), :id 336810, :__type "Publication:http://research.microsoft.com"}, "Issues in Using Function Approximation for Reinforcement Learning 1993 Thrun, Schwartz" #cikm13_exp.core.Paper{:title "Issues in Using Function Approximation for Reinforcement Learning", :abstract "Reinforcement learning techniques address the problem of learning to select actions in unknown, dynamic environments. It is widely acknowledged that to be of use in complex domains, reinforcement learning techniques must be combined with generalizing function approximation methods such as artificial neural networks. Little, however, is understood about the theoretical properties of such combinations, and many researchers have encountered failures", :citation-count 70, :author ({:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Thrun", :first-name "Sebastian", :native-name nil, :hindex 0, :gindex 0, :id 361346, :affiliation nil, :__type "Author:http://research.microsoft.com"} {:citation-count 0, :middle-name "", :publication-count 0, :research-interest-domain nil, :homepage-url nil, :display-photo-url nil, :last-name "Schwartz", :first-name "Anton", :native-name nil, :hindex 0, :gindex 0, :id 9203524, :affiliation nil, :__type "Author:http://research.microsoft.com"}), :journal nil, :reference-count 22, :full-version-url ("http://reference.kfupm.edu.sa/content/i/s/issues_in_using_function_approximation_f_391938.pdf"), :year 1993, :doi "", :keyword ({:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 1915, :name "Approximation Method", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 2131, :name "Artificial Neural Network", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 11182, :name "Dynamic Environment", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 15320, :name "Function Approximation", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 16116, :name "Generating Function", :publication-count 0} {:__type "Keyword:http://research.microsoft.com", :citation-count 0, :id 35000, :name "Reinforcement Learning", :publication-count 0}), :conference nil, :type 1, :citation-context (), :id 392756, :__type "Publication:http://research.microsoft.com"}}, :main-topic #topic_maps.core.Topic{:kind :topic-maps.core/article, :title "reinforcement learning"}, :merged-topics {#topic_maps.core.Topic{:kind :topic-maps.core/category, :title "graph coloring"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "graph coloring"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "quantum field theory"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "quantum field theory"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational economics"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "computational economics"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "human–computer interaction"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "human–computer interaction"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "dynamical systems"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dynamical system"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "optimal decisions"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimal decision"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "necessity and sufficiency"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "necessity and sufficiency"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "dynamic programming"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "dynamic programming"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "evolutionary algorithms"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "evolutionary algorithm"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "machine learning"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "machine learning"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "nonlinear systems"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "nonlinear system"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "computational geometry"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "computational geometry"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "travelling salesman problem"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "travelling salesman problem"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "search algorithms"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "search algorithm"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "optimal control"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "optimal control"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "education"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "education"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "inductive logic programming"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "inductive logic programming"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "control theory"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "control theory"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "fuzzy logic"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "fuzzy logic"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "genetic algorithms"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "genetic algorithm"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "cognition"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "cognition"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "mathematical optimization"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "mathematical optimization"}}, #topic_maps.core.Topic{:kind :topic-maps.core/category, :title "routing"} #{#topic_maps.core.Topic{:kind :topic-maps.core/article, :title "routing"}}}}
