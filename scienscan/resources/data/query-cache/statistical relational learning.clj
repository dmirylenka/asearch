{:n-topics-max 307, :topic-index #utils.core.Success{:value {1770656 {:id 1770656, :title "Numerical linear algebra", :type :wiki-api.core/category}, 11737376 {:id 11737376, :title "Statistical natural language processing", :type :wiki-api.core/category}, 3424576 {:id 3424576, :title "Kernel methods", :type :wiki-api.core/article}, 2157920 {:id 2157920, :title "Technology in society", :type :wiki-api.core/category}, 797088 {:id 797088, :title "Computer vision", :type :wiki-api.core/category}, 1045088 {:id 1045088, :title "Semiotics", :type :wiki-api.core/category}, 470752 {:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article}, 4094720 {:id 4094720, :title "Latent class model", :type :wiki-api.core/article}, 331680 {:id 331680, :title "Edge detection", :type :wiki-api.core/article}, 336897 {:id 336897, :title "Function approximation", :type :wiki-api.core/article}, 15522913 {:id 15522913, :title "Predicate logic", :type :wiki-api.core/category}, 6539521 {:id 6539521, :title "Statistical theory", :type :wiki-api.core/category}, 871681 {:id 871681, :title "Mixture model", :type :wiki-api.core/article}, 30774561 {:id 30774561, :title "1956 in computer science", :type :wiki-api.core/category}, 32611713 {:id 32611713, :title "Emerging technologies", :type :wiki-api.core/category}, 772545 {:id 772545, :title "Probability distributions", :type :wiki-api.core/category}, 22532673 {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category}, 2883137 {:id 2883137, :title "Parametric model", :type :wiki-api.core/article}, 17306305 {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category}, 693985 {:id 693985, :title "Probability theory", :type :wiki-api.core/category}, 1911810 {:id 1911810, :title "Language model", :type :wiki-api.core/article}, 1557538 {:id 1557538, :title "Research methods", :type :wiki-api.core/category}, 3697698 {:id 3697698, :title "Statistical tests", :type :wiki-api.core/category}, 697506 {:id 697506, :title "Numerical analysis", :type :wiki-api.core/category}, 17506498 {:id 17506498, :title "Logic and statistics", :type :wiki-api.core/category}, 9500290 {:id 9500290, :title "Experiments", :type :wiki-api.core/category}, 733122 {:id 733122, :title "Grammar", :type :wiki-api.core/category}, 11034627 {:id 11034627, :title "Industrial engineering", :type :wiki-api.core/category}, 8330403 {:id 8330403, :title "Dirichlet process", :type :wiki-api.core/article}, 160995 {:id 160995, :title "Statistical significance", :type :wiki-api.core/article}, 22712867 {:id 22712867, :title "Justification", :type :wiki-api.core/category}, 24960643 {:id 24960643, :title "Statistical outliers", :type :wiki-api.core/category}, 1028771 {:id 1028771, :title "Control theory", :type :wiki-api.core/category}, 23173987 {:id 23173987, :title "Heuristic algorithms", :type :wiki-api.core/category}, 1069987 {:id 1069987, :title "Organizations in cryptography", :type :wiki-api.core/category}, 700355 {:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}, 34044100 {:id 34044100, :title "Optimization algorithms and methods", :type :wiki-api.core/category}, 22691076 {:id 22691076, :title "Graphical models", :type :wiki-api.core/category}, 787876 {:id 787876, :title "Computational linguistics", :type :wiki-api.core/category}, 8050180 {:id 8050180, :title "Multi-agent systems", :type :wiki-api.core/category}, 1098276 {:id 1098276, :title "Mathematical terminology", :type :wiki-api.core/category}, 700292 {:id 700292, :title "Scientific method", :type :wiki-api.core/category}, 26308484 {:id 26308484, :title "Summary statistics for contingency tables", :type :wiki-api.core/category}, 1487877 {:id 1487877, :title "Sampling techniques", :type :wiki-api.core/category}, 17193061 {:id 17193061, :title "Statistical data types", :type :wiki-api.core/category}, 20924581 {:id 20924581, :title "Latent variable models", :type :wiki-api.core/category}, 22820037 {:id 22820037, :title "Scientific revolution", :type :wiki-api.core/category}, 692453 {:id 692453, :title "Matrix theory", :type :wiki-api.core/category}, 9387237 {:id 9387237, :title "Parametric statistics", :type :wiki-api.core/category}, 30982373 {:id 30982373, :title "Missing data", :type :wiki-api.core/category}, 31176997 {:id 31176997, :title "Support vector machines", :type :wiki-api.core/category}, 2675045 {:id 2675045, :title "Quality", :type :wiki-api.core/category}, 21764485 {:id 21764485, :title "Statistical ratios", :type :wiki-api.core/category}, 18462661 {:id 18462661, :title "Resampling (statistics)", :type :wiki-api.core/category}, 1008581 {:id 1008581, :title "Operations research", :type :wiki-api.core/category}, 8156101 {:id 8156101, :title "Statistical methods", :type :wiki-api.core/category}, 5486694 {:id 5486694, :title "1972 introductions", :type :wiki-api.core/category}, 22589574 {:id 22589574, :title "Instance-based learning", :type :wiki-api.core/article}, 24104134 {:id 24104134, :title "Conditional probability", :type :wiki-api.core/article}, 30876902 {:id 30876902, :title "Independent and identically distributed random variables", :type :wiki-api.core/article}, 693702 {:id 693702, :title "Functional programming", :type :wiki-api.core/category}, 140806 {:id 140806, :title "Maximum likelihood", :type :wiki-api.core/article}, 22476294 {:id 22476294, :title "Matrix decompositions", :type :wiki-api.core/category}, 17503782 {:id 17503782, :title "Formal sciences", :type :wiki-api.core/category}, 10678854 {:id 10678854, :title "Collective intelligence", :type :wiki-api.core/category}, 744038 {:id 744038, :title "Philosophy of language", :type :wiki-api.core/category}, 6539078 {:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category}, 709574 {:id 709574, :title "Human communication", :type :wiki-api.core/category}, 10978278 {:id 10978278, :title "Quantitative research", :type :wiki-api.core/category}, 11504647 {:id 11504647, :title "Systems of formal logic", :type :wiki-api.core/category}, 10974311 {:id 10974311, :title "Conditional constructs", :type :wiki-api.core/category}, 19667111 {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}, 966983 {:id 966983, :title "Applied mathematics", :type :wiki-api.core/category}, 226631 {:id 226631, :title "Logistic regression", :type :wiki-api.core/article}, 5065063 {:id 5065063, :title "Network architecture", :type :wiki-api.core/category}, 6759 {:id 6759, :title "Context-free grammar", :type :wiki-api.core/article}, 821959 {:id 821959, :title "Matrices", :type :wiki-api.core/category}, 804551 {:id 804551, :title "Psychometrics", :type :wiki-api.core/category}, 10983 {:id 10983, :title "First-order logic", :type :wiki-api.core/article}, 5175143 {:id 5175143, :title "Geostatistics", :type :wiki-api.core/category}, 318439 {:id 318439, :title "Text mining", :type :wiki-api.core/article}, 380008 {:id 380008, :title "Probably approximately correct learning", :type :wiki-api.core/article}, 1126536 {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article}, 191752 {:id 191752, :title "Covariance matrix", :type :wiki-api.core/article}, 726312 {:id 726312, :title "Bioinformatics", :type :wiki-api.core/category}, 357672 {:id 357672, :title "Posterior probability", :type :wiki-api.core/article}, 693800 {:id 693800, :title "Geography", :type :wiki-api.core/category}, 878280 {:id 878280, :title "Standards organizations", :type :wiki-api.core/category}, 31024872 {:id 31024872, :title "Normal distribution", :type :wiki-api.core/category}, 744360 {:id 744360, :title "Networks", :type :wiki-api.core/category}, 21384136 {:id 21384136, :title "Computer-assisted translation", :type :wiki-api.core/category}, 3985352 {:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category}, 27955209 {:id 27955209, :title "Model selection", :type :wiki-api.core/category}, 25825321 {:id 25825321, :title "Mathematical sciences", :type :wiki-api.core/category}, 1587689 {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category}, 140841 {:id 140841, :title "Sufficient statistic", :type :wiki-api.core/article}, 14426697 {:id 14426697, :title "Market research", :type :wiki-api.core/category}, 5206601 {:id 5206601, :title "Data mining", :type :wiki-api.core/category}, 3864201 {:id 3864201, :title "Permutations", :type :wiki-api.core/category}, 694025 {:id 694025, :title "Information theory", :type :wiki-api.core/category}, 27593 {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article}, 6538378 {:id 6538378, :title "Statistical data sets", :type :wiki-api.core/category}, 916810 {:id 916810, :title "Spam filtering", :type :wiki-api.core/category}, 1152426 {:id 1152426, :title "Philosophy of mathematics", :type :wiki-api.core/category}, 504458 {:id 504458, :title "Conditional probability distribution", :type :wiki-api.core/article}, 700074 {:id 700074, :title "Logical fallacies", :type :wiki-api.core/category}, 708266 {:id 708266, :title "Artificial intelligence researchers", :type :wiki-api.core/category}, 30787370 {:id 30787370, :title "Compiler construction", :type :wiki-api.core/category}, 17594154 {:id 17594154, :title "Decision trees", :type :wiki-api.core/category}, 16989227 {:id 16989227, :title "Data", :type :wiki-api.core/category}, 87339 {:id 87339, :title "Naive Bayes classifier", :type :wiki-api.core/article}, 4289067 {:id 4289067, :title "Computational neuroscience", :type :wiki-api.core/category}, 5673643 {:id 5673643, :title "Risk analysis", :type :wiki-api.core/category}, 8135339 {:id 8135339, :title "Articles with example Haskell code", :type :wiki-api.core/category}, 1228459 {:id 1228459, :title "Business intelligence", :type :wiki-api.core/category}, 23890667 {:id 23890667, :title "Knowledge representation languages", :type :wiki-api.core/category}, 1055691 {:id 1055691, :title "Learning", :type :wiki-api.core/category}, 45035 {:id 45035, :title "Likelihood-ratio test", :type :wiki-api.core/article}, 17990732 {:id 17990732, :title "Theory of probability distributions", :type :wiki-api.core/category}, 5038508 {:id 5038508, :title "Information, knowledge, and uncertainty", :type :wiki-api.core/category}, 11472332 {:id 11472332, :title "Articles containing proofs", :type :wiki-api.core/category}, 897612 {:id 897612, :title "Syntax", :type :wiki-api.core/category}, 1817228 {:id 1817228, :title "Training set", :type :wiki-api.core/article}, 1855180 {:id 1855180, :title "Problem solving", :type :wiki-api.core/category}, 946892 {:id 946892, :title "Econometrics", :type :wiki-api.core/category}, 28335948 {:id 28335948, :title "Articles with inconsistent citation formats", :type :wiki-api.core/category}, 7279789 {:id 7279789, :title "Information retrieval", :type :wiki-api.core/category}, 3234221 {:id 3234221, :title "Analysis of algorithms", :type :wiki-api.core/category}, 31195693 {:id 31195693, :title "Design of experiments", :type :wiki-api.core/category}, 22991469 {:id 22991469, :title "Inductive reasoning", :type :wiki-api.core/category}, 472877 {:id 472877, :title "Prior probability", :type :wiki-api.core/article}, 22187885 {:id 22187885, :title "List of machine learning algorithms", :type :wiki-api.core/article}, 962413 {:id 962413, :title "Image processing", :type :wiki-api.core/category}, 15325165 {:id 15325165, :title "Data clustering algorithms", :type :wiki-api.core/category}, 772270 {:id 772270, :title "Philosophy of science", :type :wiki-api.core/category}, 8398 {:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article}, 1019150 {:id 1019150, :title "Machine translation", :type :wiki-api.core/category}, 26263822 {:id 26263822, :title "Packaging machinery", :type :wiki-api.core/category}, 1179950 {:id 1179950, :title "Feature selection", :type :wiki-api.core/article}, 36286862 {:id 36286862, :title "Behavioral and social facets of systemic risk", :type :wiki-api.core/category}, 22958 {:id 22958, :title "Sample space", :type :wiki-api.core/article}, 1195726 {:id 1195726, :title "Error", :type :wiki-api.core/category}, 9588430 {:id 9588430, :title "Higher-order statistics", :type :wiki-api.core/article}, 35718126 {:id 35718126, :title "Exponential family distributions", :type :wiki-api.core/category}, 183503 {:id 183503, :title "Description logic", :type :wiki-api.core/article}, 2023695 {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category}, 8495 {:id 8495, :title "Data set", :type :wiki-api.core/article}, 767343 {:id 767343, :title "Stochastic processes", :type :wiki-api.core/category}, 801135 {:id 801135, :title "Conditional independence", :type :wiki-api.core/article}, 17917391 {:id 17917391, :title "Missing completely at random", :type :wiki-api.core/article}, 26478223 {:id 26478223, :title "Reasoning", :type :wiki-api.core/category}, 14343887 {:id 14343887, :title "Precision and recall", :type :wiki-api.core/article}, 32549775 {:id 32549775, :title "Bayesian inference", :type :wiki-api.core/category}, 21053327 {:id 21053327, :title "United States Department of Commerce agencies", :type :wiki-api.core/category}, 706543 {:id 706543, :title "Machine learning", :type :wiki-api.core/category}, 772240 {:id 772240, :title "Epistemology", :type :wiki-api.core/category}, 22166736 {:id 22166736, :title "Non-classical logic", :type :wiki-api.core/category}, 914736 {:id 914736, :title "Programming paradigms", :type :wiki-api.core/category}, 26743152 {:id 26743152, :title "Conceptual models", :type :wiki-api.core/category}, 690672 {:id 690672, :title "Abstract algebra", :type :wiki-api.core/category}, 536080 {:id 536080, :title "Student's t-test", :type :wiki-api.core/article}, 1754736 {:id 1754736, :title "Cybernetics", :type :wiki-api.core/category}, 22121360 {:id 22121360, :title "Fellows of the Association for the Advancement of Artificial Intelligence", :type :wiki-api.core/category}, 22705265 {:id 22705265, :title "Probability assessment", :type :wiki-api.core/category}, 25174161 {:id 25174161, :title "Open problems", :type :wiki-api.core/category}, 958609 {:id 958609, :title "Statistical mechanics", :type :wiki-api.core/category}, 29549713 {:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category}, 1331441 {:id 1331441, :title "Document classification", :type :wiki-api.core/article}, 31454449 {:id 31454449, :title "Human–machine interaction", :type :wiki-api.core/category}, 17193265 {:id 17193265, :title "Statistical inference", :type :wiki-api.core/category}, 25304497 {:id 25304497, :title "Government agencies established in 1901", :type :wiki-api.core/category}, 35997105 {:id 35997105, :title "Generalized linear models", :type :wiki-api.core/category}, 1792433 {:id 1792433, :title "Maximum a posteriori estimation", :type :wiki-api.core/article}, 763505 {:id 763505, :title "Signal processing", :type :wiki-api.core/category}, 33748657 {:id 33748657, :title "Geometric measurement", :type :wiki-api.core/category}, 434897 {:id 434897, :title "Hough transform", :type :wiki-api.core/article}, 2871217 {:id 2871217, :title "Educational psychology", :type :wiki-api.core/category}, 34310097 {:id 34310097, :title "Computational learning theory", :type :wiki-api.core/category}, 1323985 {:id 1323985, :title "Markov random field", :type :wiki-api.core/article}, 5490673 {:id 5490673, :title "Quality control", :type :wiki-api.core/category}, 427282 {:id 427282, :title "Mutual information", :type :wiki-api.core/article}, 4873906 {:id 4873906, :title "Systems engineering", :type :wiki-api.core/category}, 34379506 {:id 34379506, :title "Epistemology of science", :type :wiki-api.core/category}, 4861714 {:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category}, 29313810 {:id 29313810, :title "Gaithersburg, Maryland", :type :wiki-api.core/category}, 792595 {:id 792595, :title "Computational physics", :type :wiki-api.core/category}, 21523 {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}, 8707155 {:id 8707155, :title "Statistical shape analysis", :type :wiki-api.core/article}, 689427 {:id 689427, :title "Latent semantic analysis", :type :wiki-api.core/article}, 17179027 {:id 17179027, :title "Categorical data", :type :wiki-api.core/category}, 7699923 {:id 7699923, :title "Speech processing", :type :wiki-api.core/category}, 34698963 {:id 34698963, :title "Methods in sociology", :type :wiki-api.core/category}, 7484211 {:id 7484211, :title "Fellows of the American Academy of Arts and Sciences", :type :wiki-api.core/category}, 36425555 {:id 36425555, :title "Factor analysis", :type :wiki-api.core/category}, 954323 {:id 954323, :title "Tikhonov regularization", :type :wiki-api.core/article}, 29003796 {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category}, 36477012 {:id 36477012, :title "Concepts in physics", :type :wiki-api.core/category}, 6890644 {:id 6890644, :title "Singular value decomposition", :type :wiki-api.core/category}, 172244 {:id 172244, :title "Simulated annealing", :type :wiki-api.core/article}, 9387252 {:id 9387252, :title "Non-parametric statistics", :type :wiki-api.core/category}, 76340 {:id 76340, :title "Principal component analysis", :type :wiki-api.core/article}, 1009204 {:id 1009204, :title "Information science", :type :wiki-api.core/category}, 12469844 {:id 12469844, :title "Wikipedia articles with ASCII art", :type :wiki-api.core/category}, 10690420 {:id 10690420, :title "Cognitive neuroscience", :type :wiki-api.core/category}, 2210772 {:id 2210772, :title "Continuous distributions", :type :wiki-api.core/category}, 879637 {:id 879637, :title "Joint probability distribution", :type :wiki-api.core/article}, 8439989 {:id 8439989, :title "Jewish American scientists", :type :wiki-api.core/category}, 5657877 {:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article}, 3670357 {:id 3670357, :title "Markov logic network", :type :wiki-api.core/article}, 17735029 {:id 17735029, :title "Probability interpretations", :type :wiki-api.core/category}, 36163957 {:id 36163957, :title "Stochastic simulation", :type :wiki-api.core/category}, 31229429 {:id 31229429, :title "Fellow Members of the IEEE", :type :wiki-api.core/category}, 17612277 {:id 17612277, :title "Multi-robot systems", :type :wiki-api.core/category}, 960021 {:id 960021, :title "Natural language processing", :type :wiki-api.core/category}, 716309 {:id 716309, :title "Cartography", :type :wiki-api.core/category}, 20941685 {:id 20941685, :title "Tasks of Natural language processing", :type :wiki-api.core/category}, 22718453 {:id 22718453, :title "Markov networks", :type :wiki-api.core/category}, 5200150 {:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category}, 313942 {:id 313942, :title "Local search (optimization)", :type :wiki-api.core/article}, 1991254 {:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category}, 66294 {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article}, 1034006 {:id 1034006, :title "Particle physics", :type :wiki-api.core/category}, 8190902 {:id 8190902, :title "Anomaly detection", :type :wiki-api.core/article}, 15690807 {:id 15690807, :title "Count data", :type :wiki-api.core/article}, 5699671 {:id 5699671, :title "Programming language topics", :type :wiki-api.core/category}, 2276471 {:id 2276471, :title "Databases", :type :wiki-api.core/category}, 2538775 {:id 2538775, :title "Predictive modelling", :type :wiki-api.core/article}, 17684887 {:id 17684887, :title "Statistical approximations", :type :wiki-api.core/category}, 32190039 {:id 32190039, :title "Speech", :type :wiki-api.core/category}, 1053303 {:id 1053303, :title "Statistical learning theory", :type :wiki-api.core/article}, 1532663 {:id 1532663, :title "Artificial intelligence applications", :type :wiki-api.core/category}, 6533911 {:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category}, 5646263 {:id 5646263, :title "University of California, Los Angeles faculty", :type :wiki-api.core/category}, 30319607 {:id 30319607, :title "Human–computer interaction", :type :wiki-api.core/category}, 4842680 {:id 4842680, :title "Data security", :type :wiki-api.core/category}, 36312376 {:id 36312376, :title "Belief revision", :type :wiki-api.core/category}, 405944 {:id 405944, :title "Time complexity", :type :wiki-api.core/article}, 12535256 {:id 12535256, :title "Kernel methods for machine learning", :type :wiki-api.core/category}, 16920 {:id 16920, :title "Knowledge representation and reasoning", :type :wiki-api.core/article}, 6535800 {:id 6535800, :title "Covariance and correlation", :type :wiki-api.core/category}, 4628120 {:id 4628120, :title "Computational resources", :type :wiki-api.core/category}, 694008 {:id 694008, :title "Statistics", :type :wiki-api.core/category}, 598776 {:id 598776, :title "Score (statistics)", :type :wiki-api.core/article}, 233497 {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article}, 784409 {:id 784409, :title "Marketing", :type :wiki-api.core/category}, 1171513 {:id 1171513, :title "Neural networks", :type :wiki-api.core/category}, 990361 {:id 990361, :title "Theoretical computer science", :type :wiki-api.core/category}, 3224825 {:id 3224825, :title "Scientific modeling", :type :wiki-api.core/category}, 1406201 {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category}, 27248985 {:id 27248985, :title "Mathematical concepts", :type :wiki-api.core/category}, 9272793 {:id 9272793, :title "Computational statistics", :type :wiki-api.core/category}, 1009209 {:id 1009209, :title "Information", :type :wiki-api.core/category}, 690777 {:id 690777, :title "Linear algebra", :type :wiki-api.core/category}, 28927577 {:id 28927577, :title "Probabilistic models", :type :wiki-api.core/category}, 4822234 {:id 4822234, :title "Mathematical and quantitative methods (economics)", :type :wiki-api.core/category}, 160986 {:id 160986, :title "Order statistic", :type :wiki-api.core/article}, 1686106 {:id 1686106, :title "Israeli computer scientists", :type :wiki-api.core/category}, 691866 {:id 691866, :title "Functional analysis", :type :wiki-api.core/category}, 6537978 {:id 6537978, :title "Summary statistics", :type :wiki-api.core/category}, 21917434 {:id 21917434, :title "Information Age", :type :wiki-api.core/category}, 4890 {:id 4890, :title "Bayesian probability", :type :wiki-api.core/article}, 28979098 {:id 28979098, :title "Distributions with conjugate priors", :type :wiki-api.core/category}, 972730 {:id 972730, :title "1936 births", :type :wiki-api.core/category}, 17652827 {:id 17652827, :title "Statistical dependence", :type :wiki-api.core/category}, 951835 {:id 951835, :title "Computer data", :type :wiki-api.core/category}, 709243 {:id 709243, :title "Communication", :type :wiki-api.core/category}, 693979 {:id 693979, :title "Model theory", :type :wiki-api.core/category}, 598971 {:id 598971, :title "Fisher information", :type :wiki-api.core/article}, 796635 {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category}, 4594748 {:id 4594748, :title "Computational problems", :type :wiki-api.core/category}, 958652 {:id 958652, :title "Speech recognition", :type :wiki-api.core/category}, 4183228 {:id 4183228, :title "Israeli philosophers", :type :wiki-api.core/category}, 29421852 {:id 29421852, :title "M-estimators", :type :wiki-api.core/category}, 14482748 {:id 14482748, :title "Data analysis", :type :wiki-api.core/category}, 20189596 {:id 20189596, :title "Probabilistic complexity theory", :type :wiki-api.core/category}, 17923612 {:id 17923612, :title "Types of probability distributions", :type :wiki-api.core/category}, 699964 {:id 699964, :title "Judea Pearl", :type :wiki-api.core/article}, 17228412 {:id 17228412, :title "Spatial data analysis", :type :wiki-api.core/category}, 1775388 {:id 1775388, :title "K-nearest neighbor algorithm", :type :wiki-api.core/article}, 6075324 {:id 6075324, :title "National Institute of Standards and Technology", :type :wiki-api.core/category}, 1188828 {:id 1188828, :title "Logic programming", :type :wiki-api.core/category}, 33547228 {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category}, 5232797 {:id 5232797, :title "Rutgers University alumni", :type :wiki-api.core/category}, 31813917 {:id 31813917, :title "Hidden Markov models", :type :wiki-api.core/category}, 37832157 {:id 37832157, :title "Deductive reasoning", :type :wiki-api.core/category}, 18956829 {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category}, 6536797 {:id 6536797, :title "Statistical algorithms", :type :wiki-api.core/category}, 20926 {:id 20926, :title "Supervised learning", :type :wiki-api.core/article}, 26339806 {:id 26339806, :title "Auxiliary sciences of history", :type :wiki-api.core/category}, 24059390 {:id 24059390, :title "Markov models", :type :wiki-api.core/category}, 4003390 {:id 4003390, :title "Evaluation methods", :type :wiki-api.core/category}, 18070174 {:id 18070174, :title "Engineering statistics", :type :wiki-api.core/category}, 1966814 {:id 1966814, :title "C4.5 algorithm", :type :wiki-api.core/article}, 946910 {:id 946910, :title "Decision theory", :type :wiki-api.core/category}, 699134 {:id 699134, :title "Formal languages", :type :wiki-api.core/category}, 3782398 {:id 3782398, :title "Living people", :type :wiki-api.core/category}, 3175294 {:id 3175294, :title "Dimension", :type :wiki-api.core/category}, 1228702 {:id 1228702, :title "Product management", :type :wiki-api.core/category}, 22042655 {:id 22042655, :title "Statistical principles", :type :wiki-api.core/category}, 21981503 {:id 21981503, :title "Conditionals", :type :wiki-api.core/category}, 693727 {:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category}, 17193471 {:id 17193471, :title "Statistical models", :type :wiki-api.core/category}, 21936671 {:id 21936671, :title "Non-parametric Bayesian methods", :type :wiki-api.core/category}, 3190431 {:id 3190431, :title "Spatial analysis", :type :wiki-api.core/article}, 1718975 {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category}, 28978911 {:id 28978911, :title "Conjugate prior distributions", :type :wiki-api.core/category}, 3515391 {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}}, :n-topics 5, :svg #utils.core.Success{:value "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.30.1 (20130314.1049)\n -->\n<!-- Title: G Pages: 1 -->\n<svg width=\"440pt\" height=\"130pt\"\n viewBox=\"0.00 0.00 440.00 130.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 126)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"white\" points=\"-4,5 -4,-126 437,-126 437,5 -4,5\"/>\n<!-- 706543 -->\n<g id=\"node1\" class=\"node\"><title>706543</title>\n<path fill=\"#eeeeff\" stroke=\"#eeeeff\" d=\"M195.938,-79.498C195.938,-79.498 12.062,-79.498 12.062,-79.498 6.06204,-79.498 0.0620377,-73.498 0.0620377,-67.498 0.0620377,-67.498 0.0620377,-54.502 0.0620377,-54.502 0.0620377,-48.502 6.06204,-42.502 12.062,-42.502 12.062,-42.502 195.938,-42.502 195.938,-42.502 201.938,-42.502 207.938,-48.502 207.938,-54.502 207.938,-54.502 207.938,-67.498 207.938,-67.498 207.938,-73.498 201.938,-79.498 195.938,-79.498\"/>\n<text text-anchor=\"middle\" x=\"104\" y=\"-52.499\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#6688aa\">Machine learning (72)</text>\n</g>\n<!-- 5206601 -->\n<g id=\"node2\" class=\"node\"><title>5206601</title>\n<path fill=\"#eeeeff\" stroke=\"#eeeeff\" d=\"M373.814,-122C373.814,-122 302.186,-122 302.186,-122 296.186,-122 290.186,-116 290.186,-110 290.186,-110 290.186,-98 290.186,-98 290.186,-92 296.186,-86 302.186,-86 302.186,-86 373.814,-86 373.814,-86 379.814,-86 385.814,-92 385.814,-98 385.814,-98 385.814,-110 385.814,-110 385.814,-116 379.814,-122 373.814,-122\"/>\n<text text-anchor=\"middle\" x=\"338\" y=\"-99.3245\" font-family=\"Arial\" font-size=\"11.00\" fill=\"#6688aa\">Data mining (15)</text>\n</g>\n<!-- 706543&#45;&gt;5206601 -->\n<g id=\"edge1\" class=\"edge\"><title>706543&#45;&gt;5206601</title>\n<path fill=\"none\" stroke=\"#9999bb\" d=\"M203.725,-79.2982C232.641,-84.6575 263.021,-90.2884 287.461,-94.8182\"/>\n<polygon fill=\"#9999bb\" stroke=\"#9999bb\" points=\"287.491,-95.7136 290.109,-95.3089 287.81,-93.9929 287.491,-95.7136\"/>\n</g>\n<!-- 29003796 -->\n<g id=\"node3\" class=\"node\"><title>29003796</title>\n<path fill=\"#eeeeff\" stroke=\"#eeeeff\" d=\"M405.621,-79C405.621,-79 270.379,-79 270.379,-79 264.379,-79 258.379,-73 258.379,-67 258.379,-67 258.379,-55 258.379,-55 258.379,-49 264.379,-43 270.379,-43 270.379,-43 405.621,-43 405.621,-43 411.621,-43 417.621,-49 417.621,-55 417.621,-55 417.621,-67 417.621,-67 417.621,-73 411.621,-79 405.621,-79\"/>\n<text text-anchor=\"middle\" x=\"338\" y=\"-56.7495\" font-family=\"Arial\" font-size=\"10.00\" fill=\"#6688aa\">Inductive logic programming (10)</text>\n</g>\n<!-- 706543&#45;&gt;29003796 -->\n<g id=\"edge2\" class=\"edge\"><title>706543&#45;&gt;29003796</title>\n<path fill=\"none\" stroke=\"#9999bb\" d=\"M207.772,-61C223.812,-61 240.194,-61 255.67,-61\"/>\n<polygon fill=\"#9999bb\" stroke=\"#9999bb\" points=\"255.847,-61.8751 258.347,-61 255.847,-60.1251 255.847,-61.8751\"/>\n</g>\n<!-- 19667111 -->\n<g id=\"node4\" class=\"node\"><title>19667111</title>\n<path fill=\"#eeeeff\" stroke=\"#eeeeff\" d=\"M420.088,-36C420.088,-36 255.912,-36 255.912,-36 249.912,-36 243.912,-30 243.912,-24 243.912,-24 243.912,-12 243.912,-12 243.912,-6 249.912,-0 255.912,-0 255.912,-0 420.088,-0 420.088,-0 426.088,-0 432.088,-6 432.088,-12 432.088,-12 432.088,-24 432.088,-24 432.088,-30 426.088,-36 420.088,-36\"/>\n<text text-anchor=\"middle\" x=\"338\" y=\"-12.8994\" font-family=\"Arial\" font-size=\"12.00\" fill=\"#6688aa\">Statistical relational learning (20)</text>\n</g>\n<!-- 706543&#45;&gt;19667111 -->\n<g id=\"edge3\" class=\"edge\"><title>706543&#45;&gt;19667111</title>\n<path fill=\"none\" stroke=\"#9999bb\" d=\"M203.725,-42.7018C216.212,-40.3874 228.973,-38.0223 241.41,-35.7172\"/>\n<polygon fill=\"#9999bb\" stroke=\"#9999bb\" points=\"241.594,-36.573 243.893,-35.257 241.275,-34.8523 241.594,-36.573\"/>\n</g>\n<!-- 1406201 -->\n<g id=\"node5\" class=\"node\"><title>1406201</title>\n<path fill=\"#eeeeff\" stroke=\"#eeeeff\" d=\"M146.592,-122C146.592,-122 61.4084,-122 61.4084,-122 55.4084,-122 49.4084,-116 49.4084,-110 49.4084,-110 49.4084,-98 49.4084,-98 49.4084,-92 55.4084,-86 61.4084,-86 61.4084,-86 146.592,-86 146.592,-86 152.592,-86 158.592,-92 158.592,-98 158.592,-98 158.592,-110 158.592,-110 158.592,-116 152.592,-122 146.592,-122\"/>\n<text text-anchor=\"middle\" x=\"104\" y=\"-99.7495\" font-family=\"Arial\" font-size=\"10.00\" fill=\"#6688aa\">Search algorithms (7)</text>\n</g>\n</g>\n</svg>\n"}, :topic-map #utils.core.Success{:value #topic_maps.core.TopicMap{:topic-graph #graphs.core.Digraph{:nodes #{{:id 763505, :title "Signal processing", :type :wiki-api.core/category} {:id 5486694, :title "1972 introductions", :type :wiki-api.core/category} {:id 693702, :title "Functional programming", :type :wiki-api.core/category} {:id 1228702, :title "Product management", :type :wiki-api.core/category} {:id 598776, :title "Score (statistics)", :type :wiki-api.core/article} {:id 536080, :title "Student's t-test", :type :wiki-api.core/article} {:id 690672, :title "Abstract algebra", :type :wiki-api.core/category} {:id 2675045, :title "Quality", :type :wiki-api.core/category} {:id 17228412, :title "Spatial data analysis", :type :wiki-api.core/category} {:id 10690420, :title "Cognitive neuroscience", :type :wiki-api.core/category} {:id 1034006, :title "Particle physics", :type :wiki-api.core/category} {:id 1188828, :title "Logic programming", :type :wiki-api.core/category} {:id 22820037, :title "Scientific revolution", :type :wiki-api.core/category} {:id 767343, :title "Stochastic processes", :type :wiki-api.core/category} {:id 17179027, :title "Categorical data", :type :wiki-api.core/category} {:id 23173987, :title "Heuristic algorithms", :type :wiki-api.core/category} {:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} {:id 1009204, :title "Information science", :type :wiki-api.core/category} {:id 787876, :title "Computational linguistics", :type :wiki-api.core/category} {:id 24059390, :title "Markov models", :type :wiki-api.core/category} {:id 8190902, :title "Anomaly detection", :type :wiki-api.core/article} {:id 3234221, :title "Analysis of algorithms", :type :wiki-api.core/category} {:id 22705265, :title "Probability assessment", :type :wiki-api.core/category} {:id 6538378, :title "Statistical data sets", :type :wiki-api.core/category} {:id 32549775, :title "Bayesian inference", :type :wiki-api.core/category} {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} {:id 22718453, :title "Markov networks", :type :wiki-api.core/category} {:id 34044100, :title "Optimization algorithms and methods", :type :wiki-api.core/category} {:id 25304497, :title "Government agencies established in 1901", :type :wiki-api.core/category} {:id 958652, :title "Speech recognition", :type :wiki-api.core/category} {:id 744360, :title "Networks", :type :wiki-api.core/category} {:id 6890644, :title "Singular value decomposition", :type :wiki-api.core/category} {:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 700074, :title "Logical fallacies", :type :wiki-api.core/category} {:id 17503782, :title "Formal sciences", :type :wiki-api.core/category} {:id 1323985, :title "Markov random field", :type :wiki-api.core/article} {:id 2883137, :title "Parametric model", :type :wiki-api.core/article} {:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category} {:id 871681, :title "Mixture model", :type :wiki-api.core/article} {:id 990361, :title "Theoretical computer science", :type :wiki-api.core/category} {:id 25825321, :title "Mathematical sciences", :type :wiki-api.core/category} {:id 6535800, :title "Covariance and correlation", :type :wiki-api.core/category} {:id 1195726, :title "Error", :type :wiki-api.core/category} {:id 36425555, :title "Factor analysis", :type :wiki-api.core/category} {:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} {:id 3697698, :title "Statistical tests", :type :wiki-api.core/category} {:id 4842680, :title "Data security", :type :wiki-api.core/category} {:id 1179950, :title "Feature selection", :type :wiki-api.core/article} {:id 4628120, :title "Computational resources", :type :wiki-api.core/category} {:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} {:id 8135339, :title "Articles with example Haskell code", :type :wiki-api.core/category} {:id 11737376, :title "Statistical natural language processing", :type :wiki-api.core/category} {:id 24104134, :title "Conditional probability", :type :wiki-api.core/article} {:id 22187885, :title "List of machine learning algorithms", :type :wiki-api.core/article} {:id 9387252, :title "Non-parametric statistics", :type :wiki-api.core/category} {:id 31229429, :title "Fellow Members of the IEEE", :type :wiki-api.core/category} {:id 30982373, :title "Missing data", :type :wiki-api.core/category} {:id 18462661, :title "Resampling (statistics)", :type :wiki-api.core/category} {:id 692453, :title "Matrix theory", :type :wiki-api.core/category} {:id 709243, :title "Communication", :type :wiki-api.core/category} {:id 405944, :title "Time complexity", :type :wiki-api.core/article} {:id 7484211, :title "Fellows of the American Academy of Arts and Sciences", :type :wiki-api.core/category} {:id 598971, :title "Fisher information", :type :wiki-api.core/article} {:id 772270, :title "Philosophy of science", :type :wiki-api.core/category} {:id 12469844, :title "Wikipedia articles with ASCII art", :type :wiki-api.core/category} {:id 1331441, :title "Document classification", :type :wiki-api.core/article} {:id 30319607, :title "Human–computer interaction", :type :wiki-api.core/category} {:id 2157920, :title "Technology in society", :type :wiki-api.core/category} {:id 21053327, :title "United States Department of Commerce agencies", :type :wiki-api.core/category} {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} {:id 709574, :title "Human communication", :type :wiki-api.core/category} {:id 24960643, :title "Statistical outliers", :type :wiki-api.core/category} {:id 22042655, :title "Statistical principles", :type :wiki-api.core/category} {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category} {:id 10978278, :title "Quantitative research", :type :wiki-api.core/category} {:id 5065063, :title "Network architecture", :type :wiki-api.core/category} {:id 31454449, :title "Human–machine interaction", :type :wiki-api.core/category} {:id 31195693, :title "Design of experiments", :type :wiki-api.core/category} {:id 7699923, :title "Speech processing", :type :wiki-api.core/category} {:id 22991469, :title "Inductive reasoning", :type :wiki-api.core/category} {:id 191752, :title "Covariance matrix", :type :wiki-api.core/article} {:id 30774561, :title "1956 in computer science", :type :wiki-api.core/category} {:id 26743152, :title "Conceptual models", :type :wiki-api.core/category} {:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category} {:id 36312376, :title "Belief revision", :type :wiki-api.core/category} {:id 45035, :title "Likelihood-ratio test", :type :wiki-api.core/article} {:id 17193061, :title "Statistical data types", :type :wiki-api.core/category} {:id 772545, :title "Probability distributions", :type :wiki-api.core/category} {:id 16989227, :title "Data", :type :wiki-api.core/category} {:id 23890667, :title "Knowledge representation languages", :type :wiki-api.core/category} {:id 76340, :title "Principal component analysis", :type :wiki-api.core/article} {:id 5232797, :title "Rutgers University alumni", :type :wiki-api.core/category} {:id 21384136, :title "Computer-assisted translation", :type :wiki-api.core/category} {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 17193265, :title "Statistical inference", :type :wiki-api.core/category} {:id 3190431, :title "Spatial analysis", :type :wiki-api.core/article} {:id 691866, :title "Functional analysis", :type :wiki-api.core/category} {:id 31813917, :title "Hidden Markov models", :type :wiki-api.core/category} {:id 28979098, :title "Distributions with conjugate priors", :type :wiki-api.core/category} {:id 3864201, :title "Permutations", :type :wiki-api.core/category} {:id 35718126, :title "Exponential family distributions", :type :wiki-api.core/category} {:id 226631, :title "Logistic regression", :type :wiki-api.core/article} {:id 946892, :title "Econometrics", :type :wiki-api.core/category} {:id 17612277, :title "Multi-robot systems", :type :wiki-api.core/category} {:id 716309, :title "Cartography", :type :wiki-api.core/category} {:id 172244, :title "Simulated annealing", :type :wiki-api.core/article} {:id 693979, :title "Model theory", :type :wiki-api.core/category} {:id 1754736, :title "Cybernetics", :type :wiki-api.core/category} {:id 17652827, :title "Statistical dependence", :type :wiki-api.core/category} {:id 2871217, :title "Educational psychology", :type :wiki-api.core/category} {:id 699964, :title "Judea Pearl", :type :wiki-api.core/article} {:id 2276471, :title "Databases", :type :wiki-api.core/category} {:id 697506, :title "Numerical analysis", :type :wiki-api.core/category} {:id 318439, :title "Text mining", :type :wiki-api.core/article} {:id 17923612, :title "Types of probability distributions", :type :wiki-api.core/category} {:id 34698963, :title "Methods in sociology", :type :wiki-api.core/category} {:id 4594748, :title "Computational problems", :type :wiki-api.core/category} {:id 5673643, :title "Risk analysis", :type :wiki-api.core/category} {:id 1045088, :title "Semiotics", :type :wiki-api.core/category} {:id 28927577, :title "Probabilistic models", :type :wiki-api.core/category} {:id 20926, :title "Supervised learning", :type :wiki-api.core/article} {:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category} {:id 12535256, :title "Kernel methods for machine learning", :type :wiki-api.core/category} {:id 5038508, :title "Information, knowledge, and uncertainty", :type :wiki-api.core/category} {:id 8495, :title "Data set", :type :wiki-api.core/article} {:id 6537978, :title "Summary statistics", :type :wiki-api.core/category} {:id 26263822, :title "Packaging machinery", :type :wiki-api.core/category} {:id 954323, :title "Tikhonov regularization", :type :wiki-api.core/article} {:id 4822234, :title "Mathematical and quantitative methods (economics)", :type :wiki-api.core/category} {:id 22476294, :title "Matrix decompositions", :type :wiki-api.core/category} {:id 22589574, :title "Instance-based learning", :type :wiki-api.core/article} {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 30876902, :title "Independent and identically distributed random variables", :type :wiki-api.core/article} {:id 8439989, :title "Jewish American scientists", :type :wiki-api.core/category} {:id 34310097, :title "Computational learning theory", :type :wiki-api.core/category} {:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article} {:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category} {:id 331680, :title "Edge detection", :type :wiki-api.core/article} {:id 21764485, :title "Statistical ratios", :type :wiki-api.core/category} {:id 8156101, :title "Statistical methods", :type :wiki-api.core/category} {:id 26339806, :title "Auxiliary sciences of history", :type :wiki-api.core/category} {:id 1069987, :title "Organizations in cryptography", :type :wiki-api.core/category} {:id 36163957, :title "Stochastic simulation", :type :wiki-api.core/category} {:id 504458, :title "Conditional probability distribution", :type :wiki-api.core/article} {:id 733122, :title "Grammar", :type :wiki-api.core/category} {:id 28335948, :title "Articles with inconsistent citation formats", :type :wiki-api.core/category} {:id 1911810, :title "Language model", :type :wiki-api.core/article} {:id 6539521, :title "Statistical theory", :type :wiki-api.core/category} {:id 1028771, :title "Control theory", :type :wiki-api.core/category} {:id 4873906, :title "Systems engineering", :type :wiki-api.core/category} {:id 708266, :title "Artificial intelligence researchers", :type :wiki-api.core/category} {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article} {:id 27248985, :title "Mathematical concepts", :type :wiki-api.core/category} {:id 792595, :title "Computational physics", :type :wiki-api.core/category} {:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category} {:id 2210772, :title "Continuous distributions", :type :wiki-api.core/category} {:id 1008581, :title "Operations research", :type :wiki-api.core/category} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} {:id 17990732, :title "Theory of probability distributions", :type :wiki-api.core/category} {:id 140841, :title "Sufficient statistic", :type :wiki-api.core/article} {:id 17735029, :title "Probability interpretations", :type :wiki-api.core/category} {:id 962413, :title "Image processing", :type :wiki-api.core/category} {:id 17594154, :title "Decision trees", :type :wiki-api.core/category} {:id 313942, :title "Local search (optimization)", :type :wiki-api.core/article} {:id 5490673, :title "Quality control", :type :wiki-api.core/category} {:id 694008, :title "Statistics", :type :wiki-api.core/category} {:id 22166736, :title "Non-classical logic", :type :wiki-api.core/category} {:id 1532663, :title "Artificial intelligence applications", :type :wiki-api.core/category} {:id 17917391, :title "Missing completely at random", :type :wiki-api.core/article} {:id 27955209, :title "Model selection", :type :wiki-api.core/category} {:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category} {:id 26308484, :title "Summary statistics for contingency tables", :type :wiki-api.core/category} {:id 6536797, :title "Statistical algorithms", :type :wiki-api.core/category} {:id 30787370, :title "Compiler construction", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 33748657, :title "Geometric measurement", :type :wiki-api.core/category} {:id 22958, :title "Sample space", :type :wiki-api.core/article} {:id 3175294, :title "Dimension", :type :wiki-api.core/category} {:id 35997105, :title "Generalized linear models", :type :wiki-api.core/category} {:id 804551, :title "Psychometrics", :type :wiki-api.core/category} {:id 3782398, :title "Living people", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 726312, :title "Bioinformatics", :type :wiki-api.core/category} {:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category} {:id 916810, :title "Spam filtering", :type :wiki-api.core/category} {:id 11034627, :title "Industrial engineering", :type :wiki-api.core/category} {:id 797088, :title "Computer vision", :type :wiki-api.core/category} {:id 36477012, :title "Concepts in physics", :type :wiki-api.core/category} {:id 784409, :title "Marketing", :type :wiki-api.core/category} {:id 5646263, :title "University of California, Los Angeles faculty", :type :wiki-api.core/category} {:id 772240, :title "Epistemology", :type :wiki-api.core/category} {:id 9387237, :title "Parametric statistics", :type :wiki-api.core/category} {:id 160986, :title "Order statistic", :type :wiki-api.core/article} {:id 17684887, :title "Statistical approximations", :type :wiki-api.core/category} {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article} {:id 380008, :title "Probably approximately correct learning", :type :wiki-api.core/article} {:id 1019150, :title "Machine translation", :type :wiki-api.core/category} {:id 15690807, :title "Count data", :type :wiki-api.core/article} {:id 878280, :title "Standards organizations", :type :wiki-api.core/category} {:id 9588430, :title "Higher-order statistics", :type :wiki-api.core/article} {:id 693985, :title "Probability theory", :type :wiki-api.core/category} {:id 960021, :title "Natural language processing", :type :wiki-api.core/category} {:id 10678854, :title "Collective intelligence", :type :wiki-api.core/category} {:id 946910, :title "Decision theory", :type :wiki-api.core/category} {:id 32190039, :title "Speech", :type :wiki-api.core/category} {:id 1855180, :title "Problem solving", :type :wiki-api.core/category} {:id 1557538, :title "Research methods", :type :wiki-api.core/category} {:id 1775388, :title "K-nearest neighbor algorithm", :type :wiki-api.core/article} {:id 160995, :title "Statistical significance", :type :wiki-api.core/article} {:id 8330403, :title "Dirichlet process", :type :wiki-api.core/article} {:id 4289067, :title "Computational neuroscience", :type :wiki-api.core/category} {:id 34379506, :title "Epistemology of science", :type :wiki-api.core/category} {:id 36286862, :title "Behavioral and social facets of systemic risk", :type :wiki-api.core/category} {:id 17506498, :title "Logic and statistics", :type :wiki-api.core/category} {:id 744038, :title "Philosophy of language", :type :wiki-api.core/category} {:id 694025, :title "Information theory", :type :wiki-api.core/category} {:id 699134, :title "Formal languages", :type :wiki-api.core/category} {:id 966983, :title "Applied mathematics", :type :wiki-api.core/category} {:id 434897, :title "Hough transform", :type :wiki-api.core/article} {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category} {:id 5175143, :title "Geostatistics", :type :wiki-api.core/category} {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category} {:id 3670357, :title "Markov logic network", :type :wiki-api.core/article} {:id 801135, :title "Conditional independence", :type :wiki-api.core/article} {:id 8050180, :title "Multi-agent systems", :type :wiki-api.core/category} {:id 2538775, :title "Predictive modelling", :type :wiki-api.core/article} {:id 914736, :title "Programming paradigms", :type :wiki-api.core/category} {:id 11472332, :title "Articles containing proofs", :type :wiki-api.core/category} {:id 10983, :title "First-order logic", :type :wiki-api.core/article} {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article} {:id 16920, :title "Knowledge representation and reasoning", :type :wiki-api.core/article} {:id 357672, :title "Posterior probability", :type :wiki-api.core/article} {:id 5699671, :title "Programming language topics", :type :wiki-api.core/category} {:id 21981503, :title "Conditionals", :type :wiki-api.core/category} {:id 4890, :title "Bayesian probability", :type :wiki-api.core/article} {:id 10974311, :title "Conditional constructs", :type :wiki-api.core/category} {:id 22691076, :title "Graphical models", :type :wiki-api.core/category} {:id 1686106, :title "Israeli computer scientists", :type :wiki-api.core/category} {:id 22712867, :title "Justification", :type :wiki-api.core/category} {:id 7279789, :title "Information retrieval", :type :wiki-api.core/category} {:id 87339, :title "Naive Bayes classifier", :type :wiki-api.core/article} {:id 26478223, :title "Reasoning", :type :wiki-api.core/category} {:id 31176997, :title "Support vector machines", :type :wiki-api.core/category} {:id 1966814, :title "C4.5 algorithm", :type :wiki-api.core/article} {:id 972730, :title "1936 births", :type :wiki-api.core/category} {:id 1009209, :title "Information", :type :wiki-api.core/category} {:id 4094720, :title "Latent class model", :type :wiki-api.core/article} {:id 6075324, :title "National Institute of Standards and Technology", :type :wiki-api.core/category} {:id 6759, :title "Context-free grammar", :type :wiki-api.core/article} {:id 693800, :title "Geography", :type :wiki-api.core/category} {:id 700292, :title "Scientific method", :type :wiki-api.core/category} {:id 140806, :title "Maximum likelihood", :type :wiki-api.core/article} {:id 4003390, :title "Evaluation methods", :type :wiki-api.core/category} {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} {:id 15522913, :title "Predicate logic", :type :wiki-api.core/category} {:id 690777, :title "Linear algebra", :type :wiki-api.core/category} {:id 17193471, :title "Statistical models", :type :wiki-api.core/category} {:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category} {:id 336897, :title "Function approximation", :type :wiki-api.core/article} {:id 21936671, :title "Non-parametric Bayesian methods", :type :wiki-api.core/category} {:id 3424576, :title "Kernel methods", :type :wiki-api.core/article} {:id 183503, :title "Description logic", :type :wiki-api.core/article} {:id 1053303, :title "Statistical learning theory", :type :wiki-api.core/article} {:id 22121360, :title "Fellows of the Association for the Advancement of Artificial Intelligence", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article} {:id 20924581, :title "Latent variable models", :type :wiki-api.core/category} {:id 31024872, :title "Normal distribution", :type :wiki-api.core/category} {:id 1770656, :title "Numerical linear algebra", :type :wiki-api.core/category} {:id 21917434, :title "Information Age", :type :wiki-api.core/category} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} {:id 37832157, :title "Deductive reasoning", :type :wiki-api.core/category} {:id 1152426, :title "Philosophy of mathematics", :type :wiki-api.core/category} {:id 20189596, :title "Probabilistic complexity theory", :type :wiki-api.core/category} {:id 1817228, :title "Training set", :type :wiki-api.core/article} {:id 14426697, :title "Market research", :type :wiki-api.core/category} {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category} {:id 18070174, :title "Engineering statistics", :type :wiki-api.core/category} {:id 28978911, :title "Conjugate prior distributions", :type :wiki-api.core/category} {:id 879637, :title "Joint probability distribution", :type :wiki-api.core/article} {:id 1792433, :title "Maximum a posteriori estimation", :type :wiki-api.core/article} {:id 8707155, :title "Statistical shape analysis", :type :wiki-api.core/article} {:id 1098276, :title "Mathematical terminology", :type :wiki-api.core/category} {:id 1055691, :title "Learning", :type :wiki-api.core/category} {:id 29421852, :title "M-estimators", :type :wiki-api.core/category} {:id 11504647, :title "Systems of formal logic", :type :wiki-api.core/category} {:id 821959, :title "Matrices", :type :wiki-api.core/category} {:id 897612, :title "Syntax", :type :wiki-api.core/category} {:id 14343887, :title "Precision and recall", :type :wiki-api.core/article} {:id 3224825, :title "Scientific modeling", :type :wiki-api.core/category} {:id 1487877, :title "Sampling techniques", :type :wiki-api.core/category} {:id 29313810, :title "Gaithersburg, Maryland", :type :wiki-api.core/category} {:id 9500290, :title "Experiments", :type :wiki-api.core/category} {:id 9272793, :title "Computational statistics", :type :wiki-api.core/category} {:id 427282, :title "Mutual information", :type :wiki-api.core/article} {:id 1228459, :title "Business intelligence", :type :wiki-api.core/category} {:id 20941685, :title "Tasks of Natural language processing", :type :wiki-api.core/category} {:id 951835, :title "Computer data", :type :wiki-api.core/category} {:id 472877, :title "Prior probability", :type :wiki-api.core/article} {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category} {:id 689427, :title "Latent semantic analysis", :type :wiki-api.core/article} {:id 958609, :title "Statistical mechanics", :type :wiki-api.core/category} {:id 32611713, :title "Emerging technologies", :type :wiki-api.core/category} {:id 4183228, :title "Israeli philosophers", :type :wiki-api.core/category} {:id 15325165, :title "Data clustering algorithms", :type :wiki-api.core/category} {:id 25174161, :title "Open problems", :type :wiki-api.core/category}}, :in-map {{:id 763505, :title "Signal processing", :type :wiki-api.core/category} #{{:id 966983, :title "Applied mathematics", :type :wiki-api.core/category}}, {:id 5486694, :title "1972 introductions", :type :wiki-api.core/category} #{}, {:id 693702, :title "Functional programming", :type :wiki-api.core/category} #{}, {:id 1228702, :title "Product management", :type :wiki-api.core/category} #{}, {:id 598776, :title "Score (statistics)", :type :wiki-api.core/article} #{{:id 1587689, :title "Estimation theory", :type :wiki-api.core/category}}, {:id 536080, :title "Student's t-test", :type :wiki-api.core/article} #{{:id 3697698, :title "Statistical tests", :type :wiki-api.core/category} {:id 8156101, :title "Statistical methods", :type :wiki-api.core/category} {:id 9387237, :title "Parametric statistics", :type :wiki-api.core/category}}, {:id 690672, :title "Abstract algebra", :type :wiki-api.core/category} #{}, {:id 2675045, :title "Quality", :type :wiki-api.core/category} #{}, {:id 17228412, :title "Spatial data analysis", :type :wiki-api.core/category} #{{:id 17193061, :title "Statistical data types", :type :wiki-api.core/category} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category}}, {:id 10690420, :title "Cognitive neuroscience", :type :wiki-api.core/category} #{}, {:id 1034006, :title "Particle physics", :type :wiki-api.core/category} #{}, {:id 1188828, :title "Logic programming", :type :wiki-api.core/category} #{{:id 5486694, :title "1972 introductions", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 914736, :title "Programming paradigms", :type :wiki-api.core/category}}, {:id 22820037, :title "Scientific revolution", :type :wiki-api.core/category} #{{:id 772270, :title "Philosophy of science", :type :wiki-api.core/category}}, {:id 767343, :title "Stochastic processes", :type :wiki-api.core/category} #{{:id 17193061, :title "Statistical data types", :type :wiki-api.core/category} {:id 693985, :title "Probability theory", :type :wiki-api.core/category}}, {:id 17179027, :title "Categorical data", :type :wiki-api.core/category} #{{:id 17193061, :title "Statistical data types", :type :wiki-api.core/category}}, {:id 23173987, :title "Heuristic algorithms", :type :wiki-api.core/category} #{}, {:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} #{{:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category} {:id 1195726, :title "Error", :type :wiki-api.core/category} {:id 31195693, :title "Design of experiments", :type :wiki-api.core/category} {:id 916810, :title "Spam filtering", :type :wiki-api.core/category}}, {:id 1009204, :title "Information science", :type :wiki-api.core/category} #{{:id 1009209, :title "Information", :type :wiki-api.core/category}}, {:id 787876, :title "Computational linguistics", :type :wiki-api.core/category} #{}, {:id 24059390, :title "Markov models", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 22691076, :title "Graphical models", :type :wiki-api.core/category}}, {:id 8190902, :title "Anomaly detection", :type :wiki-api.core/article} #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 4842680, :title "Data security", :type :wiki-api.core/category} {:id 24960643, :title "Statistical outliers", :type :wiki-api.core/category}}, {:id 3234221, :title "Analysis of algorithms", :type :wiki-api.core/category} #{{:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category}}, {:id 22705265, :title "Probability assessment", :type :wiki-api.core/category} #{}, {:id 6538378, :title "Statistical data sets", :type :wiki-api.core/category} #{{:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 32549775, :title "Bayesian inference", :type :wiki-api.core/category} #{{:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category} {:id 17193265, :title "Statistical inference", :type :wiki-api.core/category}}, {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} #{{:id 744360, :title "Networks", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 22691076, :title "Graphical models", :type :wiki-api.core/category}}, {:id 22718453, :title "Markov networks", :type :wiki-api.core/category} #{{:id 24059390, :title "Markov models", :type :wiki-api.core/category}}, {:id 34044100, :title "Optimization algorithms and methods", :type :wiki-api.core/category} #{{:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category}}, {:id 25304497, :title "Government agencies established in 1901", :type :wiki-api.core/category} #{}, {:id 958652, :title "Speech recognition", :type :wiki-api.core/category} #{{:id 787876, :title "Computational linguistics", :type :wiki-api.core/category} {:id 30319607, :title "Human–computer interaction", :type :wiki-api.core/category} {:id 1532663, :title "Artificial intelligence applications", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 32190039, :title "Speech", :type :wiki-api.core/category}}, {:id 744360, :title "Networks", :type :wiki-api.core/category} #{}, {:id 6890644, :title "Singular value decomposition", :type :wiki-api.core/category} #{{:id 691866, :title "Functional analysis", :type :wiki-api.core/category} {:id 22476294, :title "Matrix decompositions", :type :wiki-api.core/category}}, {:id 5206601, :title "Data mining", :type :wiki-api.core/category} #{{:id 1009204, :title "Information science", :type :wiki-api.core/category} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category}}, {:id 700074, :title "Logical fallacies", :type :wiki-api.core/category} #{}, {:id 17503782, :title "Formal sciences", :type :wiki-api.core/category} #{}, {:id 1323985, :title "Markov random field", :type :wiki-api.core/article} #{{:id 22718453, :title "Markov networks", :type :wiki-api.core/category} {:id 693985, :title "Probability theory", :type :wiki-api.core/category}}, {:id 2883137, :title "Parametric model", :type :wiki-api.core/article} #{{:id 17193471, :title "Statistical models", :type :wiki-api.core/category}}, {:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category} #{{:id 17193265, :title "Statistical inference", :type :wiki-api.core/category} {:id 946892, :title "Econometrics", :type :wiki-api.core/category}}, {:id 871681, :title "Mixture model", :type :wiki-api.core/article} #{{:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} {:id 28927577, :title "Probabilistic models", :type :wiki-api.core/category} {:id 20924581, :title "Latent variable models", :type :wiki-api.core/category}}, {:id 990361, :title "Theoretical computer science", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 25825321, :title "Mathematical sciences", :type :wiki-api.core/category} #{}, {:id 6535800, :title "Covariance and correlation", :type :wiki-api.core/category} #{{:id 17652827, :title "Statistical dependence", :type :wiki-api.core/category} {:id 17990732, :title "Theory of probability distributions", :type :wiki-api.core/category}}, {:id 1195726, :title "Error", :type :wiki-api.core/category} #{}, {:id 36425555, :title "Factor analysis", :type :wiki-api.core/category} #{{:id 1228702, :title "Product management", :type :wiki-api.core/category} {:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category} {:id 804551, :title "Psychometrics", :type :wiki-api.core/category} {:id 20924581, :title "Latent variable models", :type :wiki-api.core/category} {:id 14426697, :title "Market research", :type :wiki-api.core/category}}, {:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} #{{:id 990361, :title "Theoretical computer science", :type :wiki-api.core/category}}, {:id 3697698, :title "Statistical tests", :type :wiki-api.core/category} #{{:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category}}, {:id 4842680, :title "Data security", :type :wiki-api.core/category} #{{:id 951835, :title "Computer data", :type :wiki-api.core/category}}, {:id 1179950, :title "Feature selection", :type :wiki-api.core/article} #{{:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category} {:id 27955209, :title "Model selection", :type :wiki-api.core/category}}, {:id 4628120, :title "Computational resources", :type :wiki-api.core/category} #{{:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category}}, {:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} #{{:id 690672, :title "Abstract algebra", :type :wiki-api.core/category} {:id 27248985, :title "Mathematical concepts", :type :wiki-api.core/category} {:id 3175294, :title "Dimension", :type :wiki-api.core/category}}, {:id 8135339, :title "Articles with example Haskell code", :type :wiki-api.core/category} #{}, {:id 11737376, :title "Statistical natural language processing", :type :wiki-api.core/category} #{{:id 24059390, :title "Markov models", :type :wiki-api.core/category} {:id 960021, :title "Natural language processing", :type :wiki-api.core/category}}, {:id 24104134, :title "Conditional probability", :type :wiki-api.core/article} #{{:id 700074, :title "Logical fallacies", :type :wiki-api.core/category} {:id 21764485, :title "Statistical ratios", :type :wiki-api.core/category} {:id 693985, :title "Probability theory", :type :wiki-api.core/category} {:id 21981503, :title "Conditionals", :type :wiki-api.core/category}}, {:id 22187885, :title "List of machine learning algorithms", :type :wiki-api.core/article} #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category}}, {:id 9387252, :title "Non-parametric statistics", :type :wiki-api.core/category} #{{:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 31229429, :title "Fellow Members of the IEEE", :type :wiki-api.core/category} #{}, {:id 30982373, :title "Missing data", :type :wiki-api.core/category} #{{:id 17193061, :title "Statistical data types", :type :wiki-api.core/category}}, {:id 18462661, :title "Resampling (statistics)", :type :wiki-api.core/category} #{{:id 9387252, :title "Non-parametric statistics", :type :wiki-api.core/category} {:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category}}, {:id 692453, :title "Matrix theory", :type :wiki-api.core/category} #{{:id 690777, :title "Linear algebra", :type :wiki-api.core/category}}, {:id 709243, :title "Communication", :type :wiki-api.core/category} #{{:id 1009209, :title "Information", :type :wiki-api.core/category}}, {:id 405944, :title "Time complexity", :type :wiki-api.core/article} #{{:id 3234221, :title "Analysis of algorithms", :type :wiki-api.core/category} {:id 4628120, :title "Computational resources", :type :wiki-api.core/category}}, {:id 7484211, :title "Fellows of the American Academy of Arts and Sciences", :type :wiki-api.core/category} #{}, {:id 598971, :title "Fisher information", :type :wiki-api.core/article} #{{:id 31195693, :title "Design of experiments", :type :wiki-api.core/category} {:id 694025, :title "Information theory", :type :wiki-api.core/category} {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category}}, {:id 772270, :title "Philosophy of science", :type :wiki-api.core/category} #{{:id 772240, :title "Epistemology", :type :wiki-api.core/category}}, {:id 12469844, :title "Wikipedia articles with ASCII art", :type :wiki-api.core/category} #{}, {:id 1331441, :title "Document classification", :type :wiki-api.core/article} #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 960021, :title "Natural language processing", :type :wiki-api.core/category} {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category}}, {:id 30319607, :title "Human–computer interaction", :type :wiki-api.core/category} #{{:id 22820037, :title "Scientific revolution", :type :wiki-api.core/category} {:id 31454449, :title "Human–machine interaction", :type :wiki-api.core/category} {:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category} {:id 36286862, :title "Behavioral and social facets of systemic risk", :type :wiki-api.core/category}}, {:id 2157920, :title "Technology in society", :type :wiki-api.core/category} #{}, {:id 21053327, :title "United States Department of Commerce agencies", :type :wiki-api.core/category} #{}, {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category} {:id 5175143, :title "Geostatistics", :type :wiki-api.core/category}}, {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} #{{:id 24059390, :title "Markov models", :type :wiki-api.core/category} {:id 36312376, :title "Belief revision", :type :wiki-api.core/category} {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category}}, {:id 709574, :title "Human communication", :type :wiki-api.core/category} #{}, {:id 24960643, :title "Statistical outliers", :type :wiki-api.core/category} #{{:id 17193061, :title "Statistical data types", :type :wiki-api.core/category}}, {:id 22042655, :title "Statistical principles", :type :wiki-api.core/category} #{{:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category} #{{:id 693702, :title "Functional programming", :type :wiki-api.core/category} {:id 8135339, :title "Articles with example Haskell code", :type :wiki-api.core/category} {:id 699134, :title "Formal languages", :type :wiki-api.core/category} {:id 10974311, :title "Conditional constructs", :type :wiki-api.core/category}}, {:id 10978278, :title "Quantitative research", :type :wiki-api.core/category} #{{:id 1557538, :title "Research methods", :type :wiki-api.core/category} {:id 4003390, :title "Evaluation methods", :type :wiki-api.core/category}}, {:id 5065063, :title "Network architecture", :type :wiki-api.core/category} #{}, {:id 31454449, :title "Human–machine interaction", :type :wiki-api.core/category} #{{:id 1009204, :title "Information science", :type :wiki-api.core/category} {:id 709574, :title "Human communication", :type :wiki-api.core/category}}, {:id 31195693, :title "Design of experiments", :type :wiki-api.core/category} #{{:id 2675045, :title "Quality", :type :wiki-api.core/category} {:id 8156101, :title "Statistical methods", :type :wiki-api.core/category} {:id 6539521, :title "Statistical theory", :type :wiki-api.core/category} {:id 5490673, :title "Quality control", :type :wiki-api.core/category} {:id 18070174, :title "Engineering statistics", :type :wiki-api.core/category} {:id 9500290, :title "Experiments", :type :wiki-api.core/category}}, {:id 7699923, :title "Speech processing", :type :wiki-api.core/category} #{{:id 958652, :title "Speech recognition", :type :wiki-api.core/category}}, {:id 22991469, :title "Inductive reasoning", :type :wiki-api.core/category} #{{:id 26478223, :title "Reasoning", :type :wiki-api.core/category} {:id 1055691, :title "Learning", :type :wiki-api.core/category} {:id 3224825, :title "Scientific modeling", :type :wiki-api.core/category}}, {:id 191752, :title "Covariance matrix", :type :wiki-api.core/article} #{{:id 6535800, :title "Covariance and correlation", :type :wiki-api.core/category} {:id 6537978, :title "Summary statistics", :type :wiki-api.core/category} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} {:id 821959, :title "Matrices", :type :wiki-api.core/category}}, {:id 30774561, :title "1956 in computer science", :type :wiki-api.core/category} #{}, {:id 26743152, :title "Conceptual models", :type :wiki-api.core/category} #{}, {:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category} #{{:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 36312376, :title "Belief revision", :type :wiki-api.core/category} #{{:id 1188828, :title "Logic programming", :type :wiki-api.core/category} {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category}}, {:id 45035, :title "Likelihood-ratio test", :type :wiki-api.core/article} #{{:id 3697698, :title "Statistical tests", :type :wiki-api.core/category} {:id 21764485, :title "Statistical ratios", :type :wiki-api.core/category}}, {:id 17193061, :title "Statistical data types", :type :wiki-api.core/category} #{{:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 772545, :title "Probability distributions", :type :wiki-api.core/category} #{{:id 693985, :title "Probability theory", :type :wiki-api.core/category} {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}, {:id 16989227, :title "Data", :type :wiki-api.core/category} #{}, {:id 23890667, :title "Knowledge representation languages", :type :wiki-api.core/category} #{{:id 796635, :title "Knowledge representation", :type :wiki-api.core/category}}, {:id 76340, :title "Principal component analysis", :type :wiki-api.core/article} #{{:id 22476294, :title "Matrix decompositions", :type :wiki-api.core/category} {:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category}}, {:id 5232797, :title "Rutgers University alumni", :type :wiki-api.core/category} #{}, {:id 21384136, :title "Computer-assisted translation", :type :wiki-api.core/category} #{}, {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}}, {:id 17193265, :title "Statistical inference", :type :wiki-api.core/category} #{{:id 22991469, :title "Inductive reasoning", :type :wiki-api.core/category} {:id 6539521, :title "Statistical theory", :type :wiki-api.core/category} {:id 804551, :title "Psychometrics", :type :wiki-api.core/category} {:id 17506498, :title "Logic and statistics", :type :wiki-api.core/category} {:id 37832157, :title "Deductive reasoning", :type :wiki-api.core/category}}, {:id 3190431, :title "Spatial analysis", :type :wiki-api.core/article} #{{:id 716309, :title "Cartography", :type :wiki-api.core/category} {:id 5175143, :title "Geostatistics", :type :wiki-api.core/category} {:id 693800, :title "Geography", :type :wiki-api.core/category}}, {:id 691866, :title "Functional analysis", :type :wiki-api.core/category} #{}, {:id 31813917, :title "Hidden Markov models", :type :wiki-api.core/category} #{{:id 24059390, :title "Markov models", :type :wiki-api.core/category} {:id 726312, :title "Bioinformatics", :type :wiki-api.core/category}}, {:id 28979098, :title "Distributions with conjugate priors", :type :wiki-api.core/category} #{{:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category} {:id 772545, :title "Probability distributions", :type :wiki-api.core/category}}, {:id 3864201, :title "Permutations", :type :wiki-api.core/category} #{}, {:id 35718126, :title "Exponential family distributions", :type :wiki-api.core/category} #{{:id 772545, :title "Probability distributions", :type :wiki-api.core/category}}, {:id 226631, :title "Logistic regression", :type :wiki-api.core/article} #{{:id 17179027, :title "Categorical data", :type :wiki-api.core/category} {:id 35997105, :title "Generalized linear models", :type :wiki-api.core/category}}, {:id 946892, :title "Econometrics", :type :wiki-api.core/category} #{{:id 4822234, :title "Mathematical and quantitative methods (economics)", :type :wiki-api.core/category}}, {:id 17612277, :title "Multi-robot systems", :type :wiki-api.core/category} #{}, {:id 716309, :title "Cartography", :type :wiki-api.core/category} #{}, {:id 172244, :title "Simulated annealing", :type :wiki-api.core/article} #{{:id 23173987, :title "Heuristic algorithms", :type :wiki-api.core/category} {:id 34044100, :title "Optimization algorithms and methods", :type :wiki-api.core/category} {:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category}}, {:id 693979, :title "Model theory", :type :wiki-api.core/category} #{}, {:id 1754736, :title "Cybernetics", :type :wiki-api.core/category} #{{:id 966983, :title "Applied mathematics", :type :wiki-api.core/category}}, {:id 17652827, :title "Statistical dependence", :type :wiki-api.core/category} #{{:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 2871217, :title "Educational psychology", :type :wiki-api.core/category} #{}, {:id 699964, :title "Judea Pearl", :type :wiki-api.core/article} #{{:id 31229429, :title "Fellow Members of the IEEE", :type :wiki-api.core/category} {:id 7484211, :title "Fellows of the American Academy of Arts and Sciences", :type :wiki-api.core/category} {:id 5232797, :title "Rutgers University alumni", :type :wiki-api.core/category} {:id 8439989, :title "Jewish American scientists", :type :wiki-api.core/category} {:id 3782398, :title "Living people", :type :wiki-api.core/category} {:id 5646263, :title "University of California, Los Angeles faculty", :type :wiki-api.core/category} {:id 1686106, :title "Israeli computer scientists", :type :wiki-api.core/category} {:id 972730, :title "1936 births", :type :wiki-api.core/category} {:id 22121360, :title "Fellows of the Association for the Advancement of Artificial Intelligence", :type :wiki-api.core/category} {:id 4183228, :title "Israeli philosophers", :type :wiki-api.core/category}}, {:id 2276471, :title "Databases", :type :wiki-api.core/category} #{{:id 951835, :title "Computer data", :type :wiki-api.core/category}}, {:id 697506, :title "Numerical analysis", :type :wiki-api.core/category} #{}, {:id 318439, :title "Text mining", :type :wiki-api.core/article} #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 11737376, :title "Statistical natural language processing", :type :wiki-api.core/category}}, {:id 17923612, :title "Types of probability distributions", :type :wiki-api.core/category} #{{:id 772545, :title "Probability distributions", :type :wiki-api.core/category}}, {:id 34698963, :title "Methods in sociology", :type :wiki-api.core/category} #{}, {:id 4594748, :title "Computational problems", :type :wiki-api.core/category} #{{:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category}}, {:id 5673643, :title "Risk analysis", :type :wiki-api.core/category} #{{:id 946910, :title "Decision theory", :type :wiki-api.core/category}}, {:id 1045088, :title "Semiotics", :type :wiki-api.core/category} #{{:id 1754736, :title "Cybernetics", :type :wiki-api.core/category} {:id 744038, :title "Philosophy of language", :type :wiki-api.core/category}}, {:id 28927577, :title "Probabilistic models", :type :wiki-api.core/category} #{}, {:id 20926, :title "Supervised learning", :type :wiki-api.core/article} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category} #{{:id 17503782, :title "Formal sciences", :type :wiki-api.core/category} {:id 2157920, :title "Technology in society", :type :wiki-api.core/category} {:id 4289067, :title "Computational neuroscience", :type :wiki-api.core/category} {:id 32611713, :title "Emerging technologies", :type :wiki-api.core/category} {:id 25174161, :title "Open problems", :type :wiki-api.core/category}}, {:id 12535256, :title "Kernel methods for machine learning", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 5038508, :title "Information, knowledge, and uncertainty", :type :wiki-api.core/category} #{{:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 8495, :title "Data set", :type :wiki-api.core/article} #{{:id 6538378, :title "Statistical data sets", :type :wiki-api.core/category} {:id 951835, :title "Computer data", :type :wiki-api.core/category}}, {:id 6537978, :title "Summary statistics", :type :wiki-api.core/category} #{{:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 26263822, :title "Packaging machinery", :type :wiki-api.core/category} #{}, {:id 954323, :title "Tikhonov regularization", :type :wiki-api.core/article} #{{:id 1587689, :title "Estimation theory", :type :wiki-api.core/category} {:id 690777, :title "Linear algebra", :type :wiki-api.core/category}}, {:id 4822234, :title "Mathematical and quantitative methods (economics)", :type :wiki-api.core/category} #{{:id 10978278, :title "Quantitative research", :type :wiki-api.core/category}}, {:id 22476294, :title "Matrix decompositions", :type :wiki-api.core/category} #{{:id 692453, :title "Matrix theory", :type :wiki-api.core/category} {:id 1770656, :title "Numerical linear algebra", :type :wiki-api.core/category}}, {:id 22589574, :title "Instance-based learning", :type :wiki-api.core/article} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} #{{:id 697506, :title "Numerical analysis", :type :wiki-api.core/category} {:id 4822234, :title "Mathematical and quantitative methods (economics)", :type :wiki-api.core/category} {:id 1008581, :title "Operations research", :type :wiki-api.core/category}}, {:id 30876902, :title "Independent and identically distributed random variables", :type :wiki-api.core/article} #{{:id 693985, :title "Probability theory", :type :wiki-api.core/category} {:id 17193471, :title "Statistical models", :type :wiki-api.core/category}}, {:id 8439989, :title "Jewish American scientists", :type :wiki-api.core/category} #{}, {:id 34310097, :title "Computational learning theory", :type :wiki-api.core/category} #{{:id 990361, :title "Theoretical computer science", :type :wiki-api.core/category}}, {:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article} #{{:id 34044100, :title "Optimization algorithms and methods", :type :wiki-api.core/category} {:id 30982373, :title "Missing data", :type :wiki-api.core/category} {:id 6536797, :title "Statistical algorithms", :type :wiki-api.core/category} {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category} {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category} {:id 15325165, :title "Data clustering algorithms", :type :wiki-api.core/category}}, {:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category} #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category} {:id 3175294, :title "Dimension", :type :wiki-api.core/category}}, {:id 331680, :title "Edge detection", :type :wiki-api.core/article} #{{:id 797088, :title "Computer vision", :type :wiki-api.core/category}}, {:id 21764485, :title "Statistical ratios", :type :wiki-api.core/category} #{{:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 8156101, :title "Statistical methods", :type :wiki-api.core/category} #{{:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 26339806, :title "Auxiliary sciences of history", :type :wiki-api.core/category} #{}, {:id 1069987, :title "Organizations in cryptography", :type :wiki-api.core/category} #{}, {:id 36163957, :title "Stochastic simulation", :type :wiki-api.core/category} #{{:id 767343, :title "Stochastic processes", :type :wiki-api.core/category}}, {:id 504458, :title "Conditional probability distribution", :type :wiki-api.core/article} #{{:id 17923612, :title "Types of probability distributions", :type :wiki-api.core/category}}, {:id 733122, :title "Grammar", :type :wiki-api.core/category} #{}, {:id 28335948, :title "Articles with inconsistent citation formats", :type :wiki-api.core/category} #{}, {:id 1911810, :title "Language model", :type :wiki-api.core/article} #{{:id 11737376, :title "Statistical natural language processing", :type :wiki-api.core/category}}, {:id 6539521, :title "Statistical theory", :type :wiki-api.core/category} #{{:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 1028771, :title "Control theory", :type :wiki-api.core/category} #{{:id 17503782, :title "Formal sciences", :type :wiki-api.core/category} {:id 1754736, :title "Cybernetics", :type :wiki-api.core/category}}, {:id 4873906, :title "Systems engineering", :type :wiki-api.core/category} #{}, {:id 708266, :title "Artificial intelligence researchers", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}}, {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article} #{{:id 4594748, :title "Computational problems", :type :wiki-api.core/category}}, {:id 27248985, :title "Mathematical concepts", :type :wiki-api.core/category} #{}, {:id 792595, :title "Computational physics", :type :wiki-api.core/category} #{}, {:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category} #{{:id 17193061, :title "Statistical data types", :type :wiki-api.core/category}}, {:id 2210772, :title "Continuous distributions", :type :wiki-api.core/category} #{{:id 772545, :title "Probability distributions", :type :wiki-api.core/category}}, {:id 1008581, :title "Operations research", :type :wiki-api.core/category} #{{:id 966983, :title "Applied mathematics", :type :wiki-api.core/category}}, {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} #{{:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} #{{:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category} {:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category}}, {:id 17990732, :title "Theory of probability distributions", :type :wiki-api.core/category} #{{:id 772545, :title "Probability distributions", :type :wiki-api.core/category}}, {:id 140841, :title "Sufficient statistic", :type :wiki-api.core/article} #{{:id 22042655, :title "Statistical principles", :type :wiki-api.core/category} {:id 6539521, :title "Statistical theory", :type :wiki-api.core/category} {:id 11472332, :title "Articles containing proofs", :type :wiki-api.core/category}}, {:id 17735029, :title "Probability interpretations", :type :wiki-api.core/category} #{{:id 6539521, :title "Statistical theory", :type :wiki-api.core/category} {:id 693985, :title "Probability theory", :type :wiki-api.core/category}}, {:id 962413, :title "Image processing", :type :wiki-api.core/category} #{{:id 763505, :title "Signal processing", :type :wiki-api.core/category}}, {:id 17594154, :title "Decision trees", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 946910, :title "Decision theory", :type :wiki-api.core/category}}, {:id 313942, :title "Local search (optimization)", :type :wiki-api.core/article} #{{:id 34044100, :title "Optimization algorithms and methods", :type :wiki-api.core/category}}, {:id 5490673, :title "Quality control", :type :wiki-api.core/category} #{{:id 17193061, :title "Statistical data types", :type :wiki-api.core/category}}, {:id 694008, :title "Statistics", :type :wiki-api.core/category} #{{:id 4822234, :title "Mathematical and quantitative methods (economics)", :type :wiki-api.core/category} {:id 26339806, :title "Auxiliary sciences of history", :type :wiki-api.core/category} {:id 1009209, :title "Information", :type :wiki-api.core/category}}, {:id 22166736, :title "Non-classical logic", :type :wiki-api.core/category} #{}, {:id 1532663, :title "Artificial intelligence applications", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}}, {:id 17917391, :title "Missing completely at random", :type :wiki-api.core/article} #{{:id 30982373, :title "Missing data", :type :wiki-api.core/category} {:id 804551, :title "Psychometrics", :type :wiki-api.core/category}}, {:id 27955209, :title "Model selection", :type :wiki-api.core/category} #{{:id 17193265, :title "Statistical inference", :type :wiki-api.core/category} {:id 17193471, :title "Statistical models", :type :wiki-api.core/category}}, {:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 26308484, :title "Summary statistics for contingency tables", :type :wiki-api.core/category} #{{:id 17179027, :title "Categorical data", :type :wiki-api.core/category} {:id 6537978, :title "Summary statistics", :type :wiki-api.core/category}}, {:id 6536797, :title "Statistical algorithms", :type :wiki-api.core/category} #{{:id 9272793, :title "Computational statistics", :type :wiki-api.core/category}}, {:id 30787370, :title "Compiler construction", :type :wiki-api.core/category} #{}, {:id 706543, :title "Machine learning", :type :wiki-api.core/category} #{{:id 1055691, :title "Learning", :type :wiki-api.core/category} {:id 9272793, :title "Computational statistics", :type :wiki-api.core/category}}, {:id 33748657, :title "Geometric measurement", :type :wiki-api.core/category} #{}, {:id 22958, :title "Sample space", :type :wiki-api.core/article} #{{:id 693985, :title "Probability theory", :type :wiki-api.core/category}}, {:id 3175294, :title "Dimension", :type :wiki-api.core/category} #{{:id 33748657, :title "Geometric measurement", :type :wiki-api.core/category} {:id 36477012, :title "Concepts in physics", :type :wiki-api.core/category} {:id 1098276, :title "Mathematical terminology", :type :wiki-api.core/category}}, {:id 35997105, :title "Generalized linear models", :type :wiki-api.core/category} #{{:id 17193471, :title "Statistical models", :type :wiki-api.core/category}}, {:id 804551, :title "Psychometrics", :type :wiki-api.core/category} #{{:id 2871217, :title "Educational psychology", :type :wiki-api.core/category}}, {:id 3782398, :title "Living people", :type :wiki-api.core/category} #{}, {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} #{{:id 744360, :title "Networks", :type :wiki-api.core/category} {:id 5065063, :title "Network architecture", :type :wiki-api.core/category} {:id 946892, :title "Econometrics", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}, {:id 726312, :title "Bioinformatics", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category} #{{:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category}}, {:id 916810, :title "Spam filtering", :type :wiki-api.core/category} #{}, {:id 11034627, :title "Industrial engineering", :type :wiki-api.core/category} #{}, {:id 797088, :title "Computer vision", :type :wiki-api.core/category} #{{:id 26263822, :title "Packaging machinery", :type :wiki-api.core/category} {:id 962413, :title "Image processing", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 36477012, :title "Concepts in physics", :type :wiki-api.core/category} #{}, {:id 784409, :title "Marketing", :type :wiki-api.core/category} #{}, {:id 5646263, :title "University of California, Los Angeles faculty", :type :wiki-api.core/category} #{}, {:id 772240, :title "Epistemology", :type :wiki-api.core/category} #{}, {:id 9387237, :title "Parametric statistics", :type :wiki-api.core/category} #{{:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 160986, :title "Order statistic", :type :wiki-api.core/article} #{{:id 3864201, :title "Permutations", :type :wiki-api.core/category} {:id 6537978, :title "Summary statistics", :type :wiki-api.core/category}}, {:id 17684887, :title "Statistical approximations", :type :wiki-api.core/category} #{{:id 6539521, :title "Statistical theory", :type :wiki-api.core/category}}, {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 380008, :title "Probably approximately correct learning", :type :wiki-api.core/article} #{{:id 34310097, :title "Computational learning theory", :type :wiki-api.core/category}}, {:id 1019150, :title "Machine translation", :type :wiki-api.core/category} #{{:id 21384136, :title "Computer-assisted translation", :type :wiki-api.core/category} {:id 20941685, :title "Tasks of Natural language processing", :type :wiki-api.core/category}}, {:id 15690807, :title "Count data", :type :wiki-api.core/article} #{{:id 17193061, :title "Statistical data types", :type :wiki-api.core/category}}, {:id 878280, :title "Standards organizations", :type :wiki-api.core/category} #{}, {:id 9588430, :title "Higher-order statistics", :type :wiki-api.core/article} #{{:id 6537978, :title "Summary statistics", :type :wiki-api.core/category}}, {:id 693985, :title "Probability theory", :type :wiki-api.core/category} #{{:id 966983, :title "Applied mathematics", :type :wiki-api.core/category}}, {:id 960021, :title "Natural language processing", :type :wiki-api.core/category} #{{:id 958652, :title "Speech recognition", :type :wiki-api.core/category}}, {:id 10678854, :title "Collective intelligence", :type :wiki-api.core/category} #{}, {:id 946910, :title "Decision theory", :type :wiki-api.core/category} #{{:id 1028771, :title "Control theory", :type :wiki-api.core/category} {:id 1008581, :title "Operations research", :type :wiki-api.core/category} {:id 694008, :title "Statistics", :type :wiki-api.core/category} {:id 693985, :title "Probability theory", :type :wiki-api.core/category} {:id 34379506, :title "Epistemology of science", :type :wiki-api.core/category}}, {:id 32190039, :title "Speech", :type :wiki-api.core/category} #{{:id 709574, :title "Human communication", :type :wiki-api.core/category}}, {:id 1855180, :title "Problem solving", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}}, {:id 1557538, :title "Research methods", :type :wiki-api.core/category} #{{:id 700292, :title "Scientific method", :type :wiki-api.core/category}}, {:id 1775388, :title "K-nearest neighbor algorithm", :type :wiki-api.core/article} #{{:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category} {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category}}, {:id 160995, :title "Statistical significance", :type :wiki-api.core/article} #{{:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category}}, {:id 8330403, :title "Dirichlet process", :type :wiki-api.core/article} #{{:id 767343, :title "Stochastic processes", :type :wiki-api.core/category} {:id 21936671, :title "Non-parametric Bayesian methods", :type :wiki-api.core/category}}, {:id 4289067, :title "Computational neuroscience", :type :wiki-api.core/category} #{{:id 10690420, :title "Cognitive neuroscience", :type :wiki-api.core/category} {:id 1754736, :title "Cybernetics", :type :wiki-api.core/category}}, {:id 34379506, :title "Epistemology of science", :type :wiki-api.core/category} #{{:id 772270, :title "Philosophy of science", :type :wiki-api.core/category}}, {:id 36286862, :title "Behavioral and social facets of systemic risk", :type :wiki-api.core/category} #{}, {:id 17506498, :title "Logic and statistics", :type :wiki-api.core/category} #{{:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 744038, :title "Philosophy of language", :type :wiki-api.core/category} #{}, {:id 694025, :title "Information theory", :type :wiki-api.core/category} #{{:id 990361, :title "Theoretical computer science", :type :wiki-api.core/category} {:id 709243, :title "Communication", :type :wiki-api.core/category} {:id 6539521, :title "Statistical theory", :type :wiki-api.core/category} {:id 21917434, :title "Information Age", :type :wiki-api.core/category}}, {:id 699134, :title "Formal languages", :type :wiki-api.core/category} #{{:id 990361, :title "Theoretical computer science", :type :wiki-api.core/category} {:id 733122, :title "Grammar", :type :wiki-api.core/category}}, {:id 966983, :title "Applied mathematics", :type :wiki-api.core/category} #{{:id 25825321, :title "Mathematical sciences", :type :wiki-api.core/category}}, {:id 434897, :title "Hough transform", :type :wiki-api.core/article} #{{:id 797088, :title "Computer vision", :type :wiki-api.core/category}}, {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category} #{{:id 763505, :title "Signal processing", :type :wiki-api.core/category} {:id 17193265, :title "Statistical inference", :type :wiki-api.core/category} {:id 1028771, :title "Control theory", :type :wiki-api.core/category}}, {:id 5175143, :title "Geostatistics", :type :wiki-api.core/category} #{{:id 17228412, :title "Spatial data analysis", :type :wiki-api.core/category}}, {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 3670357, :title "Markov logic network", :type :wiki-api.core/article} #{{:id 22718453, :title "Markov networks", :type :wiki-api.core/category} {:id 990361, :title "Theoretical computer science", :type :wiki-api.core/category}}, {:id 801135, :title "Conditional independence", :type :wiki-api.core/article} #{{:id 17652827, :title "Statistical dependence", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 693985, :title "Probability theory", :type :wiki-api.core/category}}, {:id 8050180, :title "Multi-agent systems", :type :wiki-api.core/category} #{{:id 17612277, :title "Multi-robot systems", :type :wiki-api.core/category} {:id 34698963, :title "Methods in sociology", :type :wiki-api.core/category} {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category}}, {:id 2538775, :title "Predictive modelling", :type :wiki-api.core/article} #{{:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} {:id 17193471, :title "Statistical models", :type :wiki-api.core/category} {:id 1228459, :title "Business intelligence", :type :wiki-api.core/category}}, {:id 914736, :title "Programming paradigms", :type :wiki-api.core/category} #{{:id 5699671, :title "Programming language topics", :type :wiki-api.core/category}}, {:id 11472332, :title "Articles containing proofs", :type :wiki-api.core/category} #{}, {:id 10983, :title "First-order logic", :type :wiki-api.core/article} #{{:id 693979, :title "Model theory", :type :wiki-api.core/category} {:id 15522913, :title "Predicate logic", :type :wiki-api.core/category}}, {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article} #{{:id 17652827, :title "Statistical dependence", :type :wiki-api.core/category} {:id 693985, :title "Probability theory", :type :wiki-api.core/category}}, {:id 16920, :title "Knowledge representation and reasoning", :type :wiki-api.core/article} #{{:id 914736, :title "Programming paradigms", :type :wiki-api.core/category} {:id 26478223, :title "Reasoning", :type :wiki-api.core/category} {:id 3224825, :title "Scientific modeling", :type :wiki-api.core/category} {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category}}, {:id 357672, :title "Posterior probability", :type :wiki-api.core/article} #{{:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category}}, {:id 5699671, :title "Programming language topics", :type :wiki-api.core/category} #{}, {:id 21981503, :title "Conditionals", :type :wiki-api.core/category} #{}, {:id 4890, :title "Bayesian probability", :type :wiki-api.core/article} #{{:id 772270, :title "Philosophy of science", :type :wiki-api.core/category} {:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category} {:id 17735029, :title "Probability interpretations", :type :wiki-api.core/category} {:id 22712867, :title "Justification", :type :wiki-api.core/category} {:id 1152426, :title "Philosophy of mathematics", :type :wiki-api.core/category}}, {:id 10974311, :title "Conditional constructs", :type :wiki-api.core/category} #{{:id 21981503, :title "Conditionals", :type :wiki-api.core/category}}, {:id 22691076, :title "Graphical models", :type :wiki-api.core/category} #{{:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category} {:id 28927577, :title "Probabilistic models", :type :wiki-api.core/category} {:id 17193471, :title "Statistical models", :type :wiki-api.core/category}}, {:id 1686106, :title "Israeli computer scientists", :type :wiki-api.core/category} #{}, {:id 22712867, :title "Justification", :type :wiki-api.core/category} #{}, {:id 7279789, :title "Information retrieval", :type :wiki-api.core/category} #{{:id 28335948, :title "Articles with inconsistent citation formats", :type :wiki-api.core/category} {:id 960021, :title "Natural language processing", :type :wiki-api.core/category}}, {:id 87339, :title "Naive Bayes classifier", :type :wiki-api.core/article} #{{:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category}}, {:id 26478223, :title "Reasoning", :type :wiki-api.core/category} #{}, {:id 31176997, :title "Support vector machines", :type :wiki-api.core/category} #{{:id 12535256, :title "Kernel methods for machine learning", :type :wiki-api.core/category} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category}}, {:id 1966814, :title "C4.5 algorithm", :type :wiki-api.core/article} #{{:id 17594154, :title "Decision trees", :type :wiki-api.core/category} {:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category}}, {:id 972730, :title "1936 births", :type :wiki-api.core/category} #{}, {:id 1009209, :title "Information", :type :wiki-api.core/category} #{{:id 16989227, :title "Data", :type :wiki-api.core/category}}, {:id 4094720, :title "Latent class model", :type :wiki-api.core/article} #{{:id 17179027, :title "Categorical data", :type :wiki-api.core/category} {:id 20924581, :title "Latent variable models", :type :wiki-api.core/category}}, {:id 6075324, :title "National Institute of Standards and Technology", :type :wiki-api.core/category} #{{:id 25304497, :title "Government agencies established in 1901", :type :wiki-api.core/category} {:id 21053327, :title "United States Department of Commerce agencies", :type :wiki-api.core/category} {:id 1069987, :title "Organizations in cryptography", :type :wiki-api.core/category} {:id 878280, :title "Standards organizations", :type :wiki-api.core/category} {:id 29313810, :title "Gaithersburg, Maryland", :type :wiki-api.core/category}}, {:id 6759, :title "Context-free grammar", :type :wiki-api.core/article} #{{:id 12469844, :title "Wikipedia articles with ASCII art", :type :wiki-api.core/category} {:id 30774561, :title "1956 in computer science", :type :wiki-api.core/category} {:id 30787370, :title "Compiler construction", :type :wiki-api.core/category} {:id 699134, :title "Formal languages", :type :wiki-api.core/category} {:id 5699671, :title "Programming language topics", :type :wiki-api.core/category}}, {:id 693800, :title "Geography", :type :wiki-api.core/category} #{}, {:id 700292, :title "Scientific method", :type :wiki-api.core/category} #{{:id 1855180, :title "Problem solving", :type :wiki-api.core/category}}, {:id 140806, :title "Maximum likelihood", :type :wiki-api.core/article} #{{:id 29421852, :title "M-estimators", :type :wiki-api.core/category}}, {:id 4003390, :title "Evaluation methods", :type :wiki-api.core/category} #{}, {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} #{{:id 1188828, :title "Logic programming", :type :wiki-api.core/category}}, {:id 15522913, :title "Predicate logic", :type :wiki-api.core/category} #{{:id 11504647, :title "Systems of formal logic", :type :wiki-api.core/category}}, {:id 690777, :title "Linear algebra", :type :wiki-api.core/category} #{}, {:id 17193471, :title "Statistical models", :type :wiki-api.core/category} #{{:id 6539521, :title "Statistical theory", :type :wiki-api.core/category} {:id 3224825, :title "Scientific modeling", :type :wiki-api.core/category}}, {:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category} #{{:id 697506, :title "Numerical analysis", :type :wiki-api.core/category} {:id 5673643, :title "Risk analysis", :type :wiki-api.core/category} {:id 36163957, :title "Stochastic simulation", :type :wiki-api.core/category} {:id 792595, :title "Computational physics", :type :wiki-api.core/category} {:id 6536797, :title "Statistical algorithms", :type :wiki-api.core/category} {:id 17684887, :title "Statistical approximations", :type :wiki-api.core/category} {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category} {:id 20189596, :title "Probabilistic complexity theory", :type :wiki-api.core/category} {:id 1487877, :title "Sampling techniques", :type :wiki-api.core/category} {:id 958609, :title "Statistical mechanics", :type :wiki-api.core/category}}, {:id 336897, :title "Function approximation", :type :wiki-api.core/article} #{{:id 17684887, :title "Statistical approximations", :type :wiki-api.core/category} {:id 17193471, :title "Statistical models", :type :wiki-api.core/category}}, {:id 21936671, :title "Non-parametric Bayesian methods", :type :wiki-api.core/category} #{{:id 9387252, :title "Non-parametric statistics", :type :wiki-api.core/category} {:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category}}, {:id 3424576, :title "Kernel methods", :type :wiki-api.core/article} #{{:id 12535256, :title "Kernel methods for machine learning", :type :wiki-api.core/category} {:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category} {:id 5175143, :title "Geostatistics", :type :wiki-api.core/category}}, {:id 183503, :title "Description logic", :type :wiki-api.core/article} #{{:id 23890667, :title "Knowledge representation languages", :type :wiki-api.core/category} {:id 22166736, :title "Non-classical logic", :type :wiki-api.core/category}}, {:id 1053303, :title "Statistical learning theory", :type :wiki-api.core/article} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 22121360, :title "Fellows of the Association for the Advancement of Artificial Intelligence", :type :wiki-api.core/category} #{{:id 708266, :title "Artificial intelligence researchers", :type :wiki-api.core/category}}, {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 17193471, :title "Statistical models", :type :wiki-api.core/category}}, {:id 20924581, :title "Latent variable models", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 17193471, :title "Statistical models", :type :wiki-api.core/category}}, {:id 31024872, :title "Normal distribution", :type :wiki-api.core/category} #{{:id 28979098, :title "Distributions with conjugate priors", :type :wiki-api.core/category} {:id 35718126, :title "Exponential family distributions", :type :wiki-api.core/category} {:id 2210772, :title "Continuous distributions", :type :wiki-api.core/category} {:id 28978911, :title "Conjugate prior distributions", :type :wiki-api.core/category}}, {:id 1770656, :title "Numerical linear algebra", :type :wiki-api.core/category} #{{:id 697506, :title "Numerical analysis", :type :wiki-api.core/category} {:id 690777, :title "Linear algebra", :type :wiki-api.core/category}}, {:id 21917434, :title "Information Age", :type :wiki-api.core/category} #{}, {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} #{{:id 1034006, :title "Particle physics", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 37832157, :title "Deductive reasoning", :type :wiki-api.core/category} #{{:id 26478223, :title "Reasoning", :type :wiki-api.core/category}}, {:id 1152426, :title "Philosophy of mathematics", :type :wiki-api.core/category} #{}, {:id 20189596, :title "Probabilistic complexity theory", :type :wiki-api.core/category} #{{:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category}}, {:id 1817228, :title "Training set", :type :wiki-api.core/article} #{{:id 14482748, :title "Data analysis", :type :wiki-api.core/category}}, {:id 14426697, :title "Market research", :type :wiki-api.core/category} #{{:id 784409, :title "Marketing", :type :wiki-api.core/category} {:id 4003390, :title "Evaluation methods", :type :wiki-api.core/category}}, {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category} #{{:id 10678854, :title "Collective intelligence", :type :wiki-api.core/category} {:id 1098276, :title "Mathematical terminology", :type :wiki-api.core/category} {:id 3224825, :title "Scientific modeling", :type :wiki-api.core/category} {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category}}, {:id 18070174, :title "Engineering statistics", :type :wiki-api.core/category} #{{:id 4873906, :title "Systems engineering", :type :wiki-api.core/category} {:id 1008581, :title "Operations research", :type :wiki-api.core/category} {:id 11034627, :title "Industrial engineering", :type :wiki-api.core/category}}, {:id 28978911, :title "Conjugate prior distributions", :type :wiki-api.core/category} #{{:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category} {:id 772545, :title "Probability distributions", :type :wiki-api.core/category}}, {:id 879637, :title "Joint probability distribution", :type :wiki-api.core/article} #{{:id 17923612, :title "Types of probability distributions", :type :wiki-api.core/category} {:id 17990732, :title "Theory of probability distributions", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 1792433, :title "Maximum a posteriori estimation", :type :wiki-api.core/article} #{{:id 32549775, :title "Bayesian inference", :type :wiki-api.core/category} {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category}}, {:id 8707155, :title "Statistical shape analysis", :type :wiki-api.core/article} #{{:id 17228412, :title "Spatial data analysis", :type :wiki-api.core/category} {:id 797088, :title "Computer vision", :type :wiki-api.core/category}}, {:id 1098276, :title "Mathematical terminology", :type :wiki-api.core/category} #{}, {:id 1055691, :title "Learning", :type :wiki-api.core/category} #{}, {:id 29421852, :title "M-estimators", :type :wiki-api.core/category} #{{:id 1587689, :title "Estimation theory", :type :wiki-api.core/category}}, {:id 11504647, :title "Systems of formal logic", :type :wiki-api.core/category} #{}, {:id 821959, :title "Matrices", :type :wiki-api.core/category} #{{:id 692453, :title "Matrix theory", :type :wiki-api.core/category}}, {:id 897612, :title "Syntax", :type :wiki-api.core/category} #{{:id 1045088, :title "Semiotics", :type :wiki-api.core/category} {:id 733122, :title "Grammar", :type :wiki-api.core/category}}, {:id 14343887, :title "Precision and recall", :type :wiki-api.core/article} #{{:id 26308484, :title "Summary statistics for contingency tables", :type :wiki-api.core/category} {:id 726312, :title "Bioinformatics", :type :wiki-api.core/category} {:id 7279789, :title "Information retrieval", :type :wiki-api.core/category}}, {:id 3224825, :title "Scientific modeling", :type :wiki-api.core/category} #{{:id 26743152, :title "Conceptual models", :type :wiki-api.core/category} {:id 34379506, :title "Epistemology of science", :type :wiki-api.core/category} {:id 700292, :title "Scientific method", :type :wiki-api.core/category}}, {:id 1487877, :title "Sampling techniques", :type :wiki-api.core/category} #{{:id 34698963, :title "Methods in sociology", :type :wiki-api.core/category} {:id 1557538, :title "Research methods", :type :wiki-api.core/category} {:id 4003390, :title "Evaluation methods", :type :wiki-api.core/category}}, {:id 29313810, :title "Gaithersburg, Maryland", :type :wiki-api.core/category} #{}, {:id 9500290, :title "Experiments", :type :wiki-api.core/category} #{{:id 700292, :title "Scientific method", :type :wiki-api.core/category}}, {:id 9272793, :title "Computational statistics", :type :wiki-api.core/category} #{{:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 427282, :title "Mutual information", :type :wiki-api.core/article} #{{:id 694025, :title "Information theory", :type :wiki-api.core/category}}, {:id 1228459, :title "Business intelligence", :type :wiki-api.core/category} #{}, {:id 20941685, :title "Tasks of Natural language processing", :type :wiki-api.core/category} #{{:id 960021, :title "Natural language processing", :type :wiki-api.core/category}}, {:id 951835, :title "Computer data", :type :wiki-api.core/category} #{{:id 16989227, :title "Data", :type :wiki-api.core/category}}, {:id 472877, :title "Prior probability", :type :wiki-api.core/article} #{{:id 22705265, :title "Probability assessment", :type :wiki-api.core/category} {:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category}}, {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category} #{{:id 1009204, :title "Information science", :type :wiki-api.core/category} {:id 5038508, :title "Information, knowledge, and uncertainty", :type :wiki-api.core/category}}, {:id 689427, :title "Latent semantic analysis", :type :wiki-api.core/article} #{{:id 7279789, :title "Information retrieval", :type :wiki-api.core/category} {:id 20924581, :title "Latent variable models", :type :wiki-api.core/category}}, {:id 958609, :title "Statistical mechanics", :type :wiki-api.core/category} #{}, {:id 32611713, :title "Emerging technologies", :type :wiki-api.core/category} #{}, {:id 4183228, :title "Israeli philosophers", :type :wiki-api.core/category} #{}, {:id 15325165, :title "Data clustering algorithms", :type :wiki-api.core/category} #{{:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category}}, {:id 25174161, :title "Open problems", :type :wiki-api.core/category} #{}}, :out-map {{:id 763505, :title "Signal processing", :type :wiki-api.core/category} #{{:id 962413, :title "Image processing", :type :wiki-api.core/category} {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category}}, {:id 5486694, :title "1972 introductions", :type :wiki-api.core/category} #{{:id 1188828, :title "Logic programming", :type :wiki-api.core/category}}, {:id 693702, :title "Functional programming", :type :wiki-api.core/category} #{{:id 2023695, :title "Pattern matching", :type :wiki-api.core/category}}, {:id 1228702, :title "Product management", :type :wiki-api.core/category} #{{:id 36425555, :title "Factor analysis", :type :wiki-api.core/category}}, {:id 598776, :title "Score (statistics)", :type :wiki-api.core/article} #{}, {:id 536080, :title "Student's t-test", :type :wiki-api.core/article} #{}, {:id 690672, :title "Abstract algebra", :type :wiki-api.core/category} #{{:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article}}, {:id 2675045, :title "Quality", :type :wiki-api.core/category} #{{:id 31195693, :title "Design of experiments", :type :wiki-api.core/category}}, {:id 17228412, :title "Spatial data analysis", :type :wiki-api.core/category} #{{:id 5175143, :title "Geostatistics", :type :wiki-api.core/category} {:id 8707155, :title "Statistical shape analysis", :type :wiki-api.core/article}}, {:id 10690420, :title "Cognitive neuroscience", :type :wiki-api.core/category} #{{:id 4289067, :title "Computational neuroscience", :type :wiki-api.core/category}}, {:id 1034006, :title "Particle physics", :type :wiki-api.core/category} #{{:id 14482748, :title "Data analysis", :type :wiki-api.core/category}}, {:id 1188828, :title "Logic programming", :type :wiki-api.core/category} #{{:id 36312376, :title "Belief revision", :type :wiki-api.core/category} {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category}}, {:id 22820037, :title "Scientific revolution", :type :wiki-api.core/category} #{{:id 30319607, :title "Human–computer interaction", :type :wiki-api.core/category}}, {:id 767343, :title "Stochastic processes", :type :wiki-api.core/category} #{{:id 36163957, :title "Stochastic simulation", :type :wiki-api.core/category} {:id 8330403, :title "Dirichlet process", :type :wiki-api.core/article}}, {:id 17179027, :title "Categorical data", :type :wiki-api.core/category} #{{:id 226631, :title "Logistic regression", :type :wiki-api.core/article} {:id 26308484, :title "Summary statistics for contingency tables", :type :wiki-api.core/category} {:id 4094720, :title "Latent class model", :type :wiki-api.core/article}}, {:id 23173987, :title "Heuristic algorithms", :type :wiki-api.core/category} #{{:id 172244, :title "Simulated annealing", :type :wiki-api.core/article}}, {:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} #{}, {:id 1009204, :title "Information science", :type :wiki-api.core/category} #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 31454449, :title "Human–machine interaction", :type :wiki-api.core/category} {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category}}, {:id 787876, :title "Computational linguistics", :type :wiki-api.core/category} #{{:id 958652, :title "Speech recognition", :type :wiki-api.core/category}}, {:id 24059390, :title "Markov models", :type :wiki-api.core/category} #{{:id 22718453, :title "Markov networks", :type :wiki-api.core/category} {:id 11737376, :title "Statistical natural language processing", :type :wiki-api.core/category} {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} {:id 31813917, :title "Hidden Markov models", :type :wiki-api.core/category}}, {:id 8190902, :title "Anomaly detection", :type :wiki-api.core/article} #{}, {:id 3234221, :title "Analysis of algorithms", :type :wiki-api.core/category} #{{:id 405944, :title "Time complexity", :type :wiki-api.core/article}}, {:id 22705265, :title "Probability assessment", :type :wiki-api.core/category} #{{:id 472877, :title "Prior probability", :type :wiki-api.core/article}}, {:id 6538378, :title "Statistical data sets", :type :wiki-api.core/category} #{{:id 8495, :title "Data set", :type :wiki-api.core/article}}, {:id 32549775, :title "Bayesian inference", :type :wiki-api.core/category} #{{:id 1792433, :title "Maximum a posteriori estimation", :type :wiki-api.core/article}}, {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} #{}, {:id 22718453, :title "Markov networks", :type :wiki-api.core/category} #{{:id 1323985, :title "Markov random field", :type :wiki-api.core/article} {:id 3670357, :title "Markov logic network", :type :wiki-api.core/article}}, {:id 34044100, :title "Optimization algorithms and methods", :type :wiki-api.core/category} #{{:id 172244, :title "Simulated annealing", :type :wiki-api.core/article} {:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article} {:id 313942, :title "Local search (optimization)", :type :wiki-api.core/article}}, {:id 25304497, :title "Government agencies established in 1901", :type :wiki-api.core/category} #{{:id 6075324, :title "National Institute of Standards and Technology", :type :wiki-api.core/category}}, {:id 958652, :title "Speech recognition", :type :wiki-api.core/category} #{{:id 7699923, :title "Speech processing", :type :wiki-api.core/category} {:id 960021, :title "Natural language processing", :type :wiki-api.core/category}}, {:id 744360, :title "Networks", :type :wiki-api.core/category} #{{:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 6890644, :title "Singular value decomposition", :type :wiki-api.core/category} #{}, {:id 5206601, :title "Data mining", :type :wiki-api.core/category} #{{:id 8190902, :title "Anomaly detection", :type :wiki-api.core/article} {:id 22187885, :title "List of machine learning algorithms", :type :wiki-api.core/article} {:id 1331441, :title "Document classification", :type :wiki-api.core/article} {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} {:id 318439, :title "Text mining", :type :wiki-api.core/article} {:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category}}, {:id 700074, :title "Logical fallacies", :type :wiki-api.core/category} #{{:id 24104134, :title "Conditional probability", :type :wiki-api.core/article}}, {:id 17503782, :title "Formal sciences", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category} {:id 1028771, :title "Control theory", :type :wiki-api.core/category}}, {:id 1323985, :title "Markov random field", :type :wiki-api.core/article} #{}, {:id 2883137, :title "Parametric model", :type :wiki-api.core/article} #{}, {:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category} #{{:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} {:id 3697698, :title "Statistical tests", :type :wiki-api.core/category} {:id 160995, :title "Statistical significance", :type :wiki-api.core/article}}, {:id 871681, :title "Mixture model", :type :wiki-api.core/article} #{}, {:id 990361, :title "Theoretical computer science", :type :wiki-api.core/category} #{{:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} {:id 34310097, :title "Computational learning theory", :type :wiki-api.core/category} {:id 694025, :title "Information theory", :type :wiki-api.core/category} {:id 699134, :title "Formal languages", :type :wiki-api.core/category} {:id 3670357, :title "Markov logic network", :type :wiki-api.core/article}}, {:id 25825321, :title "Mathematical sciences", :type :wiki-api.core/category} #{{:id 966983, :title "Applied mathematics", :type :wiki-api.core/category}}, {:id 6535800, :title "Covariance and correlation", :type :wiki-api.core/category} #{{:id 191752, :title "Covariance matrix", :type :wiki-api.core/article}}, {:id 1195726, :title "Error", :type :wiki-api.core/category} #{{:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article}}, {:id 36425555, :title "Factor analysis", :type :wiki-api.core/category} #{}, {:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} #{{:id 3234221, :title "Analysis of algorithms", :type :wiki-api.core/category} {:id 4628120, :title "Computational resources", :type :wiki-api.core/category} {:id 4594748, :title "Computational problems", :type :wiki-api.core/category} {:id 20189596, :title "Probabilistic complexity theory", :type :wiki-api.core/category}}, {:id 3697698, :title "Statistical tests", :type :wiki-api.core/category} #{{:id 536080, :title "Student's t-test", :type :wiki-api.core/article} {:id 45035, :title "Likelihood-ratio test", :type :wiki-api.core/article}}, {:id 4842680, :title "Data security", :type :wiki-api.core/category} #{{:id 8190902, :title "Anomaly detection", :type :wiki-api.core/article}}, {:id 1179950, :title "Feature selection", :type :wiki-api.core/article} #{}, {:id 4628120, :title "Computational resources", :type :wiki-api.core/category} #{{:id 405944, :title "Time complexity", :type :wiki-api.core/article}}, {:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} #{}, {:id 8135339, :title "Articles with example Haskell code", :type :wiki-api.core/category} #{{:id 2023695, :title "Pattern matching", :type :wiki-api.core/category}}, {:id 11737376, :title "Statistical natural language processing", :type :wiki-api.core/category} #{{:id 318439, :title "Text mining", :type :wiki-api.core/article} {:id 1911810, :title "Language model", :type :wiki-api.core/article}}, {:id 24104134, :title "Conditional probability", :type :wiki-api.core/article} #{}, {:id 22187885, :title "List of machine learning algorithms", :type :wiki-api.core/article} #{}, {:id 9387252, :title "Non-parametric statistics", :type :wiki-api.core/category} #{{:id 18462661, :title "Resampling (statistics)", :type :wiki-api.core/category} {:id 21936671, :title "Non-parametric Bayesian methods", :type :wiki-api.core/category}}, {:id 31229429, :title "Fellow Members of the IEEE", :type :wiki-api.core/category} #{{:id 699964, :title "Judea Pearl", :type :wiki-api.core/article}}, {:id 30982373, :title "Missing data", :type :wiki-api.core/category} #{{:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article} {:id 17917391, :title "Missing completely at random", :type :wiki-api.core/article}}, {:id 18462661, :title "Resampling (statistics)", :type :wiki-api.core/category} #{}, {:id 692453, :title "Matrix theory", :type :wiki-api.core/category} #{{:id 22476294, :title "Matrix decompositions", :type :wiki-api.core/category} {:id 821959, :title "Matrices", :type :wiki-api.core/category}}, {:id 709243, :title "Communication", :type :wiki-api.core/category} #{{:id 694025, :title "Information theory", :type :wiki-api.core/category}}, {:id 405944, :title "Time complexity", :type :wiki-api.core/article} #{}, {:id 7484211, :title "Fellows of the American Academy of Arts and Sciences", :type :wiki-api.core/category} #{{:id 699964, :title "Judea Pearl", :type :wiki-api.core/article}}, {:id 598971, :title "Fisher information", :type :wiki-api.core/article} #{}, {:id 772270, :title "Philosophy of science", :type :wiki-api.core/category} #{{:id 22820037, :title "Scientific revolution", :type :wiki-api.core/category} {:id 34379506, :title "Epistemology of science", :type :wiki-api.core/category} {:id 4890, :title "Bayesian probability", :type :wiki-api.core/article}}, {:id 12469844, :title "Wikipedia articles with ASCII art", :type :wiki-api.core/category} #{{:id 6759, :title "Context-free grammar", :type :wiki-api.core/article}}, {:id 1331441, :title "Document classification", :type :wiki-api.core/article} #{}, {:id 30319607, :title "Human–computer interaction", :type :wiki-api.core/category} #{{:id 958652, :title "Speech recognition", :type :wiki-api.core/category}}, {:id 2157920, :title "Technology in society", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}}, {:id 21053327, :title "United States Department of Commerce agencies", :type :wiki-api.core/category} #{{:id 6075324, :title "National Institute of Standards and Technology", :type :wiki-api.core/category}}, {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} #{{:id 871681, :title "Mixture model", :type :wiki-api.core/article} {:id 15325165, :title "Data clustering algorithms", :type :wiki-api.core/category}}, {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} #{}, {:id 709574, :title "Human communication", :type :wiki-api.core/category} #{{:id 31454449, :title "Human–machine interaction", :type :wiki-api.core/category} {:id 32190039, :title "Speech", :type :wiki-api.core/category}}, {:id 24960643, :title "Statistical outliers", :type :wiki-api.core/category} #{{:id 8190902, :title "Anomaly detection", :type :wiki-api.core/article}}, {:id 22042655, :title "Statistical principles", :type :wiki-api.core/category} #{{:id 140841, :title "Sufficient statistic", :type :wiki-api.core/article}}, {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category} #{}, {:id 10978278, :title "Quantitative research", :type :wiki-api.core/category} #{{:id 4822234, :title "Mathematical and quantitative methods (economics)", :type :wiki-api.core/category}}, {:id 5065063, :title "Network architecture", :type :wiki-api.core/category} #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 31454449, :title "Human–machine interaction", :type :wiki-api.core/category} #{{:id 30319607, :title "Human–computer interaction", :type :wiki-api.core/category}}, {:id 31195693, :title "Design of experiments", :type :wiki-api.core/category} #{{:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} {:id 598971, :title "Fisher information", :type :wiki-api.core/article}}, {:id 7699923, :title "Speech processing", :type :wiki-api.core/category} #{}, {:id 22991469, :title "Inductive reasoning", :type :wiki-api.core/category} #{{:id 17193265, :title "Statistical inference", :type :wiki-api.core/category}}, {:id 191752, :title "Covariance matrix", :type :wiki-api.core/article} #{}, {:id 30774561, :title "1956 in computer science", :type :wiki-api.core/category} #{{:id 6759, :title "Context-free grammar", :type :wiki-api.core/article}}, {:id 26743152, :title "Conceptual models", :type :wiki-api.core/category} #{{:id 3224825, :title "Scientific modeling", :type :wiki-api.core/category}}, {:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category} #{{:id 32549775, :title "Bayesian inference", :type :wiki-api.core/category} {:id 28979098, :title "Distributions with conjugate priors", :type :wiki-api.core/category} {:id 357672, :title "Posterior probability", :type :wiki-api.core/article} {:id 4890, :title "Bayesian probability", :type :wiki-api.core/article} {:id 22691076, :title "Graphical models", :type :wiki-api.core/category} {:id 87339, :title "Naive Bayes classifier", :type :wiki-api.core/article} {:id 21936671, :title "Non-parametric Bayesian methods", :type :wiki-api.core/category} {:id 28978911, :title "Conjugate prior distributions", :type :wiki-api.core/category} {:id 472877, :title "Prior probability", :type :wiki-api.core/article}}, {:id 36312376, :title "Belief revision", :type :wiki-api.core/category} #{{:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article}}, {:id 45035, :title "Likelihood-ratio test", :type :wiki-api.core/article} #{}, {:id 17193061, :title "Statistical data types", :type :wiki-api.core/category} #{{:id 17228412, :title "Spatial data analysis", :type :wiki-api.core/category} {:id 767343, :title "Stochastic processes", :type :wiki-api.core/category} {:id 17179027, :title "Categorical data", :type :wiki-api.core/category} {:id 30982373, :title "Missing data", :type :wiki-api.core/category} {:id 24960643, :title "Statistical outliers", :type :wiki-api.core/category} {:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category} {:id 5490673, :title "Quality control", :type :wiki-api.core/category} {:id 15690807, :title "Count data", :type :wiki-api.core/article}}, {:id 772545, :title "Probability distributions", :type :wiki-api.core/category} #{{:id 28979098, :title "Distributions with conjugate priors", :type :wiki-api.core/category} {:id 35718126, :title "Exponential family distributions", :type :wiki-api.core/category} {:id 17923612, :title "Types of probability distributions", :type :wiki-api.core/category} {:id 2210772, :title "Continuous distributions", :type :wiki-api.core/category} {:id 17990732, :title "Theory of probability distributions", :type :wiki-api.core/category} {:id 28978911, :title "Conjugate prior distributions", :type :wiki-api.core/category}}, {:id 16989227, :title "Data", :type :wiki-api.core/category} #{{:id 1009209, :title "Information", :type :wiki-api.core/category} {:id 951835, :title "Computer data", :type :wiki-api.core/category}}, {:id 23890667, :title "Knowledge representation languages", :type :wiki-api.core/category} #{{:id 183503, :title "Description logic", :type :wiki-api.core/article}}, {:id 76340, :title "Principal component analysis", :type :wiki-api.core/article} #{}, {:id 5232797, :title "Rutgers University alumni", :type :wiki-api.core/category} #{{:id 699964, :title "Judea Pearl", :type :wiki-api.core/article}}, {:id 21384136, :title "Computer-assisted translation", :type :wiki-api.core/category} #{{:id 1019150, :title "Machine translation", :type :wiki-api.core/category}}, {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} #{{:id 1775388, :title "K-nearest neighbor algorithm", :type :wiki-api.core/article}}, {:id 17193265, :title "Statistical inference", :type :wiki-api.core/category} #{{:id 32549775, :title "Bayesian inference", :type :wiki-api.core/category} {:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category} {:id 27955209, :title "Model selection", :type :wiki-api.core/category} {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category}}, {:id 3190431, :title "Spatial analysis", :type :wiki-api.core/article} #{}, {:id 691866, :title "Functional analysis", :type :wiki-api.core/category} #{{:id 6890644, :title "Singular value decomposition", :type :wiki-api.core/category}}, {:id 31813917, :title "Hidden Markov models", :type :wiki-api.core/category} #{}, {:id 28979098, :title "Distributions with conjugate priors", :type :wiki-api.core/category} #{{:id 31024872, :title "Normal distribution", :type :wiki-api.core/category}}, {:id 3864201, :title "Permutations", :type :wiki-api.core/category} #{{:id 160986, :title "Order statistic", :type :wiki-api.core/article}}, {:id 35718126, :title "Exponential family distributions", :type :wiki-api.core/category} #{{:id 31024872, :title "Normal distribution", :type :wiki-api.core/category}}, {:id 226631, :title "Logistic regression", :type :wiki-api.core/article} #{}, {:id 946892, :title "Econometrics", :type :wiki-api.core/category} #{{:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 17612277, :title "Multi-robot systems", :type :wiki-api.core/category} #{{:id 8050180, :title "Multi-agent systems", :type :wiki-api.core/category}}, {:id 716309, :title "Cartography", :type :wiki-api.core/category} #{{:id 3190431, :title "Spatial analysis", :type :wiki-api.core/article}}, {:id 172244, :title "Simulated annealing", :type :wiki-api.core/article} #{}, {:id 693979, :title "Model theory", :type :wiki-api.core/category} #{{:id 10983, :title "First-order logic", :type :wiki-api.core/article}}, {:id 1754736, :title "Cybernetics", :type :wiki-api.core/category} #{{:id 1045088, :title "Semiotics", :type :wiki-api.core/category} {:id 1028771, :title "Control theory", :type :wiki-api.core/category} {:id 4289067, :title "Computational neuroscience", :type :wiki-api.core/category}}, {:id 17652827, :title "Statistical dependence", :type :wiki-api.core/category} #{{:id 6535800, :title "Covariance and correlation", :type :wiki-api.core/category} {:id 801135, :title "Conditional independence", :type :wiki-api.core/article} {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article}}, {:id 2871217, :title "Educational psychology", :type :wiki-api.core/category} #{{:id 804551, :title "Psychometrics", :type :wiki-api.core/category}}, {:id 699964, :title "Judea Pearl", :type :wiki-api.core/article} #{}, {:id 2276471, :title "Databases", :type :wiki-api.core/category} #{}, {:id 697506, :title "Numerical analysis", :type :wiki-api.core/category} #{{:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category} {:id 1770656, :title "Numerical linear algebra", :type :wiki-api.core/category}}, {:id 318439, :title "Text mining", :type :wiki-api.core/article} #{}, {:id 17923612, :title "Types of probability distributions", :type :wiki-api.core/category} #{{:id 504458, :title "Conditional probability distribution", :type :wiki-api.core/article} {:id 879637, :title "Joint probability distribution", :type :wiki-api.core/article}}, {:id 34698963, :title "Methods in sociology", :type :wiki-api.core/category} #{{:id 8050180, :title "Multi-agent systems", :type :wiki-api.core/category} {:id 1487877, :title "Sampling techniques", :type :wiki-api.core/category}}, {:id 4594748, :title "Computational problems", :type :wiki-api.core/category} #{{:id 1126536, :title "Optimization problem", :type :wiki-api.core/article}}, {:id 5673643, :title "Risk analysis", :type :wiki-api.core/category} #{{:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category}}, {:id 1045088, :title "Semiotics", :type :wiki-api.core/category} #{{:id 897612, :title "Syntax", :type :wiki-api.core/category}}, {:id 28927577, :title "Probabilistic models", :type :wiki-api.core/category} #{{:id 871681, :title "Mixture model", :type :wiki-api.core/article} {:id 22691076, :title "Graphical models", :type :wiki-api.core/category}}, {:id 20926, :title "Supervised learning", :type :wiki-api.core/article} #{}, {:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category} #{{:id 30319607, :title "Human–computer interaction", :type :wiki-api.core/category} {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 708266, :title "Artificial intelligence researchers", :type :wiki-api.core/category} {:id 1532663, :title "Artificial intelligence applications", :type :wiki-api.core/category} {:id 1855180, :title "Problem solving", :type :wiki-api.core/category}}, {:id 12535256, :title "Kernel methods for machine learning", :type :wiki-api.core/category} #{{:id 31176997, :title "Support vector machines", :type :wiki-api.core/category} {:id 3424576, :title "Kernel methods", :type :wiki-api.core/article}}, {:id 5038508, :title "Information, knowledge, and uncertainty", :type :wiki-api.core/category} #{{:id 796635, :title "Knowledge representation", :type :wiki-api.core/category}}, {:id 8495, :title "Data set", :type :wiki-api.core/article} #{}, {:id 6537978, :title "Summary statistics", :type :wiki-api.core/category} #{{:id 191752, :title "Covariance matrix", :type :wiki-api.core/article} {:id 26308484, :title "Summary statistics for contingency tables", :type :wiki-api.core/category} {:id 160986, :title "Order statistic", :type :wiki-api.core/article} {:id 9588430, :title "Higher-order statistics", :type :wiki-api.core/article}}, {:id 26263822, :title "Packaging machinery", :type :wiki-api.core/category} #{{:id 797088, :title "Computer vision", :type :wiki-api.core/category}}, {:id 954323, :title "Tikhonov regularization", :type :wiki-api.core/article} #{}, {:id 4822234, :title "Mathematical and quantitative methods (economics)", :type :wiki-api.core/category} #{{:id 946892, :title "Econometrics", :type :wiki-api.core/category} {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 22476294, :title "Matrix decompositions", :type :wiki-api.core/category} #{{:id 6890644, :title "Singular value decomposition", :type :wiki-api.core/category} {:id 76340, :title "Principal component analysis", :type :wiki-api.core/article}}, {:id 22589574, :title "Instance-based learning", :type :wiki-api.core/article} #{}, {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} #{{:id 34044100, :title "Optimization algorithms and methods", :type :wiki-api.core/category}}, {:id 30876902, :title "Independent and identically distributed random variables", :type :wiki-api.core/article} #{}, {:id 8439989, :title "Jewish American scientists", :type :wiki-api.core/category} #{{:id 699964, :title "Judea Pearl", :type :wiki-api.core/article}}, {:id 34310097, :title "Computational learning theory", :type :wiki-api.core/category} #{{:id 380008, :title "Probably approximately correct learning", :type :wiki-api.core/article}}, {:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article} #{}, {:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category} #{{:id 1179950, :title "Feature selection", :type :wiki-api.core/article} {:id 76340, :title "Principal component analysis", :type :wiki-api.core/article}}, {:id 331680, :title "Edge detection", :type :wiki-api.core/article} #{}, {:id 21764485, :title "Statistical ratios", :type :wiki-api.core/category} #{{:id 24104134, :title "Conditional probability", :type :wiki-api.core/article} {:id 45035, :title "Likelihood-ratio test", :type :wiki-api.core/article}}, {:id 8156101, :title "Statistical methods", :type :wiki-api.core/category} #{{:id 536080, :title "Student's t-test", :type :wiki-api.core/article} {:id 31195693, :title "Design of experiments", :type :wiki-api.core/category}}, {:id 26339806, :title "Auxiliary sciences of history", :type :wiki-api.core/category} #{{:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 1069987, :title "Organizations in cryptography", :type :wiki-api.core/category} #{{:id 6075324, :title "National Institute of Standards and Technology", :type :wiki-api.core/category}}, {:id 36163957, :title "Stochastic simulation", :type :wiki-api.core/category} #{{:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category}}, {:id 504458, :title "Conditional probability distribution", :type :wiki-api.core/article} #{}, {:id 733122, :title "Grammar", :type :wiki-api.core/category} #{{:id 699134, :title "Formal languages", :type :wiki-api.core/category} {:id 897612, :title "Syntax", :type :wiki-api.core/category}}, {:id 28335948, :title "Articles with inconsistent citation formats", :type :wiki-api.core/category} #{{:id 7279789, :title "Information retrieval", :type :wiki-api.core/category}}, {:id 1911810, :title "Language model", :type :wiki-api.core/article} #{}, {:id 6539521, :title "Statistical theory", :type :wiki-api.core/category} #{{:id 31195693, :title "Design of experiments", :type :wiki-api.core/category} {:id 17193265, :title "Statistical inference", :type :wiki-api.core/category} {:id 140841, :title "Sufficient statistic", :type :wiki-api.core/article} {:id 17735029, :title "Probability interpretations", :type :wiki-api.core/category} {:id 17684887, :title "Statistical approximations", :type :wiki-api.core/category} {:id 694025, :title "Information theory", :type :wiki-api.core/category} {:id 17193471, :title "Statistical models", :type :wiki-api.core/category}}, {:id 1028771, :title "Control theory", :type :wiki-api.core/category} #{{:id 946910, :title "Decision theory", :type :wiki-api.core/category} {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category}}, {:id 4873906, :title "Systems engineering", :type :wiki-api.core/category} #{{:id 18070174, :title "Engineering statistics", :type :wiki-api.core/category}}, {:id 708266, :title "Artificial intelligence researchers", :type :wiki-api.core/category} #{{:id 22121360, :title "Fellows of the Association for the Advancement of Artificial Intelligence", :type :wiki-api.core/category}}, {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article} #{}, {:id 27248985, :title "Mathematical concepts", :type :wiki-api.core/category} #{{:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article}}, {:id 792595, :title "Computational physics", :type :wiki-api.core/category} #{{:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category}}, {:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category} #{{:id 36425555, :title "Factor analysis", :type :wiki-api.core/category} {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} {:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category}}, {:id 2210772, :title "Continuous distributions", :type :wiki-api.core/category} #{{:id 31024872, :title "Normal distribution", :type :wiki-api.core/category}}, {:id 1008581, :title "Operations research", :type :wiki-api.core/category} #{{:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 946910, :title "Decision theory", :type :wiki-api.core/category} {:id 18070174, :title "Engineering statistics", :type :wiki-api.core/category}}, {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} #{}, {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} #{{:id 2538775, :title "Predictive modelling", :type :wiki-api.core/article} {:id 87339, :title "Naive Bayes classifier", :type :wiki-api.core/article} {:id 31176997, :title "Support vector machines", :type :wiki-api.core/category}}, {:id 17990732, :title "Theory of probability distributions", :type :wiki-api.core/category} #{{:id 6535800, :title "Covariance and correlation", :type :wiki-api.core/category} {:id 879637, :title "Joint probability distribution", :type :wiki-api.core/article}}, {:id 140841, :title "Sufficient statistic", :type :wiki-api.core/article} #{}, {:id 17735029, :title "Probability interpretations", :type :wiki-api.core/category} #{{:id 4890, :title "Bayesian probability", :type :wiki-api.core/article}}, {:id 962413, :title "Image processing", :type :wiki-api.core/category} #{{:id 797088, :title "Computer vision", :type :wiki-api.core/category}}, {:id 17594154, :title "Decision trees", :type :wiki-api.core/category} #{{:id 1966814, :title "C4.5 algorithm", :type :wiki-api.core/article}}, {:id 313942, :title "Local search (optimization)", :type :wiki-api.core/article} #{}, {:id 5490673, :title "Quality control", :type :wiki-api.core/category} #{{:id 31195693, :title "Design of experiments", :type :wiki-api.core/category}}, {:id 694008, :title "Statistics", :type :wiki-api.core/category} #{{:id 6538378, :title "Statistical data sets", :type :wiki-api.core/category} {:id 9387252, :title "Non-parametric statistics", :type :wiki-api.core/category} {:id 22042655, :title "Statistical principles", :type :wiki-api.core/category} {:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category} {:id 17193061, :title "Statistical data types", :type :wiki-api.core/category} {:id 17652827, :title "Statistical dependence", :type :wiki-api.core/category} {:id 5038508, :title "Information, knowledge, and uncertainty", :type :wiki-api.core/category} {:id 6537978, :title "Summary statistics", :type :wiki-api.core/category} {:id 21764485, :title "Statistical ratios", :type :wiki-api.core/category} {:id 8156101, :title "Statistical methods", :type :wiki-api.core/category} {:id 6539521, :title "Statistical theory", :type :wiki-api.core/category} {:id 9387237, :title "Parametric statistics", :type :wiki-api.core/category} {:id 946910, :title "Decision theory", :type :wiki-api.core/category} {:id 17506498, :title "Logic and statistics", :type :wiki-api.core/category} {:id 9272793, :title "Computational statistics", :type :wiki-api.core/category}}, {:id 22166736, :title "Non-classical logic", :type :wiki-api.core/category} #{{:id 183503, :title "Description logic", :type :wiki-api.core/article}}, {:id 1532663, :title "Artificial intelligence applications", :type :wiki-api.core/category} #{{:id 958652, :title "Speech recognition", :type :wiki-api.core/category}}, {:id 17917391, :title "Missing completely at random", :type :wiki-api.core/article} #{}, {:id 27955209, :title "Model selection", :type :wiki-api.core/category} #{{:id 1179950, :title "Feature selection", :type :wiki-api.core/article}}, {:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category} #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} {:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category} {:id 1775388, :title "K-nearest neighbor algorithm", :type :wiki-api.core/article} {:id 1966814, :title "C4.5 algorithm", :type :wiki-api.core/article} {:id 3424576, :title "Kernel methods", :type :wiki-api.core/article}}, {:id 26308484, :title "Summary statistics for contingency tables", :type :wiki-api.core/category} #{{:id 14343887, :title "Precision and recall", :type :wiki-api.core/article}}, {:id 6536797, :title "Statistical algorithms", :type :wiki-api.core/category} #{{:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article} {:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category}}, {:id 30787370, :title "Compiler construction", :type :wiki-api.core/category} #{{:id 6759, :title "Context-free grammar", :type :wiki-api.core/article}}, {:id 706543, :title "Machine learning", :type :wiki-api.core/category} #{{:id 1188828, :title "Logic programming", :type :wiki-api.core/category} {:id 24059390, :title "Markov models", :type :wiki-api.core/category} {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} {:id 958652, :title "Speech recognition", :type :wiki-api.core/category} {:id 990361, :title "Theoretical computer science", :type :wiki-api.core/category} {:id 20926, :title "Supervised learning", :type :wiki-api.core/article} {:id 12535256, :title "Kernel methods for machine learning", :type :wiki-api.core/category} {:id 22589574, :title "Instance-based learning", :type :wiki-api.core/article} {:id 17594154, :title "Decision trees", :type :wiki-api.core/category} {:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 726312, :title "Bioinformatics", :type :wiki-api.core/category} {:id 797088, :title "Computer vision", :type :wiki-api.core/category} {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article} {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category} {:id 801135, :title "Conditional independence", :type :wiki-api.core/article} {:id 1053303, :title "Statistical learning theory", :type :wiki-api.core/article} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article} {:id 20924581, :title "Latent variable models", :type :wiki-api.core/category} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} {:id 879637, :title "Joint probability distribution", :type :wiki-api.core/article}}, {:id 33748657, :title "Geometric measurement", :type :wiki-api.core/category} #{{:id 3175294, :title "Dimension", :type :wiki-api.core/category}}, {:id 22958, :title "Sample space", :type :wiki-api.core/article} #{}, {:id 3175294, :title "Dimension", :type :wiki-api.core/category} #{{:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} {:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category}}, {:id 35997105, :title "Generalized linear models", :type :wiki-api.core/category} #{{:id 226631, :title "Logistic regression", :type :wiki-api.core/article}}, {:id 804551, :title "Psychometrics", :type :wiki-api.core/category} #{{:id 36425555, :title "Factor analysis", :type :wiki-api.core/category} {:id 17193265, :title "Statistical inference", :type :wiki-api.core/category} {:id 17917391, :title "Missing completely at random", :type :wiki-api.core/article}}, {:id 3782398, :title "Living people", :type :wiki-api.core/category} #{{:id 699964, :title "Judea Pearl", :type :wiki-api.core/article}}, {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} #{{:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}}, {:id 726312, :title "Bioinformatics", :type :wiki-api.core/category} #{{:id 31813917, :title "Hidden Markov models", :type :wiki-api.core/category} {:id 14343887, :title "Precision and recall", :type :wiki-api.core/article}}, {:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category} #{}, {:id 916810, :title "Spam filtering", :type :wiki-api.core/category} #{{:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article}}, {:id 11034627, :title "Industrial engineering", :type :wiki-api.core/category} #{{:id 18070174, :title "Engineering statistics", :type :wiki-api.core/category}}, {:id 797088, :title "Computer vision", :type :wiki-api.core/category} #{{:id 331680, :title "Edge detection", :type :wiki-api.core/article} {:id 434897, :title "Hough transform", :type :wiki-api.core/article} {:id 8707155, :title "Statistical shape analysis", :type :wiki-api.core/article}}, {:id 36477012, :title "Concepts in physics", :type :wiki-api.core/category} #{{:id 3175294, :title "Dimension", :type :wiki-api.core/category}}, {:id 784409, :title "Marketing", :type :wiki-api.core/category} #{{:id 14426697, :title "Market research", :type :wiki-api.core/category}}, {:id 5646263, :title "University of California, Los Angeles faculty", :type :wiki-api.core/category} #{{:id 699964, :title "Judea Pearl", :type :wiki-api.core/article}}, {:id 772240, :title "Epistemology", :type :wiki-api.core/category} #{{:id 772270, :title "Philosophy of science", :type :wiki-api.core/category}}, {:id 9387237, :title "Parametric statistics", :type :wiki-api.core/category} #{{:id 536080, :title "Student's t-test", :type :wiki-api.core/article}}, {:id 160986, :title "Order statistic", :type :wiki-api.core/article} #{}, {:id 17684887, :title "Statistical approximations", :type :wiki-api.core/category} #{{:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category} {:id 336897, :title "Function approximation", :type :wiki-api.core/article}}, {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article} #{}, {:id 380008, :title "Probably approximately correct learning", :type :wiki-api.core/article} #{}, {:id 1019150, :title "Machine translation", :type :wiki-api.core/category} #{}, {:id 15690807, :title "Count data", :type :wiki-api.core/article} #{}, {:id 878280, :title "Standards organizations", :type :wiki-api.core/category} #{{:id 6075324, :title "National Institute of Standards and Technology", :type :wiki-api.core/category}}, {:id 9588430, :title "Higher-order statistics", :type :wiki-api.core/article} #{}, {:id 693985, :title "Probability theory", :type :wiki-api.core/category} #{{:id 767343, :title "Stochastic processes", :type :wiki-api.core/category} {:id 1323985, :title "Markov random field", :type :wiki-api.core/article} {:id 24104134, :title "Conditional probability", :type :wiki-api.core/article} {:id 772545, :title "Probability distributions", :type :wiki-api.core/category} {:id 30876902, :title "Independent and identically distributed random variables", :type :wiki-api.core/article} {:id 17735029, :title "Probability interpretations", :type :wiki-api.core/category} {:id 22958, :title "Sample space", :type :wiki-api.core/article} {:id 946910, :title "Decision theory", :type :wiki-api.core/category} {:id 801135, :title "Conditional independence", :type :wiki-api.core/article} {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article}}, {:id 960021, :title "Natural language processing", :type :wiki-api.core/category} #{{:id 11737376, :title "Statistical natural language processing", :type :wiki-api.core/category} {:id 1331441, :title "Document classification", :type :wiki-api.core/article} {:id 7279789, :title "Information retrieval", :type :wiki-api.core/category} {:id 20941685, :title "Tasks of Natural language processing", :type :wiki-api.core/category}}, {:id 10678854, :title "Collective intelligence", :type :wiki-api.core/category} #{{:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}, {:id 946910, :title "Decision theory", :type :wiki-api.core/category} #{{:id 5673643, :title "Risk analysis", :type :wiki-api.core/category} {:id 17594154, :title "Decision trees", :type :wiki-api.core/category}}, {:id 32190039, :title "Speech", :type :wiki-api.core/category} #{{:id 958652, :title "Speech recognition", :type :wiki-api.core/category}}, {:id 1855180, :title "Problem solving", :type :wiki-api.core/category} #{{:id 700292, :title "Scientific method", :type :wiki-api.core/category}}, {:id 1557538, :title "Research methods", :type :wiki-api.core/category} #{{:id 10978278, :title "Quantitative research", :type :wiki-api.core/category} {:id 1487877, :title "Sampling techniques", :type :wiki-api.core/category}}, {:id 1775388, :title "K-nearest neighbor algorithm", :type :wiki-api.core/article} #{}, {:id 160995, :title "Statistical significance", :type :wiki-api.core/article} #{}, {:id 8330403, :title "Dirichlet process", :type :wiki-api.core/article} #{}, {:id 4289067, :title "Computational neuroscience", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}}, {:id 34379506, :title "Epistemology of science", :type :wiki-api.core/category} #{{:id 946910, :title "Decision theory", :type :wiki-api.core/category} {:id 3224825, :title "Scientific modeling", :type :wiki-api.core/category}}, {:id 36286862, :title "Behavioral and social facets of systemic risk", :type :wiki-api.core/category} #{{:id 30319607, :title "Human–computer interaction", :type :wiki-api.core/category}}, {:id 17506498, :title "Logic and statistics", :type :wiki-api.core/category} #{{:id 17193265, :title "Statistical inference", :type :wiki-api.core/category}}, {:id 744038, :title "Philosophy of language", :type :wiki-api.core/category} #{{:id 1045088, :title "Semiotics", :type :wiki-api.core/category}}, {:id 694025, :title "Information theory", :type :wiki-api.core/category} #{{:id 598971, :title "Fisher information", :type :wiki-api.core/article} {:id 427282, :title "Mutual information", :type :wiki-api.core/article}}, {:id 699134, :title "Formal languages", :type :wiki-api.core/category} #{{:id 2023695, :title "Pattern matching", :type :wiki-api.core/category} {:id 6759, :title "Context-free grammar", :type :wiki-api.core/article}}, {:id 966983, :title "Applied mathematics", :type :wiki-api.core/category} #{{:id 763505, :title "Signal processing", :type :wiki-api.core/category} {:id 1754736, :title "Cybernetics", :type :wiki-api.core/category} {:id 1008581, :title "Operations research", :type :wiki-api.core/category} {:id 693985, :title "Probability theory", :type :wiki-api.core/category}}, {:id 434897, :title "Hough transform", :type :wiki-api.core/article} #{}, {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category} #{{:id 598776, :title "Score (statistics)", :type :wiki-api.core/article} {:id 598971, :title "Fisher information", :type :wiki-api.core/article} {:id 954323, :title "Tikhonov regularization", :type :wiki-api.core/article} {:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article} {:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category} {:id 1792433, :title "Maximum a posteriori estimation", :type :wiki-api.core/article} {:id 29421852, :title "M-estimators", :type :wiki-api.core/category}}, {:id 5175143, :title "Geostatistics", :type :wiki-api.core/category} #{{:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} {:id 3190431, :title "Spatial analysis", :type :wiki-api.core/article} {:id 3424576, :title "Kernel methods", :type :wiki-api.core/article}}, {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category} #{{:id 22187885, :title "List of machine learning algorithms", :type :wiki-api.core/article} {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} {:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article} {:id 1775388, :title "K-nearest neighbor algorithm", :type :wiki-api.core/article}}, {:id 3670357, :title "Markov logic network", :type :wiki-api.core/article} #{}, {:id 801135, :title "Conditional independence", :type :wiki-api.core/article} #{}, {:id 8050180, :title "Multi-agent systems", :type :wiki-api.core/category} #{}, {:id 2538775, :title "Predictive modelling", :type :wiki-api.core/article} #{}, {:id 914736, :title "Programming paradigms", :type :wiki-api.core/category} #{{:id 1188828, :title "Logic programming", :type :wiki-api.core/category} {:id 16920, :title "Knowledge representation and reasoning", :type :wiki-api.core/article}}, {:id 11472332, :title "Articles containing proofs", :type :wiki-api.core/category} #{{:id 140841, :title "Sufficient statistic", :type :wiki-api.core/article}}, {:id 10983, :title "First-order logic", :type :wiki-api.core/article} #{}, {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article} #{}, {:id 16920, :title "Knowledge representation and reasoning", :type :wiki-api.core/article} #{}, {:id 357672, :title "Posterior probability", :type :wiki-api.core/article} #{}, {:id 5699671, :title "Programming language topics", :type :wiki-api.core/category} #{{:id 914736, :title "Programming paradigms", :type :wiki-api.core/category} {:id 6759, :title "Context-free grammar", :type :wiki-api.core/article}}, {:id 21981503, :title "Conditionals", :type :wiki-api.core/category} #{{:id 24104134, :title "Conditional probability", :type :wiki-api.core/article} {:id 10974311, :title "Conditional constructs", :type :wiki-api.core/category}}, {:id 4890, :title "Bayesian probability", :type :wiki-api.core/article} #{}, {:id 10974311, :title "Conditional constructs", :type :wiki-api.core/category} #{{:id 2023695, :title "Pattern matching", :type :wiki-api.core/category}}, {:id 22691076, :title "Graphical models", :type :wiki-api.core/category} #{{:id 24059390, :title "Markov models", :type :wiki-api.core/category} {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category}}, {:id 1686106, :title "Israeli computer scientists", :type :wiki-api.core/category} #{{:id 699964, :title "Judea Pearl", :type :wiki-api.core/article}}, {:id 22712867, :title "Justification", :type :wiki-api.core/category} #{{:id 4890, :title "Bayesian probability", :type :wiki-api.core/article}}, {:id 7279789, :title "Information retrieval", :type :wiki-api.core/category} #{{:id 14343887, :title "Precision and recall", :type :wiki-api.core/article} {:id 689427, :title "Latent semantic analysis", :type :wiki-api.core/article}}, {:id 87339, :title "Naive Bayes classifier", :type :wiki-api.core/article} #{}, {:id 26478223, :title "Reasoning", :type :wiki-api.core/category} #{{:id 22991469, :title "Inductive reasoning", :type :wiki-api.core/category} {:id 16920, :title "Knowledge representation and reasoning", :type :wiki-api.core/article} {:id 37832157, :title "Deductive reasoning", :type :wiki-api.core/category}}, {:id 31176997, :title "Support vector machines", :type :wiki-api.core/category} #{}, {:id 1966814, :title "C4.5 algorithm", :type :wiki-api.core/article} #{}, {:id 972730, :title "1936 births", :type :wiki-api.core/category} #{{:id 699964, :title "Judea Pearl", :type :wiki-api.core/article}}, {:id 1009209, :title "Information", :type :wiki-api.core/category} #{{:id 1009204, :title "Information science", :type :wiki-api.core/category} {:id 709243, :title "Communication", :type :wiki-api.core/category} {:id 694008, :title "Statistics", :type :wiki-api.core/category}}, {:id 4094720, :title "Latent class model", :type :wiki-api.core/article} #{}, {:id 6075324, :title "National Institute of Standards and Technology", :type :wiki-api.core/category} #{}, {:id 6759, :title "Context-free grammar", :type :wiki-api.core/article} #{}, {:id 693800, :title "Geography", :type :wiki-api.core/category} #{{:id 3190431, :title "Spatial analysis", :type :wiki-api.core/article}}, {:id 700292, :title "Scientific method", :type :wiki-api.core/category} #{{:id 1557538, :title "Research methods", :type :wiki-api.core/category} {:id 3224825, :title "Scientific modeling", :type :wiki-api.core/category} {:id 9500290, :title "Experiments", :type :wiki-api.core/category}}, {:id 140806, :title "Maximum likelihood", :type :wiki-api.core/article} #{}, {:id 4003390, :title "Evaluation methods", :type :wiki-api.core/category} #{{:id 10978278, :title "Quantitative research", :type :wiki-api.core/category} {:id 14426697, :title "Market research", :type :wiki-api.core/category} {:id 1487877, :title "Sampling techniques", :type :wiki-api.core/category}}, {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} #{}, {:id 15522913, :title "Predicate logic", :type :wiki-api.core/category} #{{:id 10983, :title "First-order logic", :type :wiki-api.core/article}}, {:id 690777, :title "Linear algebra", :type :wiki-api.core/category} #{{:id 692453, :title "Matrix theory", :type :wiki-api.core/category} {:id 954323, :title "Tikhonov regularization", :type :wiki-api.core/article} {:id 1770656, :title "Numerical linear algebra", :type :wiki-api.core/category}}, {:id 17193471, :title "Statistical models", :type :wiki-api.core/category} #{{:id 2883137, :title "Parametric model", :type :wiki-api.core/article} {:id 30876902, :title "Independent and identically distributed random variables", :type :wiki-api.core/article} {:id 27955209, :title "Model selection", :type :wiki-api.core/category} {:id 35997105, :title "Generalized linear models", :type :wiki-api.core/category} {:id 2538775, :title "Predictive modelling", :type :wiki-api.core/article} {:id 22691076, :title "Graphical models", :type :wiki-api.core/category} {:id 336897, :title "Function approximation", :type :wiki-api.core/article} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article} {:id 20924581, :title "Latent variable models", :type :wiki-api.core/category}}, {:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category} #{{:id 18462661, :title "Resampling (statistics)", :type :wiki-api.core/category} {:id 172244, :title "Simulated annealing", :type :wiki-api.core/article}}, {:id 336897, :title "Function approximation", :type :wiki-api.core/article} #{}, {:id 21936671, :title "Non-parametric Bayesian methods", :type :wiki-api.core/category} #{{:id 8330403, :title "Dirichlet process", :type :wiki-api.core/article}}, {:id 3424576, :title "Kernel methods", :type :wiki-api.core/article} #{}, {:id 183503, :title "Description logic", :type :wiki-api.core/article} #{}, {:id 1053303, :title "Statistical learning theory", :type :wiki-api.core/article} #{}, {:id 22121360, :title "Fellows of the Association for the Advancement of Artificial Intelligence", :type :wiki-api.core/category} #{{:id 699964, :title "Judea Pearl", :type :wiki-api.core/article}}, {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article} #{}, {:id 20924581, :title "Latent variable models", :type :wiki-api.core/category} #{{:id 871681, :title "Mixture model", :type :wiki-api.core/article} {:id 36425555, :title "Factor analysis", :type :wiki-api.core/category} {:id 4094720, :title "Latent class model", :type :wiki-api.core/article} {:id 689427, :title "Latent semantic analysis", :type :wiki-api.core/article}}, {:id 31024872, :title "Normal distribution", :type :wiki-api.core/category} #{}, {:id 1770656, :title "Numerical linear algebra", :type :wiki-api.core/category} #{{:id 22476294, :title "Matrix decompositions", :type :wiki-api.core/category}}, {:id 21917434, :title "Information Age", :type :wiki-api.core/category} #{{:id 694025, :title "Information theory", :type :wiki-api.core/category}}, {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} #{{:id 17228412, :title "Spatial data analysis", :type :wiki-api.core/category} {:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 18462661, :title "Resampling (statistics)", :type :wiki-api.core/category} {:id 191752, :title "Covariance matrix", :type :wiki-api.core/article} {:id 1817228, :title "Training set", :type :wiki-api.core/article}}, {:id 37832157, :title "Deductive reasoning", :type :wiki-api.core/category} #{{:id 17193265, :title "Statistical inference", :type :wiki-api.core/category}}, {:id 1152426, :title "Philosophy of mathematics", :type :wiki-api.core/category} #{{:id 4890, :title "Bayesian probability", :type :wiki-api.core/article}}, {:id 20189596, :title "Probabilistic complexity theory", :type :wiki-api.core/category} #{{:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category}}, {:id 1817228, :title "Training set", :type :wiki-api.core/article} #{}, {:id 14426697, :title "Market research", :type :wiki-api.core/category} #{{:id 36425555, :title "Factor analysis", :type :wiki-api.core/category}}, {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category} #{{:id 772545, :title "Probability distributions", :type :wiki-api.core/category} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category}}, {:id 18070174, :title "Engineering statistics", :type :wiki-api.core/category} #{{:id 31195693, :title "Design of experiments", :type :wiki-api.core/category}}, {:id 28978911, :title "Conjugate prior distributions", :type :wiki-api.core/category} #{{:id 31024872, :title "Normal distribution", :type :wiki-api.core/category}}, {:id 879637, :title "Joint probability distribution", :type :wiki-api.core/article} #{}, {:id 1792433, :title "Maximum a posteriori estimation", :type :wiki-api.core/article} #{}, {:id 8707155, :title "Statistical shape analysis", :type :wiki-api.core/article} #{}, {:id 1098276, :title "Mathematical terminology", :type :wiki-api.core/category} #{{:id 3175294, :title "Dimension", :type :wiki-api.core/category} {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}, {:id 1055691, :title "Learning", :type :wiki-api.core/category} #{{:id 22991469, :title "Inductive reasoning", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 29421852, :title "M-estimators", :type :wiki-api.core/category} #{{:id 140806, :title "Maximum likelihood", :type :wiki-api.core/article}}, {:id 11504647, :title "Systems of formal logic", :type :wiki-api.core/category} #{{:id 15522913, :title "Predicate logic", :type :wiki-api.core/category}}, {:id 821959, :title "Matrices", :type :wiki-api.core/category} #{{:id 191752, :title "Covariance matrix", :type :wiki-api.core/article}}, {:id 897612, :title "Syntax", :type :wiki-api.core/category} #{}, {:id 14343887, :title "Precision and recall", :type :wiki-api.core/article} #{}, {:id 3224825, :title "Scientific modeling", :type :wiki-api.core/category} #{{:id 22991469, :title "Inductive reasoning", :type :wiki-api.core/category} {:id 16920, :title "Knowledge representation and reasoning", :type :wiki-api.core/article} {:id 17193471, :title "Statistical models", :type :wiki-api.core/category} {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}, {:id 1487877, :title "Sampling techniques", :type :wiki-api.core/category} #{{:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category}}, {:id 29313810, :title "Gaithersburg, Maryland", :type :wiki-api.core/category} #{{:id 6075324, :title "National Institute of Standards and Technology", :type :wiki-api.core/category}}, {:id 9500290, :title "Experiments", :type :wiki-api.core/category} #{{:id 31195693, :title "Design of experiments", :type :wiki-api.core/category}}, {:id 9272793, :title "Computational statistics", :type :wiki-api.core/category} #{{:id 6536797, :title "Statistical algorithms", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 427282, :title "Mutual information", :type :wiki-api.core/article} #{}, {:id 1228459, :title "Business intelligence", :type :wiki-api.core/category} #{{:id 2538775, :title "Predictive modelling", :type :wiki-api.core/article}}, {:id 20941685, :title "Tasks of Natural language processing", :type :wiki-api.core/category} #{{:id 1019150, :title "Machine translation", :type :wiki-api.core/category}}, {:id 951835, :title "Computer data", :type :wiki-api.core/category} #{{:id 4842680, :title "Data security", :type :wiki-api.core/category} {:id 2276471, :title "Databases", :type :wiki-api.core/category} {:id 8495, :title "Data set", :type :wiki-api.core/article}}, {:id 472877, :title "Prior probability", :type :wiki-api.core/article} #{}, {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category} #{{:id 1331441, :title "Document classification", :type :wiki-api.core/article} {:id 36312376, :title "Belief revision", :type :wiki-api.core/category} {:id 23890667, :title "Knowledge representation languages", :type :wiki-api.core/category} {:id 8050180, :title "Multi-agent systems", :type :wiki-api.core/category} {:id 16920, :title "Knowledge representation and reasoning", :type :wiki-api.core/article} {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}, {:id 689427, :title "Latent semantic analysis", :type :wiki-api.core/article} #{}, {:id 958609, :title "Statistical mechanics", :type :wiki-api.core/category} #{{:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category}}, {:id 32611713, :title "Emerging technologies", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}}, {:id 4183228, :title "Israeli philosophers", :type :wiki-api.core/category} #{{:id 699964, :title "Judea Pearl", :type :wiki-api.core/article}}, {:id 15325165, :title "Data clustering algorithms", :type :wiki-api.core/category} #{{:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article}}, {:id 25174161, :title "Open problems", :type :wiki-api.core/category} #{{:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}}}}, :topic-docs #graphs.core.Digraph{:nodes #{1185824 {:id 598776, :title "Score (statistics)", :type :wiki-api.core/article} {:id 536080, :title "Student's t-test", :type :wiki-api.core/article} 574400 2866177 687201 {:id 1188828, :title "Logic programming", :type :wiki-api.core/category} 1009537 3074945 {:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} {:id 8190902, :title "Anomaly detection", :type :wiki-api.core/article} 1009218 {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} 1280035 473123 336067 {:id 6890644, :title "Singular value decomposition", :type :wiki-api.core/category} {:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 1323985, :title "Markov random field", :type :wiki-api.core/article} {:id 2883137, :title "Parametric model", :type :wiki-api.core/article} {:id 871681, :title "Mixture model", :type :wiki-api.core/article} {:id 36425555, :title "Factor analysis", :type :wiki-api.core/category} {:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} 675364 138052 3027908 {:id 1179950, :title "Feature selection", :type :wiki-api.core/article} {:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} {:id 24104134, :title "Conditional probability", :type :wiki-api.core/article} {:id 22187885, :title "List of machine learning algorithms", :type :wiki-api.core/article} {:id 18462661, :title "Resampling (statistics)", :type :wiki-api.core/category} {:id 405944, :title "Time complexity", :type :wiki-api.core/article} 388102 {:id 598971, :title "Fisher information", :type :wiki-api.core/article} 303622 {:id 1331441, :title "Document classification", :type :wiki-api.core/article} 3258022 {:id 30319607, :title "Human–computer interaction", :type :wiki-api.core/category} 473095 1033319 746727 {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} 1323591 {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} 3382280 {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category} 893160 2927016 {:id 31195693, :title "Design of experiments", :type :wiki-api.core/category} {:id 7699923, :title "Speech processing", :type :wiki-api.core/category} {:id 191752, :title "Covariance matrix", :type :wiki-api.core/article} {:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category} 2881512 {:id 45035, :title "Likelihood-ratio test", :type :wiki-api.core/article} 473321 91401 {:id 76340, :title "Principal component analysis", :type :wiki-api.core/article} {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 17193265, :title "Statistical inference", :type :wiki-api.core/category} {:id 3190431, :title "Spatial analysis", :type :wiki-api.core/article} 472937 {:id 31813917, :title "Hidden Markov models", :type :wiki-api.core/category} 3786794 2927018 950858 {:id 226631, :title "Logistic regression", :type :wiki-api.core/article} 3258026 1288331 {:id 172244, :title "Simulated annealing", :type :wiki-api.core/article} {:id 699964, :title "Judea Pearl", :type :wiki-api.core/article} {:id 2276471, :title "Databases", :type :wiki-api.core/category} {:id 318439, :title "Text mining", :type :wiki-api.core/article} 797516 3074924 {:id 20926, :title "Supervised learning", :type :wiki-api.core/article} 1185901 1028237 {:id 8495, :title "Data set", :type :wiki-api.core/article} {:id 954323, :title "Tikhonov regularization", :type :wiki-api.core/article} 20109 {:id 22589574, :title "Instance-based learning", :type :wiki-api.core/article} {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} {:id 30876902, :title "Independent and identically distributed random variables", :type :wiki-api.core/article} {:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article} 541902 1010126 1009230 {:id 331680, :title "Edge detection", :type :wiki-api.core/article} 473199 {:id 504458, :title "Conditional probability distribution", :type :wiki-api.core/article} 472783 {:id 1911810, :title "Language model", :type :wiki-api.core/article} {:id 1028771, :title "Control theory", :type :wiki-api.core/category} 952368 2775184 {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} 1319504 {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} 190192 1279856 498833 {:id 140841, :title "Sufficient statistic", :type :wiki-api.core/article} {:id 17594154, :title "Decision trees", :type :wiki-api.core/category} {:id 313942, :title "Local search (optimization)", :type :wiki-api.core/article} {:id 17917391, :title "Missing completely at random", :type :wiki-api.core/article} {:id 27955209, :title "Model selection", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 22958, :title "Sample space", :type :wiki-api.core/article} {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category} 2784531 1009299 {:id 797088, :title "Computer vision", :type :wiki-api.core/category} 952340 985236 {:id 160986, :title "Order statistic", :type :wiki-api.core/article} {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article} {:id 380008, :title "Probably approximately correct learning", :type :wiki-api.core/article} 1017236 3623508 {:id 1019150, :title "Machine translation", :type :wiki-api.core/category} 1009524 {:id 15690807, :title "Count data", :type :wiki-api.core/article} 192565 {:id 9588430, :title "Higher-order statistics", :type :wiki-api.core/article} {:id 960021, :title "Natural language processing", :type :wiki-api.core/category} 1236661 302837 1032117 574389 {:id 1775388, :title "K-nearest neighbor algorithm", :type :wiki-api.core/article} {:id 160995, :title "Statistical significance", :type :wiki-api.core/article} {:id 8330403, :title "Dirichlet process", :type :wiki-api.core/article} {:id 694025, :title "Information theory", :type :wiki-api.core/category} {:id 434897, :title "Hough transform", :type :wiki-api.core/article} 57463 {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category} 746711 {:id 3670357, :title "Markov logic network", :type :wiki-api.core/article} {:id 801135, :title "Conditional independence", :type :wiki-api.core/article} {:id 8050180, :title "Multi-agent systems", :type :wiki-api.core/category} {:id 2538775, :title "Predictive modelling", :type :wiki-api.core/article} {:id 10983, :title "First-order logic", :type :wiki-api.core/article} 388504 {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article} {:id 16920, :title "Knowledge representation and reasoning", :type :wiki-api.core/article} {:id 357672, :title "Posterior probability", :type :wiki-api.core/article} {:id 4890, :title "Bayesian probability", :type :wiki-api.core/article} 644121 {:id 22691076, :title "Graphical models", :type :wiki-api.core/category} {:id 87339, :title "Naive Bayes classifier", :type :wiki-api.core/article} {:id 31176997, :title "Support vector machines", :type :wiki-api.core/category} {:id 1966814, :title "C4.5 algorithm", :type :wiki-api.core/article} {:id 4094720, :title "Latent class model", :type :wiki-api.core/article} {:id 6075324, :title "National Institute of Standards and Technology", :type :wiki-api.core/category} {:id 6759, :title "Context-free grammar", :type :wiki-api.core/article} 1009754 {:id 140806, :title "Maximum likelihood", :type :wiki-api.core/article} {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} {:id 17193471, :title "Statistical models", :type :wiki-api.core/category} 1250938 {:id 336897, :title "Function approximation", :type :wiki-api.core/article} 294586 1026778 3341018 3402586 395451 {:id 3424576, :title "Kernel methods", :type :wiki-api.core/article} {:id 183503, :title "Description logic", :type :wiki-api.core/article} {:id 1053303, :title "Statistical learning theory", :type :wiki-api.core/article} 937563 {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article} {:id 31024872, :title "Normal distribution", :type :wiki-api.core/category} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} 391548 {:id 1817228, :title "Training set", :type :wiki-api.core/article} 271964 {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category} {:id 879637, :title "Joint probability distribution", :type :wiki-api.core/article} {:id 1792433, :title "Maximum a posteriori estimation", :type :wiki-api.core/article} {:id 8707155, :title "Statistical shape analysis", :type :wiki-api.core/article} 1009341 {:id 897612, :title "Syntax", :type :wiki-api.core/category} {:id 14343887, :title "Precision and recall", :type :wiki-api.core/article} 622461 1254526 {:id 427282, :title "Mutual information", :type :wiki-api.core/article} {:id 472877, :title "Prior probability", :type :wiki-api.core/article} 3782751 {:id 689427, :title "Latent semantic analysis", :type :wiki-api.core/article}}, :in-map {1185824 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 598776, :title "Score (statistics)", :type :wiki-api.core/article} #{}, {:id 536080, :title "Student's t-test", :type :wiki-api.core/article} #{}, 574400 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} {:id 2276471, :title "Databases", :type :wiki-api.core/category} {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category}}, 2866177 #{{:id 598776, :title "Score (statistics)", :type :wiki-api.core/article}}, 687201 #{{:id 2276471, :title "Databases", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 1188828, :title "Logic programming", :type :wiki-api.core/category} #{}, 1009537 #{{:id 6890644, :title "Singular value decomposition", :type :wiki-api.core/category} {:id 36425555, :title "Factor analysis", :type :wiki-api.core/category} {:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article} {:id 15690807, :title "Count data", :type :wiki-api.core/article} {:id 4094720, :title "Latent class model", :type :wiki-api.core/article} {:id 689427, :title "Latent semantic analysis", :type :wiki-api.core/article}}, 3074945 #{{:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} #{}, {:id 8190902, :title "Anomaly detection", :type :wiki-api.core/article} #{}, 1009218 #{{:id 20926, :title "Supervised learning", :type :wiki-api.core/article} {:id 160995, :title "Statistical significance", :type :wiki-api.core/article} {:id 694025, :title "Information theory", :type :wiki-api.core/category}}, {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} #{}, 1280035 #{{:id 2883137, :title "Parametric model", :type :wiki-api.core/article} {:id 598971, :title "Fisher information", :type :wiki-api.core/article} {:id 140806, :title "Maximum likelihood", :type :wiki-api.core/article} {:id 31024872, :title "Normal distribution", :type :wiki-api.core/category} {:id 472877, :title "Prior probability", :type :wiki-api.core/article}}, 473123 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 18462661, :title "Resampling (statistics)", :type :wiki-api.core/category} {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article} {:id 1817228, :title "Training set", :type :wiki-api.core/article}}, 336067 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 6890644, :title "Singular value decomposition", :type :wiki-api.core/category} #{}, {:id 5206601, :title "Data mining", :type :wiki-api.core/category} #{}, {:id 1323985, :title "Markov random field", :type :wiki-api.core/article} #{}, {:id 2883137, :title "Parametric model", :type :wiki-api.core/article} #{}, {:id 871681, :title "Mixture model", :type :wiki-api.core/article} #{}, {:id 36425555, :title "Factor analysis", :type :wiki-api.core/category} #{}, {:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} #{}, 675364 #{{:id 1126536, :title "Optimization problem", :type :wiki-api.core/article} {:id 31176997, :title "Support vector machines", :type :wiki-api.core/category}}, 138052 #{{:id 20926, :title "Supervised learning", :type :wiki-api.core/article} {:id 10983, :title "First-order logic", :type :wiki-api.core/article} {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category}}, 3027908 #{{:id 17193471, :title "Statistical models", :type :wiki-api.core/category}}, {:id 1179950, :title "Feature selection", :type :wiki-api.core/article} #{}, {:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} #{}, {:id 24104134, :title "Conditional probability", :type :wiki-api.core/article} #{}, {:id 22187885, :title "List of machine learning algorithms", :type :wiki-api.core/article} #{}, {:id 18462661, :title "Resampling (statistics)", :type :wiki-api.core/category} #{}, {:id 405944, :title "Time complexity", :type :wiki-api.core/article} #{}, 388102 #{{:id 2276471, :title "Databases", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 598971, :title "Fisher information", :type :wiki-api.core/article} #{}, 303622 #{{:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 1331441, :title "Document classification", :type :wiki-api.core/article} #{}, 3258022 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 30319607, :title "Human–computer interaction", :type :wiki-api.core/category} #{}, 473095 #{{:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category} {:id 3190431, :title "Spatial analysis", :type :wiki-api.core/article} {:id 140841, :title "Sufficient statistic", :type :wiki-api.core/article}}, 1033319 #{{:id 954323, :title "Tikhonov regularization", :type :wiki-api.core/article} {:id 27955209, :title "Model selection", :type :wiki-api.core/category} {:id 1053303, :title "Statistical learning theory", :type :wiki-api.core/article}}, 746727 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} #{}, 1323591 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} #{}, 3382280 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category} #{}, 893160 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 1775388, :title "K-nearest neighbor algorithm", :type :wiki-api.core/article} {:id 434897, :title "Hough transform", :type :wiki-api.core/article} {:id 1966814, :title "C4.5 algorithm", :type :wiki-api.core/article}}, 2927016 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 31195693, :title "Design of experiments", :type :wiki-api.core/category} #{}, {:id 7699923, :title "Speech processing", :type :wiki-api.core/category} #{}, {:id 191752, :title "Covariance matrix", :type :wiki-api.core/article} #{}, {:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category} #{}, 2881512 #{{:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article}}, {:id 45035, :title "Likelihood-ratio test", :type :wiki-api.core/article} #{}, 473321 #{{:id 2023695, :title "Pattern matching", :type :wiki-api.core/category}}, 91401 #{{:id 22187885, :title "List of machine learning algorithms", :type :wiki-api.core/article} {:id 30319607, :title "Human–computer interaction", :type :wiki-api.core/category}}, {:id 76340, :title "Principal component analysis", :type :wiki-api.core/article} #{}, {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} #{}, {:id 17193265, :title "Statistical inference", :type :wiki-api.core/category} #{}, {:id 3190431, :title "Spatial analysis", :type :wiki-api.core/article} #{}, 472937 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 31813917, :title "Hidden Markov models", :type :wiki-api.core/category} #{}, 3786794 #{{:id 8190902, :title "Anomaly detection", :type :wiki-api.core/article}}, 2927018 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 950858 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 22958, :title "Sample space", :type :wiki-api.core/article} {:id 336897, :title "Function approximation", :type :wiki-api.core/article}}, {:id 226631, :title "Logistic regression", :type :wiki-api.core/article} #{}, 3258026 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 1288331 #{{:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} {:id 76340, :title "Principal component analysis", :type :wiki-api.core/article} {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category}}, {:id 172244, :title "Simulated annealing", :type :wiki-api.core/article} #{}, {:id 699964, :title "Judea Pearl", :type :wiki-api.core/article} #{}, {:id 2276471, :title "Databases", :type :wiki-api.core/category} #{}, {:id 318439, :title "Text mining", :type :wiki-api.core/article} #{}, 797516 #{{:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 1019150, :title "Machine translation", :type :wiki-api.core/category} {:id 6075324, :title "National Institute of Standards and Technology", :type :wiki-api.core/category} {:id 897612, :title "Syntax", :type :wiki-api.core/category}}, 3074924 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 20926, :title "Supervised learning", :type :wiki-api.core/article} #{}, 1185901 #{{:id 7699923, :title "Speech processing", :type :wiki-api.core/category}}, 1028237 #{{:id 1171513, :title "Neural networks", :type :wiki-api.core/category} {:id 160986, :title "Order statistic", :type :wiki-api.core/article} {:id 9588430, :title "Higher-order statistics", :type :wiki-api.core/article}}, {:id 8495, :title "Data set", :type :wiki-api.core/article} #{}, {:id 954323, :title "Tikhonov regularization", :type :wiki-api.core/article} #{}, 20109 #{{:id 1019150, :title "Machine translation", :type :wiki-api.core/category} {:id 6759, :title "Context-free grammar", :type :wiki-api.core/article}}, {:id 22589574, :title "Instance-based learning", :type :wiki-api.core/article} #{}, {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} #{}, {:id 30876902, :title "Independent and identically distributed random variables", :type :wiki-api.core/article} #{}, {:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article} #{}, 541902 #{{:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category}}, 1010126 #{{:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category}}, 1009230 #{{:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} {:id 1775388, :title "K-nearest neighbor algorithm", :type :wiki-api.core/article}}, {:id 331680, :title "Edge detection", :type :wiki-api.core/article} #{}, 473199 #{{:id 1179950, :title "Feature selection", :type :wiki-api.core/article} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 504458, :title "Conditional probability distribution", :type :wiki-api.core/article} #{}, 472783 #{{:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 172244, :title "Simulated annealing", :type :wiki-api.core/article} {:id 313942, :title "Local search (optimization)", :type :wiki-api.core/article} {:id 801135, :title "Conditional independence", :type :wiki-api.core/article} {:id 357672, :title "Posterior probability", :type :wiki-api.core/article} {:id 4890, :title "Bayesian probability", :type :wiki-api.core/article}}, {:id 1911810, :title "Language model", :type :wiki-api.core/article} #{}, {:id 1028771, :title "Control theory", :type :wiki-api.core/category} #{}, 952368 #{{:id 871681, :title "Mixture model", :type :wiki-api.core/article} {:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article} {:id 960021, :title "Natural language processing", :type :wiki-api.core/category} {:id 1053303, :title "Statistical learning theory", :type :wiki-api.core/article} {:id 1817228, :title "Training set", :type :wiki-api.core/article}}, 2775184 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 405944, :title "Time complexity", :type :wiki-api.core/article}}, {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article} #{}, {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} #{}, 1319504 #{{:id 536080, :title "Student's t-test", :type :wiki-api.core/article} {:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} {:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 30876902, :title "Independent and identically distributed random variables", :type :wiki-api.core/article} {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article}}, {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} #{}, 190192 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 8495, :title "Data set", :type :wiki-api.core/article} {:id 22589574, :title "Instance-based learning", :type :wiki-api.core/article} {:id 87339, :title "Naive Bayes classifier", :type :wiki-api.core/article} {:id 31176997, :title "Support vector machines", :type :wiki-api.core/category} {:id 3424576, :title "Kernel methods", :type :wiki-api.core/article} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 1279856 #{{:id 504458, :title "Conditional probability distribution", :type :wiki-api.core/article} {:id 140806, :title "Maximum likelihood", :type :wiki-api.core/article} {:id 1817228, :title "Training set", :type :wiki-api.core/article}}, 498833 #{{:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} {:id 8707155, :title "Statistical shape analysis", :type :wiki-api.core/article}}, {:id 140841, :title "Sufficient statistic", :type :wiki-api.core/article} #{}, {:id 17594154, :title "Decision trees", :type :wiki-api.core/category} #{}, {:id 313942, :title "Local search (optimization)", :type :wiki-api.core/article} #{}, {:id 17917391, :title "Missing completely at random", :type :wiki-api.core/article} #{}, {:id 27955209, :title "Model selection", :type :wiki-api.core/category} #{}, {:id 706543, :title "Machine learning", :type :wiki-api.core/category} #{}, {:id 22958, :title "Sample space", :type :wiki-api.core/article} #{}, {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} #{}, {:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category} #{}, 2784531 #{{:id 3670357, :title "Markov logic network", :type :wiki-api.core/article}}, 1009299 #{{:id 1911810, :title "Language model", :type :wiki-api.core/article} {:id 17594154, :title "Decision trees", :type :wiki-api.core/category} {:id 14343887, :title "Precision and recall", :type :wiki-api.core/article}}, {:id 797088, :title "Computer vision", :type :wiki-api.core/category} #{}, 952340 #{{:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} {:id 17193265, :title "Statistical inference", :type :wiki-api.core/category} {:id 17917391, :title "Missing completely at random", :type :wiki-api.core/article} {:id 879637, :title "Joint probability distribution", :type :wiki-api.core/article}}, 985236 #{{:id 20926, :title "Supervised learning", :type :wiki-api.core/article} {:id 10983, :title "First-order logic", :type :wiki-api.core/article} {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category}}, {:id 160986, :title "Order statistic", :type :wiki-api.core/article} #{}, {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article} #{}, {:id 380008, :title "Probably approximately correct learning", :type :wiki-api.core/article} #{}, 1017236 #{{:id 3190431, :title "Spatial analysis", :type :wiki-api.core/article} {:id 318439, :title "Text mining", :type :wiki-api.core/article} {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} {:id 8330403, :title "Dirichlet process", :type :wiki-api.core/article} {:id 689427, :title "Latent semantic analysis", :type :wiki-api.core/article}}, 3623508 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 1019150, :title "Machine translation", :type :wiki-api.core/category} #{}, 1009524 #{{:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 172244, :title "Simulated annealing", :type :wiki-api.core/article} {:id 313942, :title "Local search (optimization)", :type :wiki-api.core/article} {:id 801135, :title "Conditional independence", :type :wiki-api.core/article} {:id 357672, :title "Posterior probability", :type :wiki-api.core/article} {:id 4890, :title "Bayesian probability", :type :wiki-api.core/article}}, {:id 15690807, :title "Count data", :type :wiki-api.core/article} #{}, 192565 #{{:id 380008, :title "Probably approximately correct learning", :type :wiki-api.core/article}}, {:id 9588430, :title "Higher-order statistics", :type :wiki-api.core/article} #{}, {:id 960021, :title "Natural language processing", :type :wiki-api.core/category} #{}, 1236661 #{{:id 1188828, :title "Logic programming", :type :wiki-api.core/category} {:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category} {:id 14482748, :title "Data analysis", :type :wiki-api.core/category}}, 302837 #{{:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 226631, :title "Logistic regression", :type :wiki-api.core/article} {:id 27955209, :title "Model selection", :type :wiki-api.core/category}}, 1032117 #{{:id 24104134, :title "Conditional probability", :type :wiki-api.core/article} {:id 31195693, :title "Design of experiments", :type :wiki-api.core/category} {:id 45035, :title "Likelihood-ratio test", :type :wiki-api.core/article} {:id 8495, :title "Data set", :type :wiki-api.core/article} {:id 331680, :title "Edge detection", :type :wiki-api.core/article} {:id 504458, :title "Conditional probability distribution", :type :wiki-api.core/article} {:id 879637, :title "Joint probability distribution", :type :wiki-api.core/article}}, 574389 #{{:id 10983, :title "First-order logic", :type :wiki-api.core/article} {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 1775388, :title "K-nearest neighbor algorithm", :type :wiki-api.core/article} #{}, {:id 160995, :title "Statistical significance", :type :wiki-api.core/article} #{}, {:id 8330403, :title "Dirichlet process", :type :wiki-api.core/article} #{}, {:id 694025, :title "Information theory", :type :wiki-api.core/category} #{}, {:id 434897, :title "Hough transform", :type :wiki-api.core/article} #{}, 57463 #{{:id 8050180, :title "Multi-agent systems", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category} #{}, 746711 #{{:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} {:id 31813917, :title "Hidden Markov models", :type :wiki-api.core/category} {:id 699964, :title "Judea Pearl", :type :wiki-api.core/article} {:id 2538775, :title "Predictive modelling", :type :wiki-api.core/article} {:id 31176997, :title "Support vector machines", :type :wiki-api.core/category} {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 3670357, :title "Markov logic network", :type :wiki-api.core/article} #{}, {:id 801135, :title "Conditional independence", :type :wiki-api.core/article} #{}, {:id 8050180, :title "Multi-agent systems", :type :wiki-api.core/category} #{}, {:id 2538775, :title "Predictive modelling", :type :wiki-api.core/article} #{}, {:id 10983, :title "First-order logic", :type :wiki-api.core/article} #{}, 388504 #{{:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} {:id 2276471, :title "Databases", :type :wiki-api.core/category} {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category}}, {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article} #{}, {:id 16920, :title "Knowledge representation and reasoning", :type :wiki-api.core/article} #{}, {:id 357672, :title "Posterior probability", :type :wiki-api.core/article} #{}, {:id 4890, :title "Bayesian probability", :type :wiki-api.core/article} #{}, 644121 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}, {:id 22691076, :title "Graphical models", :type :wiki-api.core/category} #{}, {:id 87339, :title "Naive Bayes classifier", :type :wiki-api.core/article} #{}, {:id 31176997, :title "Support vector machines", :type :wiki-api.core/category} #{}, {:id 1966814, :title "C4.5 algorithm", :type :wiki-api.core/article} #{}, {:id 4094720, :title "Latent class model", :type :wiki-api.core/article} #{}, {:id 6075324, :title "National Institute of Standards and Technology", :type :wiki-api.core/category} #{}, {:id 6759, :title "Context-free grammar", :type :wiki-api.core/article} #{}, 1009754 #{{:id 17193471, :title "Statistical models", :type :wiki-api.core/category}}, {:id 140806, :title "Maximum likelihood", :type :wiki-api.core/article} #{}, {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} #{}, {:id 17193471, :title "Statistical models", :type :wiki-api.core/category} #{}, 1250938 #{{:id 1323985, :title "Markov random field", :type :wiki-api.core/article} {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 313942, :title "Local search (optimization)", :type :wiki-api.core/article} {:id 10983, :title "First-order logic", :type :wiki-api.core/article} {:id 1792433, :title "Maximum a posteriori estimation", :type :wiki-api.core/article}}, {:id 336897, :title "Function approximation", :type :wiki-api.core/article} #{}, 294586 #{{:id 797088, :title "Computer vision", :type :wiki-api.core/category}}, 1026778 #{{:id 191752, :title "Covariance matrix", :type :wiki-api.core/article} {:id 1028771, :title "Control theory", :type :wiki-api.core/category} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 427282, :title "Mutual information", :type :wiki-api.core/article}}, 3341018 #{{:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 3402586 #{{:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category}}, 395451 #{{:id 10983, :title "First-order logic", :type :wiki-api.core/article}}, {:id 3424576, :title "Kernel methods", :type :wiki-api.core/article} #{}, {:id 183503, :title "Description logic", :type :wiki-api.core/article} #{}, {:id 1053303, :title "Statistical learning theory", :type :wiki-api.core/article} #{}, 937563 #{{:id 1331441, :title "Document classification", :type :wiki-api.core/article} {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} {:id 87339, :title "Naive Bayes classifier", :type :wiki-api.core/article}}, {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article} #{}, {:id 31024872, :title "Normal distribution", :type :wiki-api.core/category} #{}, {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} #{}, 391548 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 1817228, :title "Training set", :type :wiki-api.core/article} #{}, 271964 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category} #{}, {:id 879637, :title "Joint probability distribution", :type :wiki-api.core/article} #{}, {:id 1792433, :title "Maximum a posteriori estimation", :type :wiki-api.core/article} #{}, {:id 8707155, :title "Statistical shape analysis", :type :wiki-api.core/article} #{}, 1009341 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 897612, :title "Syntax", :type :wiki-api.core/category} #{}, {:id 14343887, :title "Precision and recall", :type :wiki-api.core/article} #{}, 622461 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 22958, :title "Sample space", :type :wiki-api.core/article} {:id 336897, :title "Function approximation", :type :wiki-api.core/article}}, 1254526 #{{:id 17193265, :title "Statistical inference", :type :wiki-api.core/category} {:id 16920, :title "Knowledge representation and reasoning", :type :wiki-api.core/article} {:id 22691076, :title "Graphical models", :type :wiki-api.core/category} {:id 183503, :title "Description logic", :type :wiki-api.core/article}}, {:id 427282, :title "Mutual information", :type :wiki-api.core/article} #{}, {:id 472877, :title "Prior probability", :type :wiki-api.core/article} #{}, 3782751 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 689427, :title "Latent semantic analysis", :type :wiki-api.core/article} #{}}, :out-map {1185824 #{}, {:id 598776, :title "Score (statistics)", :type :wiki-api.core/article} #{2866177}, {:id 536080, :title "Student's t-test", :type :wiki-api.core/article} #{1319504}, 574400 #{}, 2866177 #{}, 687201 #{}, {:id 1188828, :title "Logic programming", :type :wiki-api.core/category} #{1236661}, 1009537 #{}, 3074945 #{}, {:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article} #{1319504}, {:id 8190902, :title "Anomaly detection", :type :wiki-api.core/article} #{3786794}, 1009218 #{}, {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category} #{472783 1009524 746711 388504}, 1280035 #{}, 473123 #{}, 336067 #{}, {:id 6890644, :title "Singular value decomposition", :type :wiki-api.core/category} #{1009537}, {:id 5206601, :title "Data mining", :type :wiki-api.core/category} #{574400 473123 2775184 1319504 190192 1009341}, {:id 1323985, :title "Markov random field", :type :wiki-api.core/article} #{1250938}, {:id 2883137, :title "Parametric model", :type :wiki-api.core/article} #{1280035}, {:id 871681, :title "Mixture model", :type :wiki-api.core/article} #{952368}, {:id 36425555, :title "Factor analysis", :type :wiki-api.core/category} #{1009537}, {:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category} #{952340}, 675364 #{}, 138052 #{}, 3027908 #{}, {:id 1179950, :title "Feature selection", :type :wiki-api.core/article} #{473199}, {:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article} #{498833}, {:id 24104134, :title "Conditional probability", :type :wiki-api.core/article} #{1032117}, {:id 22187885, :title "List of machine learning algorithms", :type :wiki-api.core/article} #{91401}, {:id 18462661, :title "Resampling (statistics)", :type :wiki-api.core/category} #{473123}, {:id 405944, :title "Time complexity", :type :wiki-api.core/article} #{2775184}, 388102 #{}, {:id 598971, :title "Fisher information", :type :wiki-api.core/article} #{1280035}, 303622 #{}, {:id 1331441, :title "Document classification", :type :wiki-api.core/article} #{937563}, 3258022 #{}, {:id 30319607, :title "Human–computer interaction", :type :wiki-api.core/category} #{91401}, 473095 #{}, 1033319 #{}, 746727 #{}, {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category} #{1288331}, 1323591 #{}, {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article} #{574400}, 3382280 #{}, {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category} #{473321}, 893160 #{}, 2927016 #{}, {:id 31195693, :title "Design of experiments", :type :wiki-api.core/category} #{1032117}, {:id 7699923, :title "Speech processing", :type :wiki-api.core/category} #{1185901}, {:id 191752, :title "Covariance matrix", :type :wiki-api.core/article} #{1026778}, {:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category} #{473095}, 2881512 #{}, {:id 45035, :title "Likelihood-ratio test", :type :wiki-api.core/article} #{1032117}, 473321 #{}, 91401 #{}, {:id 76340, :title "Principal component analysis", :type :wiki-api.core/article} #{1288331}, {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} #{797516 472783 1009524 302837 1250938}, {:id 17193265, :title "Statistical inference", :type :wiki-api.core/category} #{952340 1254526}, {:id 3190431, :title "Spatial analysis", :type :wiki-api.core/article} #{473095 1017236}, 472937 #{}, {:id 31813917, :title "Hidden Markov models", :type :wiki-api.core/category} #{746711}, 3786794 #{}, 2927018 #{}, 950858 #{}, {:id 226631, :title "Logistic regression", :type :wiki-api.core/article} #{302837}, 3258026 #{}, 1288331 #{}, {:id 172244, :title "Simulated annealing", :type :wiki-api.core/article} #{472783 1009524}, {:id 699964, :title "Judea Pearl", :type :wiki-api.core/article} #{746711}, {:id 2276471, :title "Databases", :type :wiki-api.core/category} #{574400 687201 388102 388504}, {:id 318439, :title "Text mining", :type :wiki-api.core/article} #{1017236}, 797516 #{}, 3074924 #{}, {:id 20926, :title "Supervised learning", :type :wiki-api.core/article} #{1009218 138052 985236}, 1185901 #{}, 1028237 #{}, {:id 8495, :title "Data set", :type :wiki-api.core/article} #{190192 1032117}, {:id 954323, :title "Tikhonov regularization", :type :wiki-api.core/article} #{1033319}, 20109 #{}, {:id 22589574, :title "Instance-based learning", :type :wiki-api.core/article} #{190192}, {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category} #{1288331}, {:id 30876902, :title "Independent and identically distributed random variables", :type :wiki-api.core/article} #{1319504}, {:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article} #{1009537 303622 952368}, 541902 #{}, 1010126 #{}, 1009230 #{}, {:id 331680, :title "Edge detection", :type :wiki-api.core/article} #{1032117}, 473199 #{}, {:id 504458, :title "Conditional probability distribution", :type :wiki-api.core/article} #{1279856 1032117}, 472783 #{}, {:id 1911810, :title "Language model", :type :wiki-api.core/article} #{1009299}, {:id 1028771, :title "Control theory", :type :wiki-api.core/category} #{1026778}, 952368 #{}, 2775184 #{}, {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article} #{675364}, {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article} #{1026778 937563}, 1319504 #{}, {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category} #{1009230 1017236}, 190192 #{}, 1279856 #{}, 498833 #{}, {:id 140841, :title "Sufficient statistic", :type :wiki-api.core/article} #{473095}, {:id 17594154, :title "Decision trees", :type :wiki-api.core/category} #{1009299}, {:id 313942, :title "Local search (optimization)", :type :wiki-api.core/article} #{472783 1009524 1250938}, {:id 17917391, :title "Missing completely at random", :type :wiki-api.core/article} #{952340}, {:id 27955209, :title "Model selection", :type :wiki-api.core/category} #{1033319 302837}, {:id 706543, :title "Machine learning", :type :wiki-api.core/category} #{1185824 1323591 3382280 472937 950858 644121 271964 622461 3782751}, {:id 22958, :title "Sample space", :type :wiki-api.core/article} #{950858 622461}, {:id 1171513, :title "Neural networks", :type :wiki-api.core/category} #{893160 1028237}, {:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category} #{1236661}, 2784531 #{}, 1009299 #{}, {:id 797088, :title "Computer vision", :type :wiki-api.core/category} #{294586}, 952340 #{}, 985236 #{}, {:id 160986, :title "Order statistic", :type :wiki-api.core/article} #{1028237}, {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article} #{2881512}, {:id 380008, :title "Probably approximately correct learning", :type :wiki-api.core/article} #{192565}, 1017236 #{}, 3623508 #{}, {:id 1019150, :title "Machine translation", :type :wiki-api.core/category} #{797516 20109}, 1009524 #{}, {:id 15690807, :title "Count data", :type :wiki-api.core/article} #{1009537}, 192565 #{}, {:id 9588430, :title "Higher-order statistics", :type :wiki-api.core/article} #{1028237}, {:id 960021, :title "Natural language processing", :type :wiki-api.core/category} #{952368}, 1236661 #{}, 302837 #{}, 1032117 #{}, 574389 #{}, {:id 1775388, :title "K-nearest neighbor algorithm", :type :wiki-api.core/article} #{893160 1009230}, {:id 160995, :title "Statistical significance", :type :wiki-api.core/article} #{1009218}, {:id 8330403, :title "Dirichlet process", :type :wiki-api.core/article} #{1017236}, {:id 694025, :title "Information theory", :type :wiki-api.core/category} #{1009218}, {:id 434897, :title "Hough transform", :type :wiki-api.core/article} #{893160}, 57463 #{}, {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category} #{388504}, 746711 #{}, {:id 3670357, :title "Markov logic network", :type :wiki-api.core/article} #{2784531}, {:id 801135, :title "Conditional independence", :type :wiki-api.core/article} #{472783 1009524}, {:id 8050180, :title "Multi-agent systems", :type :wiki-api.core/category} #{57463}, {:id 2538775, :title "Predictive modelling", :type :wiki-api.core/article} #{746711}, {:id 10983, :title "First-order logic", :type :wiki-api.core/article} #{138052 985236 574389 1250938 395451}, 388504 #{}, {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article} #{473123 1319504}, {:id 16920, :title "Knowledge representation and reasoning", :type :wiki-api.core/article} #{1254526}, {:id 357672, :title "Posterior probability", :type :wiki-api.core/article} #{472783 1009524}, {:id 4890, :title "Bayesian probability", :type :wiki-api.core/article} #{472783 1009524}, 644121 #{}, {:id 22691076, :title "Graphical models", :type :wiki-api.core/category} #{1254526}, {:id 87339, :title "Naive Bayes classifier", :type :wiki-api.core/article} #{190192 937563}, {:id 31176997, :title "Support vector machines", :type :wiki-api.core/category} #{675364 190192 746711}, {:id 1966814, :title "C4.5 algorithm", :type :wiki-api.core/article} #{893160}, {:id 4094720, :title "Latent class model", :type :wiki-api.core/article} #{1009537}, {:id 6075324, :title "National Institute of Standards and Technology", :type :wiki-api.core/category} #{797516}, {:id 6759, :title "Context-free grammar", :type :wiki-api.core/article} #{20109}, 1009754 #{}, {:id 140806, :title "Maximum likelihood", :type :wiki-api.core/article} #{1280035 1279856}, {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} #{574400 3074945 138052 541902 1010126 985236 574389 746711 3341018 3402586}, {:id 17193471, :title "Statistical models", :type :wiki-api.core/category} #{3027908 1009754}, 1250938 #{}, {:id 336897, :title "Function approximation", :type :wiki-api.core/article} #{950858 622461}, 294586 #{}, 1026778 #{}, 3341018 #{}, 3402586 #{}, 395451 #{}, {:id 3424576, :title "Kernel methods", :type :wiki-api.core/article} #{190192}, {:id 183503, :title "Description logic", :type :wiki-api.core/article} #{1254526}, {:id 1053303, :title "Statistical learning theory", :type :wiki-api.core/article} #{1033319 952368}, 937563 #{}, {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article} #{687201 3074945 336067 388102 303622 3258022 746727 2927016 2927018 3258026 3074924 473199 190192 3623508 574389 57463 746711 3341018 391548 1009341}, {:id 31024872, :title "Normal distribution", :type :wiki-api.core/category} #{1280035}, {:id 14482748, :title "Data analysis", :type :wiki-api.core/category} #{1236661}, 391548 #{}, {:id 1817228, :title "Training set", :type :wiki-api.core/article} #{473123 952368 1279856}, 271964 #{}, {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category} #{644121}, {:id 879637, :title "Joint probability distribution", :type :wiki-api.core/article} #{952340 1032117}, {:id 1792433, :title "Maximum a posteriori estimation", :type :wiki-api.core/article} #{1250938}, {:id 8707155, :title "Statistical shape analysis", :type :wiki-api.core/article} #{498833}, 1009341 #{}, {:id 897612, :title "Syntax", :type :wiki-api.core/category} #{797516}, {:id 14343887, :title "Precision and recall", :type :wiki-api.core/article} #{1009299}, 622461 #{}, 1254526 #{}, {:id 427282, :title "Mutual information", :type :wiki-api.core/article} #{1026778}, {:id 472877, :title "Prior probability", :type :wiki-api.core/article} #{1280035}, 3782751 #{}, {:id 689427, :title "Latent semantic analysis", :type :wiki-api.core/article} #{1009537 1017236}}}, :doc-map {1185824 #search_api.search_api.Paper{:id 1185824, :key "journals/jocn/AblaKO08", :title "On-line Assessment of Statistical Learning by Event-related Potentials.", :abstract "We investigated the neural processes involved in on-line statistical learning and word segmentation. Auditory event-related potentials (ERPs) were recorded while participants were exposed to continuous, nonlinguistic auditory sequences, the elements of which were organized into “tritone words” that were sequenced in random order, with no silent spaces between them. After listening to three 6.6-min sessions of sequences, the participants performed a behavioral choice test, in which they were instructed to indicate the most familiar tone sequence in each test trial by pressing buttons. The participants were divided into three groups (high, middle, and low learners) based on their behavioral performance. The overall mean performance was 74.4%, indicating that the tone sequence was segmented and that the participants learned the tone words statistically. Grand-averaged ERPs showed that word onset (initial tone) elicited the largest N100 and N400 in the early learning session of high learners, but in middle learners, the word-onset effect was elicited in a later session, and there was no effect in low learners. The N400 amplitudes significantly differed between the three learning sessions in the high-and middle-learner groups. The results suggest that the N400 effect indicates not only on-line word segmentation but also the degree of statistical learning. This study provides insight into the neural mechanisms underlying on-line statistical learning processes.", :author (#search_api.search_api.Author{:id 85341, :first-name nil, :last-name nil, :full-name "Dilshat Abla"} #search_api.search_api.Author{:id 167451, :first-name nil, :last-name nil, :full-name "Kentaro Katahira"} #search_api.search_api.Author{:id 883499, :first-name nil, :last-name nil, :full-name "Kazuo Okanoya"}), :year 2008, :venue "J. Cognitive Neuroscience", :ncit 0, :string "On-line Assessment of Statistical Learning by Event-related Potentials.. We investigated the neural processes involved in on-line statistical learning and word segmentation. Auditory event-related potentials (ERPs) were recorded while participants were exposed to continuous, nonlinguistic auditory sequences, the elements of which were organized into “tritone words” that were sequenced in random order, with no silent spaces between them. After listening to three 6.6-min sessions of sequences, the participants performed a behavioral choice test, in which they were instructed to indicate the most familiar tone sequence in each test trial by pressing buttons. The participants were divided into three groups (high, middle, and low learners) based on their behavioral performance. The overall mean performance was 74.4%, indicating that the tone sequence was segmented and that the participants learned the tone words statistically. Grand-averaged ERPs showed that word onset (initial tone) elicited the largest N100 and N400 in the early learning session of high learners, but in middle learners, the word-onset effect was elicited in a later session, and there was no effect in low learners. The N400 amplitudes significantly differed between the three learning sessions in the high-and middle-learner groups. The results suggest that the N400 effect indicates not only on-line word segmentation but also the degree of statistical learning. This study provides insight into the neural mechanisms underlying on-line statistical learning processes.", :doc-id "On-line Assessment of Statistical Learning by Event-related Potentials. 2008 Dilshat Abla, Kentaro Katahira, Kazuo Okanoya"}, 574400 #search_api.search_api.Paper{:id 574400, :key "conf/sbia/Raedt08", :title "Logical and Relational Learning.", :abstract "I use the term logical and relational learning (LRL) to refer to the subfield of machine learning and data mining that is concerned with learning in expressive logical or relational representations. It is the union of inductive logic programming, (statistical) relational learning and multi-relational data mining and constitutes a general class of techniques and methodology for learning from structured data (such as graphs, networks, relational databases) and background knowledge. During the course of its existence, logical and relational learning has changed dramatically. Whereas early work was mainly concerned with logical issues (and even program synthesis from examples), in the 90s its focus was on the discovery of new and interpretable knowledge from structured data, often in the form of rules or patterns. Since then the range of tasks to which logical and relational learning has been applied has significantly broadened and now covers almost all machine learning problems and settings. Today, there exist logical and relational learning methods for reinforcement learning, statistical learning, distance- and kernel-based learning in addition to traditional symbolic machine learning approaches. At the same time, logical and relational learning problems are appearing everywhere. Advances in intelligent systems are enabling the generation of high-level symbolic and structured data in a wide variety of domains, including the semantic web, robotics, vision, social networks, and the life sciences, which in turn raises new challenges and opportunities for logical and relational learning. These developments have led to a new view on logical and relational learning and its role in machine learning and artificial intelligence. In this talk, I shall reflect on this view by identifying some of the lessons learned in logical and relational learning and formulating some challenges for future developments.", :author (#search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2008, :venue "SBIA", :ncit 173, :string "Logical and Relational Learning.. I use the term logical and relational learning (LRL) to refer to the subfield of machine learning and data mining that is concerned with learning in expressive logical or relational representations. It is the union of inductive logic programming, (statistical) relational learning and multi-relational data mining and constitutes a general class of techniques and methodology for learning from structured data (such as graphs, networks, relational databases) and background knowledge. During the course of its existence, logical and relational learning has changed dramatically. Whereas early work was mainly concerned with logical issues (and even program synthesis from examples), in the 90s its focus was on the discovery of new and interpretable knowledge from structured data, often in the form of rules or patterns. Since then the range of tasks to which logical and relational learning has been applied has significantly broadened and now covers almost all machine learning problems and settings. Today, there exist logical and relational learning methods for reinforcement learning, statistical learning, distance- and kernel-based learning in addition to traditional symbolic machine learning approaches. At the same time, logical and relational learning problems are appearing everywhere. Advances in intelligent systems are enabling the generation of high-level symbolic and structured data in a wide variety of domains, including the semantic web, robotics, vision, social networks, and the life sciences, which in turn raises new challenges and opportunities for logical and relational learning. These developments have led to a new view on logical and relational learning and its role in machine learning and artificial intelligence. In this talk, I shall reflect on this view by identifying some of the lessons learned in logical and relational learning and formulating some challenges for future developments.", :doc-id "Logical and Relational Learning. 2008 Luc De Raedt"}, 2866177 #search_api.search_api.Paper{:id 2866177, :key "journals/ml/LandwehrPRF10", :title "Fast learning of relational kernels.", :abstract "We develop a general theoretical framework for statistical logical learning with kernels based on dynamic propositionalization, where structure learning corresponds to inferring a suitable kernel on logical objects, and parameter learning corresponds to function learning in the resulting reproducing kernel Hilbert space. In particular, we study the case where structure learning is performed by a simple FOIL-like algorithm, and propose alternative scoring functions for guiding the search process. We present an empirical evaluation on several data sets in the single-task as well as in the multi-task setting.", :author (#search_api.search_api.Author{:id 127590, :first-name nil, :last-name nil, :full-name "Niels Landwehr"} #search_api.search_api.Author{:id 1520074, :first-name nil, :last-name nil, :full-name "Andrea Passerini"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"} #search_api.search_api.Author{:id 34394, :first-name nil, :last-name nil, :full-name "Paolo Frasconi"}), :year 2010, :venue "Machine Learning", :ncit 12, :string "Fast learning of relational kernels.. We develop a general theoretical framework for statistical logical learning with kernels based on dynamic propositionalization, where structure learning corresponds to inferring a suitable kernel on logical objects, and parameter learning corresponds to function learning in the resulting reproducing kernel Hilbert space. In particular, we study the case where structure learning is performed by a simple FOIL-like algorithm, and propose alternative scoring functions for guiding the search process. We present an empirical evaluation on several data sets in the single-task as well as in the multi-task setting.", :doc-id "Fast learning of relational kernels. 2010 Niels Landwehr, Andrea Passerini, Luc De Raedt, Paolo Frasconi"}, 687201 #search_api.search_api.Paper{:id 687201, :key "conf/grc/Chen07", :title "Research on Statistical Relational Learning and Rough Set in SRL.", :abstract "Statistical relational learning constructs statistical mod- els from relational databases, combining the powers of re- lational learning and statistical learning. Its strong abil- ity and special property make statistical relational learn- ing become one of the important areas in machine learn- ing. In this paper, the general concepts and characteris- tics of statistical relational learning are presented firstly. Then some major branches of this newly emerging field are discussed, including logic and rule-based approaches, frame and object-oriented approaches, and several other important approaches. After that some methods of apply- ing rough set in statistical relational learning are described, such as gRS-ILP and VPRSILP. Finally applications of sta- tistical relational learning are briefly introduced and some future directions of statistical relational learning and the prospects of rough set in this area are pointed out.", :author (#search_api.search_api.Author{:id 74552, :first-name nil, :last-name nil, :full-name "Fei Chen"}), :year 2007, :venue "GrC", :ncit 0, :string "Research on Statistical Relational Learning and Rough Set in SRL.. Statistical relational learning constructs statistical mod- els from relational databases, combining the powers of re- lational learning and statistical learning. Its strong abil- ity and special property make statistical relational learn- ing become one of the important areas in machine learn- ing. In this paper, the general concepts and characteris- tics of statistical relational learning are presented firstly. Then some major branches of this newly emerging field are discussed, including logic and rule-based approaches, frame and object-oriented approaches, and several other important approaches. After that some methods of apply- ing rough set in statistical relational learning are described, such as gRS-ILP and VPRSILP. Finally applications of sta- tistical relational learning are briefly introduced and some future directions of statistical relational learning and the prospects of rough set in this area are pointed out.", :doc-id "Research on Statistical Relational Learning and Rough Set in SRL. 2007 Fei Chen"}, 1009537 #search_api.search_api.Paper{:id 1009537, :key "journals/ml/Hofmann01", :title "Unsupervised Learning by Probabilistic Latent Semantic Analysis.", :abstract "This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.", :author (#search_api.search_api.Author{:id 8394, :first-name nil, :last-name nil, :full-name "Thomas Hofmann"}), :year 2001, :venue "Machine Learning", :ncit 2463, :string "Unsupervised Learning by Probabilistic Latent Semantic Analysis.. This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.", :doc-id "Unsupervised Learning by Probabilistic Latent Semantic Analysis. 2001 Thomas Hofmann"}, 3074945 #search_api.search_api.Paper{:id 3074945, :key "journals/ml/BlockeelBRDKY11", :title "Guest editorial to the special issue on inductive logic programming, mining and learning in graphs and statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 882341, :first-name nil, :last-name nil, :full-name "Hendrik Blockeel"} #search_api.search_api.Author{:id 272602, :first-name nil, :last-name nil, :full-name "Karsten M. Borgwardt"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"} #search_api.search_api.Author{:id 668085, :first-name nil, :last-name nil, :full-name "Pedro Domingos"} #search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"} #search_api.search_api.Author{:id 324907, :first-name nil, :last-name nil, :full-name "Xifeng Yan"}), :year 2011, :venue "Machine Learning", :ncit 0, :string "Guest editorial to the special issue on inductive logic programming, mining and learning in graphs and statistical relational learning.. ", :doc-id "Guest editorial to the special issue on inductive logic programming, mining and learning in graphs and statistical relational learning. 2011 Hendrik Blockeel, Karsten M. Borgwardt, Luc De Raedt, Pedro Domingos, Kristian Kersting, Xifeng Yan"}, 1009218 #search_api.search_api.Paper{:id 1009218, :key "journals/ml/Boulle04", :title "Khiops: A Statistical Discretization Method of Continuous Attributes.", :abstract "In supervised machine learning, some algorithms are restricted to discrete data and have to discretize continuous attributes. Many discretization methods, based on statistical criteria, information content, or other specialized criteria, have been studied in the past. In this paper, we propose the discretization method Khiops,1 based on the chi-square statistic. In contrast with related methods ChiMerge and ChiSplit, this method optimizes the chi-square criterion in a global manner on the whole discretization domain and does not require any stopping criterion. A theoretical study followed by experiments demonstrates the robustness and the good predictive performance of the method.", :author (#search_api.search_api.Author{:id 1526635, :first-name nil, :last-name nil, :full-name "Marc Boullé"}), :year 2004, :venue "Machine Learning", :ncit 0, :string "Khiops: A Statistical Discretization Method of Continuous Attributes.. In supervised machine learning, some algorithms are restricted to discrete data and have to discretize continuous attributes. Many discretization methods, based on statistical criteria, information content, or other specialized criteria, have been studied in the past. In this paper, we propose the discretization method Khiops,1 based on the chi-square statistic. In contrast with related methods ChiMerge and ChiSplit, this method optimizes the chi-square criterion in a global manner on the whole discretization domain and does not require any stopping criterion. A theoretical study followed by experiments demonstrates the robustness and the good predictive performance of the method.", :doc-id "Khiops: A Statistical Discretization Method of Continuous Attributes. 2004 Marc Boullé"}, 1280035 #search_api.search_api.Paper{:id 1280035, :key "journals/nn/Watanabe10", :title "Equations of states in singular statistical estimation.", :abstract "Learning machines that have hierarchical structures or hidden variables are singular statistical models because they are nonidentifiable and their Fisher information matrices are singular. In singular statistical models, neither does the Bayes a posteriori distribution converge to the normal distribution nor does the maximum likelihood estimator satisfy asymptotic normality. This is the main reason that it has been difficult to predict their generalization performance from trained states. In this paper, we study four errors, (1) the Bayes generalization error, (2) the Bayes training error, (3) the Gibbs generalization error, and (4) the Gibbs training error, and prove that there are universal mathematical relations among these errors. The formulas proved in this paper are equations of states in statistical estimation because they hold for any true distribution, any parametric model, and any a priori distribution. Also we show that the Bayes and Gibbs generalization errors can be estimated by Bayes and Gibbs training errors, and we propose widely applicable information criteria that can be applied to both regular and singular statistical models.", :author (#search_api.search_api.Author{:id 976510, :first-name nil, :last-name nil, :full-name "Sumio Watanabe"}), :year 2010, :venue "Neural Networks", :ncit 13, :string "Equations of states in singular statistical estimation.. Learning machines that have hierarchical structures or hidden variables are singular statistical models because they are nonidentifiable and their Fisher information matrices are singular. In singular statistical models, neither does the Bayes a posteriori distribution converge to the normal distribution nor does the maximum likelihood estimator satisfy asymptotic normality. This is the main reason that it has been difficult to predict their generalization performance from trained states. In this paper, we study four errors, (1) the Bayes generalization error, (2) the Bayes training error, (3) the Gibbs generalization error, and (4) the Gibbs training error, and prove that there are universal mathematical relations among these errors. The formulas proved in this paper are equations of states in statistical estimation because they hold for any true distribution, any parametric model, and any a priori distribution. Also we show that the Bayes and Gibbs generalization errors can be estimated by Bayes and Gibbs training errors, and we propose widely applicable information criteria that can be applied to both regular and singular statistical models.", :doc-id "Equations of states in singular statistical estimation. 2010 Sumio Watanabe"}, 473123 #search_api.search_api.Paper{:id 473123, :key "conf/kdd/NevilleJFH03", :title "Learning relational probability trees.", :abstract "Classification trees are widely used in the machine learning and data mining communities for modeling propositional data. Recent work has extended this basic paradigm to probability estimation trees. Traditional tree learning algorithms assume that instances in the training data are homogenous and independently distributed. Relational probability trees (RPTs) extend standard probability estimation trees to a relational setting in which data instances are heterogeneous and interdependent. Our algorithm for learning the structure and parameters of an RPT searches over a space of relational features that use aggregation functions (e.g. AVERAGE, MODE, COUNT) to dynamically propositionalize relational data and create binary splits within the RPT. Previous work has identified a number of statistical biases due to characteristics of relational data such as autocorrelation and degree disparity. The RPT algorithm uses a novel form of randomization test to adjust for these biases. On a variety of relational learning tasks, RPTs built using randomization tests are significantly smaller than other models and achieve equivalent, or better, performance.", :author (#search_api.search_api.Author{:id 150768, :first-name nil, :last-name nil, :full-name "Jennifer Neville"} #search_api.search_api.Author{:id 408381, :first-name nil, :last-name nil, :full-name "David Jensen"} #search_api.search_api.Author{:id 1434755, :first-name nil, :last-name nil, :full-name "Lisa Friedland"} #search_api.search_api.Author{:id 575750, :first-name nil, :last-name nil, :full-name "Michael Hay"}), :year 2003, :venue "KDD", :ncit 191, :string "Learning relational probability trees.. Classification trees are widely used in the machine learning and data mining communities for modeling propositional data. Recent work has extended this basic paradigm to probability estimation trees. Traditional tree learning algorithms assume that instances in the training data are homogenous and independently distributed. Relational probability trees (RPTs) extend standard probability estimation trees to a relational setting in which data instances are heterogeneous and interdependent. Our algorithm for learning the structure and parameters of an RPT searches over a space of relational features that use aggregation functions (e.g. AVERAGE, MODE, COUNT) to dynamically propositionalize relational data and create binary splits within the RPT. Previous work has identified a number of statistical biases due to characteristics of relational data such as autocorrelation and degree disparity. The RPT algorithm uses a novel form of randomization test to adjust for these biases. On a variety of relational learning tasks, RPTs built using randomization tests are significantly smaller than other models and achieve equivalent, or better, performance.", :doc-id "Learning relational probability trees. 2003 Jennifer Neville, David Jensen, Lisa Friedland, Michael Hay"}, 336067 #search_api.search_api.Paper{:id 336067, :key "conf/icml/KokD07", :title "Statistical predicate invention.", :abstract "We propose statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models and predicate invention in ILP. We propose an initial model for SPI based on second-order Markov logic, in which predicates as well as arguments can be variables, and the domain of discourse is not fully known in advance. Our approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with (e.g., it clusters relations by the clusters of the objects they relate). Since different clusterings are better for predicting different subsets of the atoms, we allow multiple cross-cutting clusterings. We show that this approach outperforms Markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets.", :author (#search_api.search_api.Author{:id 801078, :first-name nil, :last-name nil, :full-name "Stanley Kok"} #search_api.search_api.Author{:id 668085, :first-name nil, :last-name nil, :full-name "Pedro Domingos"}), :year 2007, :venue "ICML", :ncit 59, :string "Statistical predicate invention.. We propose statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models and predicate invention in ILP. We propose an initial model for SPI based on second-order Markov logic, in which predicates as well as arguments can be variables, and the domain of discourse is not fully known in advance. Our approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with (e.g., it clusters relations by the clusters of the objects they relate). Since different clusterings are better for predicting different subsets of the atoms, we allow multiple cross-cutting clusterings. We show that this approach outperforms Markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets.", :doc-id "Statistical predicate invention. 2007 Stanley Kok, Pedro Domingos"}, 675364 #search_api.search_api.Paper{:id 675364, :key "conf/www/QinLZWXL08", :title "Learning to rank relational objects and its application to web search.", :abstract "Learning to rank is a new statistical learning technology on creating a ranking model for sorting objects. The technology has been successfully applied to web search, and is becoming one of the key machineries for building search engines. Existing approaches to learning to rank, however, did not consider the cases in which there exists relationship between the objects to be ranked, despite of the fact that such situations are very common in practice. For example, in web search, given a query certain relationships usually exist among the the retrieved documents, e.g., URL hierarchy, similarity, etc., and sometimes it is necessary to utilize the information in ranking of the documents. This paper addresses the issue and formulates it as a novel learning problem, referred to as, 'learning to rank relational objects'. In the new learning task, the ranking model is defined as a function of not only the contents (features) of objects but also the relations between objects. The paper further focuses on one setting of the learning problem in which the way of using relation information is predetermined. It formalizes the learning task as an optimization problem in the setting. The paper then proposes a new method to perform the optimization task, particularly an implementation based on SVM. Experimental results show that the proposed method outperforms the baseline methods for two ranking tasks (Pseudo Relevance Feedback and Topic Distillation) in web search, indicating that the proposed method can indeed make effective use of relation information and content information in ranking.", :author (#search_api.search_api.Author{:id 17439, :first-name nil, :last-name nil, :full-name "Tao Qin"} #search_api.search_api.Author{:id 1411681, :first-name nil, :last-name nil, :full-name "Tie-Yan Liu"} #search_api.search_api.Author{:id 981048, :first-name nil, :last-name nil, :full-name "Xu-Dong Zhang"} #search_api.search_api.Author{:id 566222, :first-name nil, :last-name nil, :full-name "De-Sheng Wang"} #search_api.search_api.Author{:id 1482892, :first-name nil, :last-name nil, :full-name "Wen-Ying Xiong"} #search_api.search_api.Author{:id 742456, :first-name nil, :last-name nil, :full-name "Hang Li"}), :year 2008, :venue "WWW", :ncit 62, :string "Learning to rank relational objects and its application to web search.. Learning to rank is a new statistical learning technology on creating a ranking model for sorting objects. The technology has been successfully applied to web search, and is becoming one of the key machineries for building search engines. Existing approaches to learning to rank, however, did not consider the cases in which there exists relationship between the objects to be ranked, despite of the fact that such situations are very common in practice. For example, in web search, given a query certain relationships usually exist among the the retrieved documents, e.g., URL hierarchy, similarity, etc., and sometimes it is necessary to utilize the information in ranking of the documents. This paper addresses the issue and formulates it as a novel learning problem, referred to as, 'learning to rank relational objects'. In the new learning task, the ranking model is defined as a function of not only the contents (features) of objects but also the relations between objects. The paper further focuses on one setting of the learning problem in which the way of using relation information is predetermined. It formalizes the learning task as an optimization problem in the setting. The paper then proposes a new method to perform the optimization task, particularly an implementation based on SVM. Experimental results show that the proposed method outperforms the baseline methods for two ranking tasks (Pseudo Relevance Feedback and Topic Distillation) in web search, indicating that the proposed method can indeed make effective use of relation information and content information in ranking.", :doc-id "Learning to rank relational objects and its application to web search. 2008 Tao Qin, Tie-Yan Liu, Xu-Dong Zhang, De-Sheng Wang, Wen-Ying Xiong, Hang Li"}, 138052 #search_api.search_api.Paper{:id 138052, :key "conf/dagstuhl/PasseriniFR05", :title "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.", :abstract "We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :author (#search_api.search_api.Author{:id 1520074, :first-name nil, :last-name nil, :full-name "Andrea Passerini"} #search_api.search_api.Author{:id 34394, :first-name nil, :last-name nil, :full-name "Paolo Frasconi"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2005, :venue "Probabilistic, Logical and Relational Learning", :ncit 31, :string "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.. We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :doc-id "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting. 2005 Andrea Passerini, Paolo Frasconi, Luc De Raedt"}, 3027908 #search_api.search_api.Paper{:id 3027908, :key "conf/sigmod/GetoorM11", :title "Learning statistical models from relational data.", :abstract nil, :author (#search_api.search_api.Author{:id 1449720, :first-name nil, :last-name nil, :full-name "Lise Getoor"} #search_api.search_api.Author{:id 421571, :first-name nil, :last-name nil, :full-name "Lilyana Mihalkova"}), :year 2011, :venue "SIGMOD Conference", :ncit 87, :string "Learning statistical models from relational data.. ", :doc-id "Learning statistical models from relational data. 2011 Lise Getoor, Lilyana Mihalkova"}, 388102 #search_api.search_api.Paper{:id 388102, :key "conf/ijcai/DavisBDPRCS05", :title "View Learning for Statistical Relational Learning: With an Application to Mammography.", :abstract "Statistical relational learning (SRL) constructs probabilistic models from relational databases. A key capability of SRL is the learning of arcs (in the Bayes net sense) connecting entries in different rows of a relational table, or in different tables. Nevertheless, SRL approaches currently are constrained to use the existing database schema. For many database applications, users find it profitable to define alternative \"views\" of the database, in effect defining new fields or tables. Such new fields or tables can also be highly useful in learning. We provide SRL with the capability of learning new views.", :author (#search_api.search_api.Author{:id 747351, :first-name nil, :last-name nil, :full-name "Jesse Davis"} #search_api.search_api.Author{:id 750446, :first-name nil, :last-name nil, :full-name "Elizabeth S. Burnside"} #search_api.search_api.Author{:id 1157247, :first-name nil, :last-name nil, :full-name "Inês de Castro Dutra"} #search_api.search_api.Author{:id 138183, :first-name nil, :last-name nil, :full-name "David Page"} #search_api.search_api.Author{:id 464920, :first-name nil, :last-name nil, :full-name "Raghu Ramakrishnan"} #search_api.search_api.Author{:id 1352436, :first-name nil, :last-name nil, :full-name "Vítor Santos Costa"} #search_api.search_api.Author{:id 7, :first-name nil, :last-name nil, :full-name "Jude W. Shavlik"}), :year 2005, :venue "IJCAI", :ncit 33, :string "View Learning for Statistical Relational Learning: With an Application to Mammography.. Statistical relational learning (SRL) constructs probabilistic models from relational databases. A key capability of SRL is the learning of arcs (in the Bayes net sense) connecting entries in different rows of a relational table, or in different tables. Nevertheless, SRL approaches currently are constrained to use the existing database schema. For many database applications, users find it profitable to define alternative \"views\" of the database, in effect defining new fields or tables. Such new fields or tables can also be highly useful in learning. We provide SRL with the capability of learning new views.", :doc-id "View Learning for Statistical Relational Learning: With an Application to Mammography. 2005 Jesse Davis, Elizabeth S. Burnside, Inês de Castro Dutra, David Page, Raghu Ramakrishnan, Vítor Santos Costa, Jude W. Shavlik"}, 303622 #search_api.search_api.Paper{:id 303622, :key "conf/icdm/XiangN08", :title "Pseudolikelihood EM for Within-network Relational Learning.", :abstract "In this work, we study the problem of \\emph{within-network} relational learning and inference, where models are learned on a partially labeled relational dataset and then are applied to predict the classes of unlabeled instances in the same graph. We categorize recent work in statistical relational learning into three alternative approaches for this setting: disjoint learning with disjoint inference, disjoint learning with collective inference, and collective learning with collective inference. Models from each of these categories has been employed previously in different settings, but to our knowledge there has been no systematic comparison of models from all three categories. In this paper, we develop a novel pseudolikelihood EM method that facilitates more general \\emph{collective learning} and \\emph{collective inference} on partially labeled relational networks. We then compare this method to competing methods from the other categories on both synthetic and real-world data. We show that collective learning and inference with the pseudolikelihood EM approach achieves significantly higher accuracy than the other types of models when there are a moderate number of labeled examples in the data graph.", :author (#search_api.search_api.Author{:id 819343, :first-name nil, :last-name nil, :full-name "Rongjing Xiang"} #search_api.search_api.Author{:id 150768, :first-name nil, :last-name nil, :full-name "Jennifer Neville"}), :year 2008, :venue "ICDM", :ncit 9, :string "Pseudolikelihood EM for Within-network Relational Learning.. In this work, we study the problem of \\emph{within-network} relational learning and inference, where models are learned on a partially labeled relational dataset and then are applied to predict the classes of unlabeled instances in the same graph. We categorize recent work in statistical relational learning into three alternative approaches for this setting: disjoint learning with disjoint inference, disjoint learning with collective inference, and collective learning with collective inference. Models from each of these categories has been employed previously in different settings, but to our knowledge there has been no systematic comparison of models from all three categories. In this paper, we develop a novel pseudolikelihood EM method that facilitates more general \\emph{collective learning} and \\emph{collective inference} on partially labeled relational networks. We then compare this method to competing methods from the other categories on both synthetic and real-world data. We show that collective learning and inference with the pseudolikelihood EM approach achieves significantly higher accuracy than the other types of models when there are a moderate number of labeled examples in the data graph.", :doc-id "Pseudolikelihood EM for Within-network Relational Learning. 2008 Rongjing Xiang, Jennifer Neville"}, 3258022 #search_api.search_api.Paper{:id 3258022, :key "journals/ml/NatarajanKKGS12", :title "Gradient-based boosting for statistical relational learning: The relational dependency network case.", :abstract nil, :author (#search_api.search_api.Author{:id 883462, :first-name nil, :last-name nil, :full-name "Sriraam Natarajan"} #search_api.search_api.Author{:id 905516, :first-name nil, :last-name nil, :full-name "Tushar Khot"} #search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"} #search_api.search_api.Author{:id 1351938, :first-name nil, :last-name nil, :full-name "Bernd Gutmann"} #search_api.search_api.Author{:id 7, :first-name nil, :last-name nil, :full-name "Jude W. Shavlik"}), :year 2012, :venue "Machine Learning", :ncit 20, :string "Gradient-based boosting for statistical relational learning: The relational dependency network case.. ", :doc-id "Gradient-based boosting for statistical relational learning: The relational dependency network case. 2012 Sriraam Natarajan, Tushar Khot, Kristian Kersting, Bernd Gutmann, Jude W. Shavlik"}, 473095 #search_api.search_api.Paper{:id 473095, :key "conf/kdd/Moore06", :title "New cached-sufficient statistics algorithms for quickly answering statistical questions.", :abstract "This talk is about recent work on new ways to exploit preprocessed views of data tables for tractably solving big statistical queries. We'll describe deployments of these new algorithms in the realms of detecting killer asteroids and unnatural disease outbreaks.In recent years, several groups have looked at methods for pre-storing general sufficient statistics of the data in spatial data structures such as kd-trees and ball-trees so that both frequentist and Bayesian statistical operations become fast for large datasets. In this talk we will look at two other classes of optimization required in important statistical queries.The first involves iterating over all spatial regions (big and small). The second involves detection of tracks from noisy intermittent observations separated far apart in time and space. We will also discuss the implications that have arisen from making these operations tractable. We will focus particularly onDetecting all asteroids in the solar system larger than Pittsburgh's Cathedral of Learning (data to be collected over 2006-2010). Early detection of emerging diseases based on national monitoring of health-related transactions..", :author (#search_api.search_api.Author{:id 685289, :first-name nil, :last-name nil, :full-name "Andrew Moore"}), :year 2006, :venue "KDD", :ncit 0, :string "New cached-sufficient statistics algorithms for quickly answering statistical questions.. This talk is about recent work on new ways to exploit preprocessed views of data tables for tractably solving big statistical queries. We'll describe deployments of these new algorithms in the realms of detecting killer asteroids and unnatural disease outbreaks.In recent years, several groups have looked at methods for pre-storing general sufficient statistics of the data in spatial data structures such as kd-trees and ball-trees so that both frequentist and Bayesian statistical operations become fast for large datasets. In this talk we will look at two other classes of optimization required in important statistical queries.The first involves iterating over all spatial regions (big and small). The second involves detection of tracks from noisy intermittent observations separated far apart in time and space. We will also discuss the implications that have arisen from making these operations tractable. We will focus particularly onDetecting all asteroids in the solar system larger than Pittsburgh's Cathedral of Learning (data to be collected over 2006-2010). Early detection of emerging diseases based on national monitoring of health-related transactions..", :doc-id "New cached-sufficient statistics algorithms for quickly answering statistical questions. 2006 Andrew Moore"}, 1033319 #search_api.search_api.Paper{:id 1033319, :key "journals/pami/WechslerDLC04", :title "Motion Estimation Using Statistical Learning Theory.", :abstract "Abstract--This paper describes a novel application of Statistical Learning Theory (SLT) to single motion estimation and tracking. The problem of motion estimation can be related to statistical model selection, where the goal is to select one (correct) motion model from several possible motion models, given finite noisy samples. SLT, also known as Vapnik-Chervonenkis (VC), theory provides analytic generalization bounds for model selection, which have been used successfully for practical model selection. This paper describes a successful application of an SLT-based model selection approach to the challenging problem of estimating optimal motion models from small data sets of image measurements (flow). We present results of experiments on both synthetic and real image sequences for motion interpolation and extrapolation; these results demonstrate the feasibility and strength of our approach. Our experimental results show that for motion estimation applications, SLT-based model selection compares favorably against alternative model selection methods, such as the Akaike's fpe, Schwartz' criterion (sc), Generalized Cross-Validation (gcv), and Shibata's Model Selector (sms). The paper also shows how to address the aperture problem using SLT-based model selection for penalized linear (ridge regression) formulation.", :author (#search_api.search_api.Author{:id 1499994, :first-name nil, :last-name nil, :full-name "Harry Wechsler"} #search_api.search_api.Author{:id 398169, :first-name nil, :last-name nil, :full-name "Zoran Duric"} #search_api.search_api.Author{:id 1493163, :first-name nil, :last-name nil, :full-name "Fayin Li"} #search_api.search_api.Author{:id 1104310, :first-name nil, :last-name nil, :full-name "Vladimir Cherkassky"}), :year 2004, :venue "IEEE Trans. Pattern Anal. Mach. Intell.", :ncit 50, :string "Motion Estimation Using Statistical Learning Theory.. Abstract--This paper describes a novel application of Statistical Learning Theory (SLT) to single motion estimation and tracking. The problem of motion estimation can be related to statistical model selection, where the goal is to select one (correct) motion model from several possible motion models, given finite noisy samples. SLT, also known as Vapnik-Chervonenkis (VC), theory provides analytic generalization bounds for model selection, which have been used successfully for practical model selection. This paper describes a successful application of an SLT-based model selection approach to the challenging problem of estimating optimal motion models from small data sets of image measurements (flow). We present results of experiments on both synthetic and real image sequences for motion interpolation and extrapolation; these results demonstrate the feasibility and strength of our approach. Our experimental results show that for motion estimation applications, SLT-based model selection compares favorably against alternative model selection methods, such as the Akaike's fpe, Schwartz' criterion (sc), Generalized Cross-Validation (gcv), and Shibata's Model Selector (sms). The paper also shows how to address the aperture problem using SLT-based model selection for penalized linear (ridge regression) formulation.", :doc-id "Motion Estimation Using Statistical Learning Theory. 2004 Harry Wechsler, Zoran Duric, Fayin Li, Vladimir Cherkassky"}, 746727 #search_api.search_api.Paper{:id 746727, :key "journals/aicom/Tian06", :title "Context-based statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 260142, :first-name nil, :last-name nil, :full-name "YongHong Tian"}), :year 2006, :venue "AI Commun.", :ncit 0, :string "Context-based statistical relational learning.. ", :doc-id "Context-based statistical relational learning. 2006 YongHong Tian"}, 1323591 #search_api.search_api.Paper{:id 1323591, :key "conf/icc/TiwanaSA09", :title "Statistical Learning for Automated RRM: Application to eUTRAN Mobility.", :abstract "Self Organizing Network (SON) functionalities are currently developed to improve network performance and management tasks. SON functionalities require efficient utilization of data extracted from the network. In this context, the paper has two objectives. First it is shown that one can use simple statistical learning techniques such as regression to extract a model from data. The model comprises closed form expressions that approximate the functional relations between measured Key Performance Indicators (KPIs) and Radio Resource Management (RRM) parameters. Second, it is shown how the model can be integrated in a monitoring process and be used to devise an efficient auto-tuning algorithm. To this end, two case studies of handover monitoring and handover auto-tuning in a LTE network are described and illustrate the application of the proposed approach.", :author (#search_api.search_api.Author{:id -1, :first-name nil, :last-name nil, :full-name "Moazzam Islam Tiwana"} #search_api.search_api.Author{:id 1094720, :first-name nil, :last-name nil, :full-name "Berna Sayraç"} #search_api.search_api.Author{:id 223882, :first-name nil, :last-name nil, :full-name "Zwi Altman"}), :year 2009, :venue "ICC", :ncit 6, :string "Statistical Learning for Automated RRM: Application to eUTRAN Mobility.. Self Organizing Network (SON) functionalities are currently developed to improve network performance and management tasks. SON functionalities require efficient utilization of data extracted from the network. In this context, the paper has two objectives. First it is shown that one can use simple statistical learning techniques such as regression to extract a model from data. The model comprises closed form expressions that approximate the functional relations between measured Key Performance Indicators (KPIs) and Radio Resource Management (RRM) parameters. Second, it is shown how the model can be integrated in a monitoring process and be used to devise an efficient auto-tuning algorithm. To this end, two case studies of handover monitoring and handover auto-tuning in a LTE network are described and illustrate the application of the proposed approach.", :doc-id "Statistical Learning for Automated RRM: Application to eUTRAN Mobility. 2009 Moazzam Islam Tiwana, Berna Sayraç, Zwi Altman"}, 3382280 #search_api.search_api.Paper{:id 3382280, :key "journals/cogsci/ArciuliS12", :title "Statistical Learning Is Related to Reading Ability in Children and Adults.", :abstract nil, :author (#search_api.search_api.Author{:id 14233998, :first-name nil, :last-name nil, :full-name "Joanne Arciuli"} #search_api.search_api.Author{:id 14270159, :first-name nil, :last-name nil, :full-name "Ian C. Simpson"}), :year 2012, :venue "Cognitive Science", :ncit 0, :string "Statistical Learning Is Related to Reading Ability in Children and Adults.. ", :doc-id "Statistical Learning Is Related to Reading Ability in Children and Adults. 2012 Joanne Arciuli, Ian C. Simpson"}, 893160 #search_api.search_api.Paper{:id 893160, :key "journals/ida/CucchiaraMPR01", :title "An application of machine learning and statistics to defect detection.", :abstract "We present an application of machine learning and statistics to the problem of distinguishing between defective and non-defective industrial workpieces, where the defect takes the form of a long and thin crack on the surface of the piece. From the images of pieces a number of features are extracted by using the Hough transform and the Correlated Hough transform. Two datasets are considered, one containing only features related to the Hough transform and the other containing also features related to the Correlated Hough transform. On these datasets we have compared six different learning algorithms: an attribute-value learner, C4.5, a backpropagation neural network, NeuralWorks Predict, a k-nearest neighbour algorithm, and three statistical techniques, linear, logistic and quadratic discriminant. The experiments show that C4.5 performs best for both feature sets and gives an average accuracy of 93.3% for the first dataset and 95.9% for the second dataset.", :author (#search_api.search_api.Author{:id 394534, :first-name nil, :last-name nil, :full-name "Rita Cucchiara"} #search_api.search_api.Author{:id 1065974, :first-name nil, :last-name nil, :full-name "Paola Mello"} #search_api.search_api.Author{:id 893047, :first-name nil, :last-name nil, :full-name "Massimo Piccardi"} #search_api.search_api.Author{:id 1316705, :first-name nil, :last-name nil, :full-name "Fabrizio Riguzzi"}), :year 2001, :venue "Intell. Data Anal.", :ncit 3, :string "An application of machine learning and statistics to defect detection.. We present an application of machine learning and statistics to the problem of distinguishing between defective and non-defective industrial workpieces, where the defect takes the form of a long and thin crack on the surface of the piece. From the images of pieces a number of features are extracted by using the Hough transform and the Correlated Hough transform. Two datasets are considered, one containing only features related to the Hough transform and the other containing also features related to the Correlated Hough transform. On these datasets we have compared six different learning algorithms: an attribute-value learner, C4.5, a backpropagation neural network, NeuralWorks Predict, a k-nearest neighbour algorithm, and three statistical techniques, linear, logistic and quadratic discriminant. The experiments show that C4.5 performs best for both feature sets and gives an average accuracy of 93.3% for the first dataset and 95.9% for the second dataset.", :doc-id "An application of machine learning and statistics to defect detection. 2001 Rita Cucchiara, Paola Mello, Massimo Piccardi, Fabrizio Riguzzi"}, 2927016 #search_api.search_api.Paper{:id 2927016, :key "journals/sigkdd/Landwehr09", :title "Trading expressivity for efficiency in statistical relational learning: Ph.D. thesis abstract.", :abstract nil, :author (#search_api.search_api.Author{:id 127590, :first-name nil, :last-name nil, :full-name "Niels Landwehr"}), :year 2009, :venue "SIGKDD Explorations", :ncit 0, :string "Trading expressivity for efficiency in statistical relational learning: Ph.D. thesis abstract.. ", :doc-id "Trading expressivity for efficiency in statistical relational learning: Ph.D. thesis abstract. 2009 Niels Landwehr"}, 2881512 #search_api.search_api.Paper{:id 2881512, :key "journals/jocn/Turk-BrowneSCJ09", :title "Neural Evidence of Statistical Learning: Efficient Detection of Visual Regularities Without Awareness.", :abstract "Our environment contains regularities distributed in space and time that can be detected by way of statistical learning. This unsupervised learning occurs without intent or awareness, but little is known about how it relates to other types of learning, how it affects perceptual processing, and how quickly it can occur. Here we use fMRI during statistical learning to explore these questions. Participants viewed statistically structured versus unstructured sequences of shapes while performing a task unrelated to the structure. Robust neural responses to statistical structure were observed, and these responses were notable in four ways: First, responses to structure were observed in the striatum and medial temporal lobe, suggesting that statistical learning may be related to other forms of associative learning and relational memory. Second, statistical regularities yielded greater activation in category-specific visual regions (object-selective lateral occipital cortex and word-selective ventral occipito-temporal cortex), demonstrating that these regions are sensitive to information distributed in time. Third, evidence of learning emerged early during familiarization, showing that statistical learning can operate very quickly and with little exposure. Finally, neural signatures of learning were dissociable from subsequent explicit familiarity, suggesting that learning can occur in the absence of awareness. Overall, our findings help elucidate the underlying nature of statistical learning.", :author (#search_api.search_api.Author{:id 795446, :first-name nil, :last-name nil, :full-name "Nicholas B. Turk-Browne"} #search_api.search_api.Author{:id 1336480, :first-name nil, :last-name nil, :full-name "Brian J. Scholl"} #search_api.search_api.Author{:id 1113401, :first-name nil, :last-name nil, :full-name "Marvin M. Chun"} #search_api.search_api.Author{:id 778152, :first-name nil, :last-name nil, :full-name "Marcia K. Johnson"}), :year 2009, :venue "J. Cognitive Neuroscience", :ncit 25, :string "Neural Evidence of Statistical Learning: Efficient Detection of Visual Regularities Without Awareness.. Our environment contains regularities distributed in space and time that can be detected by way of statistical learning. This unsupervised learning occurs without intent or awareness, but little is known about how it relates to other types of learning, how it affects perceptual processing, and how quickly it can occur. Here we use fMRI during statistical learning to explore these questions. Participants viewed statistically structured versus unstructured sequences of shapes while performing a task unrelated to the structure. Robust neural responses to statistical structure were observed, and these responses were notable in four ways: First, responses to structure were observed in the striatum and medial temporal lobe, suggesting that statistical learning may be related to other forms of associative learning and relational memory. Second, statistical regularities yielded greater activation in category-specific visual regions (object-selective lateral occipital cortex and word-selective ventral occipito-temporal cortex), demonstrating that these regions are sensitive to information distributed in time. Third, evidence of learning emerged early during familiarization, showing that statistical learning can operate very quickly and with little exposure. Finally, neural signatures of learning were dissociable from subsequent explicit familiarity, suggesting that learning can occur in the absence of awareness. Overall, our findings help elucidate the underlying nature of statistical learning.", :doc-id "Neural Evidence of Statistical Learning: Efficient Detection of Visual Regularities Without Awareness. 2009 Nicholas B. Turk-Browne, Brian J. Scholl, Marvin M. Chun, Marcia K. Johnson"}, 473321 #search_api.search_api.Paper{:id 473321, :key "conf/kdd/SuchanekIW06", :title "Combining linguistic and statistical analysis to extract relations from web documents.", :abstract "The World Wide Web provides a nearly endless source of knowledge, which is mostly given in natural language. A first step towards exploiting this data automatically could be to extract pairs of a given semantic relation from text documents - for example all pairs of a person and her birthdate. One strategy for this task is to find text patterns that express the semantic relation, to generalize these patterns, and to apply them to a corpus to find new pairs. In this paper, we show that this approach profits significantly when deep linguistic structures are used instead of surface text patterns. We demonstrate how linguistic structures can be represented for machine learning, and we provide a theoretical analysis of the pattern matching approach. We show the benefits of our approach by extensive experiments with our prototype system LEILA.", :author (#search_api.search_api.Author{:id 793922, :first-name nil, :last-name nil, :full-name "Fabian M. Suchanek"} #search_api.search_api.Author{:id 528047, :first-name nil, :last-name nil, :full-name "Georgiana Ifrim"} #search_api.search_api.Author{:id 1015097, :first-name nil, :last-name nil, :full-name "Gerhard Weikum"}), :year 2006, :venue "KDD", :ncit 97, :string "Combining linguistic and statistical analysis to extract relations from web documents.. The World Wide Web provides a nearly endless source of knowledge, which is mostly given in natural language. A first step towards exploiting this data automatically could be to extract pairs of a given semantic relation from text documents - for example all pairs of a person and her birthdate. One strategy for this task is to find text patterns that express the semantic relation, to generalize these patterns, and to apply them to a corpus to find new pairs. In this paper, we show that this approach profits significantly when deep linguistic structures are used instead of surface text patterns. We demonstrate how linguistic structures can be represented for machine learning, and we provide a theoretical analysis of the pattern matching approach. We show the benefits of our approach by extensive experiments with our prototype system LEILA.", :doc-id "Combining linguistic and statistical analysis to extract relations from web documents. 2006 Fabian M. Suchanek, Georgiana Ifrim, Gerhard Weikum"}, 91401 #search_api.search_api.Paper{:id 91401, :key "conf/chi/PatelFLH08", :title "Investigating statistical machine learning as a tool for software development.", :abstract "As statistical machine learning algorithms and techniques continue to mature, many researchers and developers see statistical machine learning not only as a topic of expert study, but also as a tool for software development. Extensive prior work has studied software development, but little prior work has studied software developers applying statistical machine learning. This paper presents interviews of eleven researchers experienced in applying statistical machine learning algorithms and techniques to human-computer interaction problems, as well as a study of ten participants working during a five-hour study to apply statistical machine learning algorithms and techniques to a realistic problem. We distill three related categories of difficulties that arise in applying statistical machine learning as a tool for software development: (1) difficulty pursuing statistical machine learning as an iterative and exploratory process, (2) difficulty understanding relationships between data and the behavior of statistical machine learning algorithms, and (3) difficulty evaluating the performance of statistical machine learning algorithms and techniques in the context of applications. This paper provides important new insight into these difficulties and the need for development tools that better support the application of statistical machine learning.", :author (#search_api.search_api.Author{:id 939325, :first-name nil, :last-name nil, :full-name "Kayur Patel"} #search_api.search_api.Author{:id 1505625, :first-name nil, :last-name nil, :full-name "James Fogarty"} #search_api.search_api.Author{:id 82771, :first-name nil, :last-name nil, :full-name "James A. Landay"} #search_api.search_api.Author{:id 907542, :first-name nil, :last-name nil, :full-name "Beverly L. Harrison"}), :year 2008, :venue "CHI", :ncit 22, :string "Investigating statistical machine learning as a tool for software development.. As statistical machine learning algorithms and techniques continue to mature, many researchers and developers see statistical machine learning not only as a topic of expert study, but also as a tool for software development. Extensive prior work has studied software development, but little prior work has studied software developers applying statistical machine learning. This paper presents interviews of eleven researchers experienced in applying statistical machine learning algorithms and techniques to human-computer interaction problems, as well as a study of ten participants working during a five-hour study to apply statistical machine learning algorithms and techniques to a realistic problem. We distill three related categories of difficulties that arise in applying statistical machine learning as a tool for software development: (1) difficulty pursuing statistical machine learning as an iterative and exploratory process, (2) difficulty understanding relationships between data and the behavior of statistical machine learning algorithms, and (3) difficulty evaluating the performance of statistical machine learning algorithms and techniques in the context of applications. This paper provides important new insight into these difficulties and the need for development tools that better support the application of statistical machine learning.", :doc-id "Investigating statistical machine learning as a tool for software development. 2008 Kayur Patel, James Fogarty, James A. Landay, Beverly L. Harrison"}, 472937 #search_api.search_api.Paper{:id 472937, :key "conf/kdd/Koller03", :title "Statistical learning from relational data.", :abstract nil, :author (#search_api.search_api.Author{:id 560995, :first-name nil, :last-name nil, :full-name "Daphne Koller"}), :year 2003, :venue "KDD", :ncit 0, :string "Statistical learning from relational data.. ", :doc-id "Statistical learning from relational data. 2003 Daphne Koller"}, 3786794 #search_api.search_api.Paper{:id 3786794, :key "conf/fusion/JandelSW12", :title "Online learnability of Statistical Relational Learning in anomaly detection.", :abstract nil, :author (#search_api.search_api.Author{:id 1344508, :first-name nil, :last-name nil, :full-name "Magnus Jändel"} #search_api.search_api.Author{:id 1319563, :first-name nil, :last-name nil, :full-name "Pontus Svenson"} #search_api.search_api.Author{:id 14343624, :first-name nil, :last-name nil, :full-name "Niclas Wadströmer"}), :year 2012, :venue "FUSION", :ncit 0, :string "Online learnability of Statistical Relational Learning in anomaly detection.. ", :doc-id "Online learnability of Statistical Relational Learning in anomaly detection. 2012 Magnus Jändel, Pontus Svenson, Niclas Wadströmer"}, 2927018 #search_api.search_api.Paper{:id 2927018, :key "journals/sigkdd/RamonCFK09", :title "StReBio'09: statistical relational learning and mining in bioinformatics.", :abstract nil, :author (#search_api.search_api.Author{:id 1015109, :first-name nil, :last-name nil, :full-name "Jan Ramon"} #search_api.search_api.Author{:id 892400, :first-name nil, :last-name nil, :full-name "Fabrizio Costa"} #search_api.search_api.Author{:id 981793, :first-name nil, :last-name nil, :full-name "Christophe Costa Florêncio"} #search_api.search_api.Author{:id 19675, :first-name nil, :last-name nil, :full-name "Joost N. Kok"}), :year 2009, :venue "SIGKDD Explorations", :ncit 0, :string "StReBio'09: statistical relational learning and mining in bioinformatics.. ", :doc-id "StReBio'09: statistical relational learning and mining in bioinformatics. 2009 Jan Ramon, Fabrizio Costa, Christophe Costa Florêncio, Joost N. Kok"}, 950858 #search_api.search_api.Paper{:id 950858, :key "journals/jacm/Kearns98", :title "Efficient Noise-Tolerant Learning from Statistical Queries.", :abstract "In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :author (#search_api.search_api.Author{:id 1249307, :first-name nil, :last-name nil, :full-name "Michael J. Kearns"}), :year 1998, :venue "J. ACM", :ncit 456, :string "Efficient Noise-Tolerant Learning from Statistical Queries.. In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :doc-id "Efficient Noise-Tolerant Learning from Statistical Queries. 1998 Michael J. Kearns"}, 3258026 #search_api.search_api.Paper{:id 3258026, :key "journals/ml/RiguzziM12", :title "Applying the information bottleneck to statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 1316705, :first-name nil, :last-name nil, :full-name "Fabrizio Riguzzi"} #search_api.search_api.Author{:id 1294122, :first-name nil, :last-name nil, :full-name "Nicola Di Mauro"}), :year 2012, :venue "Machine Learning", :ncit 14, :string "Applying the information bottleneck to statistical relational learning.. ", :doc-id "Applying the information bottleneck to statistical relational learning. 2012 Fabrizio Riguzzi, Nicola Di Mauro"}, 1288331 #search_api.search_api.Paper{:id 1288331, :key "journals/sadm/SundararaghavanZ09", :title "A statistical learning approach for the design of polycrystalline materials.", :abstract "Important physical properties such as yield strength, elastic modulus, and thermal conductivity depend on the material microstructure. Realization of optimal microstructures is important for hardware components in aerospace applications where there is a need to optimize material properties for improved performance. Microstructures can be tailored through controlled deformation or heat treatment. However, identification of the optimal processing path is a non-trivial (and non-unique) problem. Data-mining techniques are eminently suitable for process design since optimal processing paths can be selected based on available information from a large database-relating processes, properties, and microstructures. In this paper, the problem of designing processing stages that lead to a desired microstructure or material property is addressed by mining over a database of microstructural signatures. A hierarchical X-means classifier is designed to match crystallographic orientation features to a class of microstructural signatures within a database. Instead of the conventional distortion minimization algorithm of k-means, X-means maximizes a Bayesian information measure for calculating cluster centers which allows automatic detection of number of classes. Using the microstructural database, an adaptive data-compression technique based on proper orthogonal decomposition (POD) has been designed to accelerate materials design. In this technique, reduced modes selected adaptively from the database are used to speed up auxiliary microstructure optimization algorithms built over the database. Copyright &copy; 2009 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 1: 000-000, 2009", :author (#search_api.search_api.Author{:id 1415860, :first-name nil, :last-name nil, :full-name "Veera Sundararaghavan"} #search_api.search_api.Author{:id 585807, :first-name nil, :last-name nil, :full-name "Nicholas Zabaras"}), :year 2009, :venue "Statistical Analysis and Data Mining", :ncit 3, :string "A statistical learning approach for the design of polycrystalline materials.. Important physical properties such as yield strength, elastic modulus, and thermal conductivity depend on the material microstructure. Realization of optimal microstructures is important for hardware components in aerospace applications where there is a need to optimize material properties for improved performance. Microstructures can be tailored through controlled deformation or heat treatment. However, identification of the optimal processing path is a non-trivial (and non-unique) problem. Data-mining techniques are eminently suitable for process design since optimal processing paths can be selected based on available information from a large database-relating processes, properties, and microstructures. In this paper, the problem of designing processing stages that lead to a desired microstructure or material property is addressed by mining over a database of microstructural signatures. A hierarchical X-means classifier is designed to match crystallographic orientation features to a class of microstructural signatures within a database. Instead of the conventional distortion minimization algorithm of k-means, X-means maximizes a Bayesian information measure for calculating cluster centers which allows automatic detection of number of classes. Using the microstructural database, an adaptive data-compression technique based on proper orthogonal decomposition (POD) has been designed to accelerate materials design. In this technique, reduced modes selected adaptively from the database are used to speed up auxiliary microstructure optimization algorithms built over the database. Copyright &copy; 2009 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 1: 000-000, 2009", :doc-id "A statistical learning approach for the design of polycrystalline materials. 2009 Veera Sundararaghavan, Nicholas Zabaras"}, 797516 #search_api.search_api.Paper{:id 797516, :key "journals/coling/OchN04", :title "The Alignment Template Approach to Statistical Machine Translation.", :abstract "A phrase-based statistical machine translation approach &mdash; the alignment template approach &mdash; is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source&ndash;channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German&ndash;English speech VERBMOBIL task, we analyze the effect of various system components. On the French&ndash;English Canadian HANSARDS task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese&ndash;English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.", :author (#search_api.search_api.Author{:id 1493982, :first-name nil, :last-name nil, :full-name "Franz Josef Och"} #search_api.search_api.Author{:id 526052, :first-name nil, :last-name nil, :full-name "Hermann Ney"}), :year 2004, :venue "Computational Linguistics", :ncit 644, :string "The Alignment Template Approach to Statistical Machine Translation.. A phrase-based statistical machine translation approach &mdash; the alignment template approach &mdash; is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source&ndash;channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German&ndash;English speech VERBMOBIL task, we analyze the effect of various system components. On the French&ndash;English Canadian HANSARDS task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese&ndash;English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.", :doc-id "The Alignment Template Approach to Statistical Machine Translation. 2004 Franz Josef Och, Hermann Ney"}, 3074924 #search_api.search_api.Paper{:id 3074924, :key "journals/ml/RettingerNT11", :title "Statistical relational learning of trust.", :abstract nil, :author (#search_api.search_api.Author{:id 444383, :first-name nil, :last-name nil, :full-name "Achim Rettinger"} #search_api.search_api.Author{:id 222492, :first-name nil, :last-name nil, :full-name "Matthias Nickles"} #search_api.search_api.Author{:id 680469, :first-name nil, :last-name nil, :full-name "Volker Tresp"}), :year 2011, :venue "Machine Learning", :ncit 6, :string "Statistical relational learning of trust.. ", :doc-id "Statistical relational learning of trust. 2011 Achim Rettinger, Matthias Nickles, Volker Tresp"}, 1185901 #search_api.search_api.Paper{:id 1185901, :key "journals/jocn/MuellerBF08", :title "The Role of Pause Cues in Language Learning: The Emergence of Event-related Potentials Related to Sequence Processing.", :abstract "Humans can derive sequential dependencies from unfamiliar artificial speech within several of minutes of exposure. However, there is an ongoing debate about the nature of the underlying learning processes. In a widely discussed study Pea et al. [Pea, M., Bonatti, L. L., Nespor, M., & Mehler, J. Signal-driven computations in speech processing. Science, 298, 604607, 2002] argued for the importance of subtle acoustic cues in the signal, such as pauses, in order to switch between two computational mechanisms, which are conceptualized as rule-based versus statistical. The present study was aimed to approach this problem by recording event-related potentials in response to correct and incorrect phrases consisting of bisyllabics after short exposure to either rule-based or random artificial speech streams. Rule-based streams contained dependencies of the form AXC, whereby A elements reliably predicted the C elements and X elements were variable. Participants were exposed to four input and test phases. Two of the input streams were rule-based and contained either only probabilistic information related to the distribution of the AXC stimuli or an additional acoustic cue indicating the boundaries of relevant units. The other two streams were random variations of the rule-based streams. During the test phase in the condition with pause cues, an early negativity and a later positivity emerged for correct and incorrect items in comparison to their acoustically identical counterparts, which were presented after the random control condition. In the noncued condition, only negativities were seen. The timing and the scalp distribution of the negativities were different for correct and incorrect sequences in both the cued and the noncued conditions. The results are interpreted in support of a view of grammatical learning in which both distributional and acoustic cues may contribute to different aspects of syntactic learning.", :author (#search_api.search_api.Author{:id 983626, :first-name nil, :last-name nil, :full-name "Jutta L. Mueller"} #search_api.search_api.Author{:id -1, :first-name nil, :last-name nil, :full-name "Jörg Bahlmann"} #search_api.search_api.Author{:id 258614, :first-name nil, :last-name nil, :full-name "Angela D. Friederici"}), :year 2008, :venue "J. Cognitive Neuroscience", :ncit 16, :string "The Role of Pause Cues in Language Learning: The Emergence of Event-related Potentials Related to Sequence Processing.. Humans can derive sequential dependencies from unfamiliar artificial speech within several of minutes of exposure. However, there is an ongoing debate about the nature of the underlying learning processes. In a widely discussed study Pea et al. [Pea, M., Bonatti, L. L., Nespor, M., & Mehler, J. Signal-driven computations in speech processing. Science, 298, 604607, 2002] argued for the importance of subtle acoustic cues in the signal, such as pauses, in order to switch between two computational mechanisms, which are conceptualized as rule-based versus statistical. The present study was aimed to approach this problem by recording event-related potentials in response to correct and incorrect phrases consisting of bisyllabics after short exposure to either rule-based or random artificial speech streams. Rule-based streams contained dependencies of the form AXC, whereby A elements reliably predicted the C elements and X elements were variable. Participants were exposed to four input and test phases. Two of the input streams were rule-based and contained either only probabilistic information related to the distribution of the AXC stimuli or an additional acoustic cue indicating the boundaries of relevant units. The other two streams were random variations of the rule-based streams. During the test phase in the condition with pause cues, an early negativity and a later positivity emerged for correct and incorrect items in comparison to their acoustically identical counterparts, which were presented after the random control condition. In the noncued condition, only negativities were seen. The timing and the scalp distribution of the negativities were different for correct and incorrect sequences in both the cued and the noncued conditions. The results are interpreted in support of a view of grammatical learning in which both distributional and acoustic cues may contribute to different aspects of syntactic learning.", :doc-id "The Role of Pause Cues in Language Learning: The Emergence of Event-related Potentials Related to Sequence Processing. 2008 Jutta L. Mueller, Jörg Bahlmann, Angela D. Friederici"}, 1028237 #search_api.search_api.Paper{:id 1028237, :key "journals/npl/LuoUN99", :title "Unsupervised Learning of Higher-Order Statistics.", :abstract "This paper deals with the adaptive extraction of higher-order statistics of related signals. We will show how to use higher-order neural networks to adaptively extract the higher-order cumulant matrices and tensors with an invariant weight norm. This proposed scheme can serve as an alternative tool in many application fields with higher-order statistics.", :author (#search_api.search_api.Author{:id 658006, :first-name nil, :last-name nil, :full-name "Fa-Long Luo"} #search_api.search_api.Author{:id 416876, :first-name nil, :last-name nil, :full-name "Rolf Unbehauen"} #search_api.search_api.Author{:id 1370072, :first-name nil, :last-name nil, :full-name "Tertulien Ndjountche"}), :year 1999, :venue "Neural Processing Letters", :ncit 0, :string "Unsupervised Learning of Higher-Order Statistics.. This paper deals with the adaptive extraction of higher-order statistics of related signals. We will show how to use higher-order neural networks to adaptively extract the higher-order cumulant matrices and tensors with an invariant weight norm. This proposed scheme can serve as an alternative tool in many application fields with higher-order statistics.", :doc-id "Unsupervised Learning of Higher-Order Statistics. 1999 Fa-Long Luo, Rolf Unbehauen, Tertulien Ndjountche"}, 20109 #search_api.search_api.Paper{:id 20109, :key "conf/acl/Chiang05", :title "A Hierarchical Phrase-Based Model for Statistical Machine Translation.", :abstract "We present a statistical phrase-based translation model that uses hierarchical phrases---phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.", :author (#search_api.search_api.Author{:id 415922, :first-name nil, :last-name nil, :full-name "David Chiang"}), :year 2005, :venue "ACL", :ncit 739, :string "A Hierarchical Phrase-Based Model for Statistical Machine Translation.. We present a statistical phrase-based translation model that uses hierarchical phrases---phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.", :doc-id "A Hierarchical Phrase-Based Model for Statistical Machine Translation. 2005 David Chiang"}, 541902 #search_api.search_api.Paper{:id 541902, :key "conf/pkdd/Raedt05", :title "Statistical Relational Learning: An Inductive Logic Programming Perspective.", :abstract nil, :author (#search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2005, :venue "PKDD", :ncit 3, :string "Statistical Relational Learning: An Inductive Logic Programming Perspective.. ", :doc-id "Statistical Relational Learning: An Inductive Logic Programming Perspective. 2005 Luc De Raedt"}, 1010126 #search_api.search_api.Paper{:id 1010126, :key "journals/ml/DietterichDGMT08", :title "Structured machine learning: the next ten years.", :abstract "The field of inductive logic programming (ILP) has made steady progress, since the first ILP workshop in 1991, based on a balance of developments in theory, implementations and applications. More recently there has been an increased emphasis on Probabilistic ILP and the related fields of Statistical Relational Learning (SRL) and Structured Prediction. The goal of the current paper is to consider these emerging trends and chart out the strategic directions and open problems for the broader area of structured machine learning for the next 10 years.", :author (#search_api.search_api.Author{:id 737377, :first-name nil, :last-name nil, :full-name "Thomas G. Dietterich"} #search_api.search_api.Author{:id 668085, :first-name nil, :last-name nil, :full-name "Pedro Domingos"} #search_api.search_api.Author{:id 1449720, :first-name nil, :last-name nil, :full-name "Lise Getoor"} #search_api.search_api.Author{:id 120131, :first-name nil, :last-name nil, :full-name "Stephen Muggleton"} #search_api.search_api.Author{:id 435675, :first-name nil, :last-name nil, :full-name "Prasad Tadepalli"}), :year 2008, :venue "Machine Learning", :ncit 42, :string "Structured machine learning: the next ten years.. The field of inductive logic programming (ILP) has made steady progress, since the first ILP workshop in 1991, based on a balance of developments in theory, implementations and applications. More recently there has been an increased emphasis on Probabilistic ILP and the related fields of Statistical Relational Learning (SRL) and Structured Prediction. The goal of the current paper is to consider these emerging trends and chart out the strategic directions and open problems for the broader area of structured machine learning for the next 10 years.", :doc-id "Structured machine learning: the next ten years. 2008 Thomas G. Dietterich, Pedro Domingos, Lise Getoor, Stephen Muggleton, Prasad Tadepalli"}, 1009230 #search_api.search_api.Paper{:id 1009230, :key "journals/ml/BrazdilSC03", :title "Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results.", :abstract "We present a meta-learning method to support selection of candidate learning algorithms. It uses a k-Nearest Neighbor algorithm to identify the datasets that are most similar to the one at hand. The distance between datasets is assessed using a relatively small set of data characteristics, which was selected to represent properties that affect algorithm performance. The performance of the candidate algorithms on those datasets is used to generate a recommendation to the user in the form of a ranking. The performance is assessed using a multicriteria evaluation measure that takes not only accuracy, but also time into account. As it is not common in Machine Learning to work with rankings, we had to identify and adapt existing statistical techniques to devise an appropriate evaluation methodology. Using that methodology, we show that the meta-learning method presented leads to significantly better rankings than the baseline ranking method. The evaluation methodology is general and can be adapted to other ranking problems. Although here we have concentrated on ranking classification algorithms, the meta-learning framework presented can provide assistance in the selection of combinations of methods or more complex problem solving strategies.", :author (#search_api.search_api.Author{:id 1157783, :first-name nil, :last-name nil, :full-name "Pavel Brazdil"} #search_api.search_api.Author{:id 791367, :first-name nil, :last-name nil, :full-name "Carlos Soares"} #search_api.search_api.Author{:id 212848, :first-name nil, :last-name nil, :full-name "Joaquim Pinto da Costa"}), :year 2003, :venue "Machine Learning", :ncit 200, :string "Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results.. We present a meta-learning method to support selection of candidate learning algorithms. It uses a k-Nearest Neighbor algorithm to identify the datasets that are most similar to the one at hand. The distance between datasets is assessed using a relatively small set of data characteristics, which was selected to represent properties that affect algorithm performance. The performance of the candidate algorithms on those datasets is used to generate a recommendation to the user in the form of a ranking. The performance is assessed using a multicriteria evaluation measure that takes not only accuracy, but also time into account. As it is not common in Machine Learning to work with rankings, we had to identify and adapt existing statistical techniques to devise an appropriate evaluation methodology. Using that methodology, we show that the meta-learning method presented leads to significantly better rankings than the baseline ranking method. The evaluation methodology is general and can be adapted to other ranking problems. Although here we have concentrated on ranking classification algorithms, the meta-learning framework presented can provide assistance in the selection of combinations of methods or more complex problem solving strategies.", :doc-id "Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results. 2003 Pavel Brazdil, Carlos Soares, Joaquim Pinto da Costa"}, 473199 #search_api.search_api.Paper{:id 473199, :key "conf/kdd/PopesculU04", :title "Cluster-based concept invention for statistical relational learning.", :abstract "We use clustering to derive new relations which augment database schema used in automatic generation of predictive features in statistical relational learning. Entities derived from clusters increase the expressivity of feature spaces by creating new first-class concepts which contribute to the creation of new features. For example, in CiteSeer, papers can be clustered based on words or citations giving \"topics\", and authors can be clustered based on documents they co-author giving \"communities\". Such cluster-derived concepts become part of more complex feature expressions. Out of the large number of generated features, those which improve predictive accuracy are kept in the model, as decided by statistical feature selection criteria. We present results demonstrating improved accuracy on two tasks, venue prediction and link prediction, using CiteSeer data.", :author (#search_api.search_api.Author{:id 597076, :first-name nil, :last-name nil, :full-name "Alexandrin Popescul"} #search_api.search_api.Author{:id 352656, :first-name nil, :last-name nil, :full-name "Lyle H. Ungar"}), :year 2004, :venue "KDD", :ncit 31, :string "Cluster-based concept invention for statistical relational learning.. We use clustering to derive new relations which augment database schema used in automatic generation of predictive features in statistical relational learning. Entities derived from clusters increase the expressivity of feature spaces by creating new first-class concepts which contribute to the creation of new features. For example, in CiteSeer, papers can be clustered based on words or citations giving \"topics\", and authors can be clustered based on documents they co-author giving \"communities\". Such cluster-derived concepts become part of more complex feature expressions. Out of the large number of generated features, those which improve predictive accuracy are kept in the model, as decided by statistical feature selection criteria. We present results demonstrating improved accuracy on two tasks, venue prediction and link prediction, using CiteSeer data.", :doc-id "Cluster-based concept invention for statistical relational learning. 2004 Alexandrin Popescul, Lyle H. Ungar"}, 472783 #search_api.search_api.Paper{:id 472783, :key "conf/kdd/HeckermanGC94", :title "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.", :abstract "We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :author (#search_api.search_api.Author{:id 114083, :first-name nil, :last-name nil, :full-name "David Heckerman"} #search_api.search_api.Author{:id 330355, :first-name nil, :last-name nil, :full-name "Dan Geiger"} #search_api.search_api.Author{:id 1215050, :first-name nil, :last-name nil, :full-name "David Maxwell Chickering"}), :year 1994, :venue "KDD Workshop", :ncit 3837, :string "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.. We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :doc-id "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data. 1994 David Heckerman, Dan Geiger, David Maxwell Chickering"}, 952368 #search_api.search_api.Paper{:id 952368, :key "journals/jair/DaumeM06", :title "Domain Adaptation for Statistical Classifiers.", :abstract "The most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution. Unfortunately, in many applications, the \"in-domain\" test data is drawn from a distribution that is related, but not identical, to the \"out-of-domain\" distribution of the training data. We consider the common case in which labeled out-of-domain data is plentiful, but labeled in-domain data is scarce. We introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts. We present efficient inference algorithms for this special case based on the technique of conditional expectation maximization. Our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain.", :author (#search_api.search_api.Author{:id 820633, :first-name nil, :last-name nil, :full-name "Hal Daumé III"} #search_api.search_api.Author{:id 911330, :first-name nil, :last-name nil, :full-name "Daniel Marcu"}), :year 2006, :venue "J. Artif. Intell. Res. (JAIR)", :ncit 250, :string "Domain Adaptation for Statistical Classifiers.. The most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution. Unfortunately, in many applications, the \"in-domain\" test data is drawn from a distribution that is related, but not identical, to the \"out-of-domain\" distribution of the training data. We consider the common case in which labeled out-of-domain data is plentiful, but labeled in-domain data is scarce. We introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts. We present efficient inference algorithms for this special case based on the technique of conditional expectation maximization. Our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain.", :doc-id "Domain Adaptation for Statistical Classifiers. 2006 Hal Daumé III, Daniel Marcu"}, 2775184 #search_api.search_api.Paper{:id 2775184, :key "conf/ai/KhosraviB10", :title "A Survey on Statistical Relational Learning.", :abstract "Frequent subgraph mining is an active research topic in the data mining community. A graph is a general model to represent data and has been used in many domains like cheminformatics and bioinformatics. Mining patterns from graph databases is challenging since graph related operations, such as subgraph testing, generally have higher time complexity than the corresponding operations on itemsets, sequences, and trees, which have been studied extensively. We propose a novel frequent subgraph mining algorithm: FFSM, which employs a vertical search scheme within an algebraic graph framework we have developed to reduce the number of redundant candidates proposed. Our empirical study on synthetic and real datasets demonstrates that FFSM achieves a substantial performance gain over the current start-of-the-art subgraph mining algorithm gSpan.", :author (#search_api.search_api.Author{:id 133910, :first-name nil, :last-name nil, :full-name "Hassan Khosravi"} #search_api.search_api.Author{:id 84842, :first-name nil, :last-name nil, :full-name "Bahareh Bina"}), :year 2010, :venue "Canadian Conference on AI", :ncit 1, :string "A Survey on Statistical Relational Learning.. Frequent subgraph mining is an active research topic in the data mining community. A graph is a general model to represent data and has been used in many domains like cheminformatics and bioinformatics. Mining patterns from graph databases is challenging since graph related operations, such as subgraph testing, generally have higher time complexity than the corresponding operations on itemsets, sequences, and trees, which have been studied extensively. We propose a novel frequent subgraph mining algorithm: FFSM, which employs a vertical search scheme within an algebraic graph framework we have developed to reduce the number of redundant candidates proposed. Our empirical study on synthetic and real datasets demonstrates that FFSM achieves a substantial performance gain over the current start-of-the-art subgraph mining algorithm gSpan.", :doc-id "A Survey on Statistical Relational Learning. 2010 Hassan Khosravi, Bahareh Bina"}, 1319504 #search_api.search_api.Paper{:id 1319504, :key "conf/icdm/NevilleGE09", :title "Evaluating Statistical Tests for Within-Network Classifiers of Relational Data.", :abstract "Recently a number of modeling techniques have been developed for data mining and machine learning in relational and network domains where the instances are not independent and identically distributed (i.i.d.). These methods specifically exploit the statistical dependencies among instances in order to improve classification accuracy. However, there has been little focus on how these same dependencies affect our ability to draw accurate conclusions about the performance of the models. More specifically, the complex link structure and attribute dependencies in network data violate the assumptions of many conventional statistical tests and make it difficult to use these tests to assess the models in an unbiased manner. In this work, we examine the task of within-network classification and the question of whether two algorithms will learn models which will result in significantly different levels of performance. We show that the commonly-used form of evaluation (paired t-test on overlapping network samples) can result in an unacceptable level of Type I error. Furthermore we show that Type I error increases as (1) the correlation among instances increases and (2) the size of the evaluation set increases (i.e., the proportion of labeled nodes in the network decreases). We propose a method for network cross-validation that combined with paired t-tests produces more acceptable levels of Type I error while still providing reasonable levels of statistical power (i.e., Type II error).", :author (#search_api.search_api.Author{:id 150768, :first-name nil, :last-name nil, :full-name "Jennifer Neville"} #search_api.search_api.Author{:id 335801, :first-name nil, :last-name nil, :full-name "Brian Gallagher"} #search_api.search_api.Author{:id 806969, :first-name nil, :last-name nil, :full-name "Tina Eliassi-Rad"}), :year 2009, :venue "ICDM", :ncit 12, :string "Evaluating Statistical Tests for Within-Network Classifiers of Relational Data.. Recently a number of modeling techniques have been developed for data mining and machine learning in relational and network domains where the instances are not independent and identically distributed (i.i.d.). These methods specifically exploit the statistical dependencies among instances in order to improve classification accuracy. However, there has been little focus on how these same dependencies affect our ability to draw accurate conclusions about the performance of the models. More specifically, the complex link structure and attribute dependencies in network data violate the assumptions of many conventional statistical tests and make it difficult to use these tests to assess the models in an unbiased manner. In this work, we examine the task of within-network classification and the question of whether two algorithms will learn models which will result in significantly different levels of performance. We show that the commonly-used form of evaluation (paired t-test on overlapping network samples) can result in an unacceptable level of Type I error. Furthermore we show that Type I error increases as (1) the correlation among instances increases and (2) the size of the evaluation set increases (i.e., the proportion of labeled nodes in the network decreases). We propose a method for network cross-validation that combined with paired t-tests produces more acceptable levels of Type I error while still providing reasonable levels of statistical power (i.e., Type II error).", :doc-id "Evaluating Statistical Tests for Within-Network Classifiers of Relational Data. 2009 Jennifer Neville, Brian Gallagher, Tina Eliassi-Rad"}, 190192 #search_api.search_api.Paper{:id 190192, :key "conf/esws/KieferBL08", :title "Adding Data Mining Support to SPARQL Via Statistical Relational Learning Methods.", :abstract "Exploiting the complex structure of relational data enables to build better models by taking into account the additional information provided by the links between objects. We extend this idea to the Semantic Web by introducing our novel SPARQL-ML approach to perform data mining for Semantic Web data. Our approach is based on traditional SPARQL and statistical relational learning methods, such as Relational Probability Trees and Relational Bayesian Classifiers. We analyze our approach thoroughly conducting three sets of experiments on synthetic as well as real-world data sets. Our analytical results show that our approach can be used for any Semantic Web data set to perform instance-based learning and classification. A comparison to kernel methods used in Support Vector Machines shows that our approach is superior in terms of classification accuracy.", :author (#search_api.search_api.Author{:id 1506617, :first-name nil, :last-name nil, :full-name "Christoph Kiefer"} #search_api.search_api.Author{:id 798825, :first-name nil, :last-name nil, :full-name "Abraham Bernstein"} #search_api.search_api.Author{:id 322034, :first-name nil, :last-name nil, :full-name "André Locher"}), :year 2008, :venue "ESWC", :ncit 26, :string "Adding Data Mining Support to SPARQL Via Statistical Relational Learning Methods.. Exploiting the complex structure of relational data enables to build better models by taking into account the additional information provided by the links between objects. We extend this idea to the Semantic Web by introducing our novel SPARQL-ML approach to perform data mining for Semantic Web data. Our approach is based on traditional SPARQL and statistical relational learning methods, such as Relational Probability Trees and Relational Bayesian Classifiers. We analyze our approach thoroughly conducting three sets of experiments on synthetic as well as real-world data sets. Our analytical results show that our approach can be used for any Semantic Web data set to perform instance-based learning and classification. A comparison to kernel methods used in Support Vector Machines shows that our approach is superior in terms of classification accuracy.", :doc-id "Adding Data Mining Support to SPARQL Via Statistical Relational Learning Methods. 2008 Christoph Kiefer, Abraham Bernstein, André Locher"}, 1279856 #search_api.search_api.Paper{:id 1279856, :key "journals/neco/AmariM93", :title "Statistical Theory of Learning Curves under Entropic Loss Criterion.", :abstract "The present paper elucidates a universal property of learning curves, which shows how the generalization error, training error, and the complexity of the underlying stochastic machine are related and how the behavior of a stochastic machine is improved as the number of training examples increases. The error is measured by the entropic loss. It is proved that the generalization error converges to H0, the entropy of the conditional distribution of the true machine, as H0 + m*/(2t), while the training error converges as H0-m*/(2t), where t is the number of examples and m* shows the complexity of the network. When the model is faithful, implying that the true machine is in the model, m* is reduced to m, the number of modifiable parameters. This is a universal law because it holds for any regular machine irrespective of its structure under the maximum likelihood estimator. Similar relations are obtained for the Bayes and Gibbs learning algorithms. These learning curves show the relation among the accuracy of learning, the complexity of a model, and the number of training examples.", :author (#search_api.search_api.Author{:id 99365, :first-name nil, :last-name nil, :full-name "Shun-ichi Amari"} #search_api.search_api.Author{:id 918942, :first-name nil, :last-name nil, :full-name "Noboru Murata"}), :year 1993, :venue "Neural Computation", :ncit 143, :string "Statistical Theory of Learning Curves under Entropic Loss Criterion.. The present paper elucidates a universal property of learning curves, which shows how the generalization error, training error, and the complexity of the underlying stochastic machine are related and how the behavior of a stochastic machine is improved as the number of training examples increases. The error is measured by the entropic loss. It is proved that the generalization error converges to H0, the entropy of the conditional distribution of the true machine, as H0 + m*/(2t), while the training error converges as H0-m*/(2t), where t is the number of examples and m* shows the complexity of the network. When the model is faithful, implying that the true machine is in the model, m* is reduced to m, the number of modifiable parameters. This is a universal law because it holds for any regular machine irrespective of its structure under the maximum likelihood estimator. Similar relations are obtained for the Bayes and Gibbs learning algorithms. These learning curves show the relation among the accuracy of learning, the complexity of a model, and the number of training examples.", :doc-id "Statistical Theory of Learning Curves under Entropic Loss Criterion. 1993 Shun-ichi Amari, Noboru Murata"}, 498833 #search_api.search_api.Paper{:id 498833, :key "conf/miccai/UnalNSF08", :title "Customized Design of Hearing Aids Using Statistical Shape Learning.", :abstract "3D shape modeling is a crucial component of rapid prototyping systems that customize shapes of implants and prosthetic devices to a patient's anatomy. In this paper, we present a solution to the problem of customized 3D shape modeling using a statistical shape analysis framework. We design a novel method to learn the relationship between two classes of shapes, which are related by certain operations or transformation. The two associated shape classes are represented in a lower dimensional manifold, and the reduced set of parameters obtained in this subspace is utilized in an estimation, which is exemplified by a multivariate regression in this paper. We demonstrate our method with a felicitous application to estimation of customized hearing aid devices.", :author (#search_api.search_api.Author{:id 987788, :first-name nil, :last-name nil, :full-name "Gozde B. Unal"} #search_api.search_api.Author{:id 1388067, :first-name nil, :last-name nil, :full-name "Delphine Nain"} #search_api.search_api.Author{:id 174234, :first-name nil, :last-name nil, :full-name "Gregory G. Slabaugh"} #search_api.search_api.Author{:id 303194, :first-name nil, :last-name nil, :full-name "Tong Fang"}), :year 2008, :venue "MICCAI (1)", :ncit 6, :string "Customized Design of Hearing Aids Using Statistical Shape Learning.. 3D shape modeling is a crucial component of rapid prototyping systems that customize shapes of implants and prosthetic devices to a patient's anatomy. In this paper, we present a solution to the problem of customized 3D shape modeling using a statistical shape analysis framework. We design a novel method to learn the relationship between two classes of shapes, which are related by certain operations or transformation. The two associated shape classes are represented in a lower dimensional manifold, and the reduced set of parameters obtained in this subspace is utilized in an estimation, which is exemplified by a multivariate regression in this paper. We demonstrate our method with a felicitous application to estimation of customized hearing aid devices.", :doc-id "Customized Design of Hearing Aids Using Statistical Shape Learning. 2008 Gozde B. Unal, Delphine Nain, Gregory G. Slabaugh, Tong Fang"}, 2784531 #search_api.search_api.Paper{:id 2784531, :key "conf/ecai/JainBB10", :title "Adaptive Markov Logic Networks: Learning Statistical Relational Models with Dynamic Parameters.", :abstract "Statistical relational models, such as Markov logic networks, seek to compactly describe properties of relational domains by representing general principles about objects belonging to particular classes. Models are intended to be independent of the set of objects to which these principles can be applied, and it is assumed that the principles will soundly generalize across arbitrary sets of objects. In this paper, we point out limitations of models that seek to represent the corresponding principles with a fixed set of parameters and discuss the conditions under which the soundness of fixed parameters is indeed questionable. We propose a novel representation formalism called adaptive Markov logic networks to allow more flexible representations of relational domains, which involve parameters that are dynamically adjusted to fit the properties of an instantiation by phrasing the model's parameters as functions over attributes of the instantiation at hand. We empirically demonstrate the value of our learning and representation system on a simple but well-motivated example domain. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :author (#search_api.search_api.Author{:id 639688, :first-name nil, :last-name nil, :full-name "Dominik Jain"} #search_api.search_api.Author{:id 1005331, :first-name nil, :last-name nil, :full-name "Andreas Barthels"} #search_api.search_api.Author{:id 361314, :first-name nil, :last-name nil, :full-name "Michael Beetz"}), :year 2010, :venue "ECAI", :ncit 8, :string "Adaptive Markov Logic Networks: Learning Statistical Relational Models with Dynamic Parameters.. Statistical relational models, such as Markov logic networks, seek to compactly describe properties of relational domains by representing general principles about objects belonging to particular classes. Models are intended to be independent of the set of objects to which these principles can be applied, and it is assumed that the principles will soundly generalize across arbitrary sets of objects. In this paper, we point out limitations of models that seek to represent the corresponding principles with a fixed set of parameters and discuss the conditions under which the soundness of fixed parameters is indeed questionable. We propose a novel representation formalism called adaptive Markov logic networks to allow more flexible representations of relational domains, which involve parameters that are dynamically adjusted to fit the properties of an instantiation by phrasing the model's parameters as functions over attributes of the instantiation at hand. We empirically demonstrate the value of our learning and representation system on a simple but well-motivated example domain. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :doc-id "Adaptive Markov Logic Networks: Learning Statistical Relational Models with Dynamic Parameters. 2010 Dominik Jain, Andreas Barthels, Michael Beetz"}, 1009299 #search_api.search_api.Paper{:id 1009299, :key "journals/ml/BeefermanBL99", :title "Statistical Models for Text Segmentation.", :abstract "This paper introduces a new statistical approach to automatically partitioning text into coherent segments. The approach is based on a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text. The models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may be domain-specific, that tend to be used near segment boundaries. Assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, Wall Street Journal news articles and television broadcast news story transcripts. Quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. This metric is used to make a quantitative assessment of the relative contributions of the different feature types, as well as a comparison with decision trees and previously proposed text segmentation algorithms.", :author (#search_api.search_api.Author{:id 1914, :first-name nil, :last-name nil, :full-name "Doug Beeferman"} #search_api.search_api.Author{:id 109818, :first-name nil, :last-name nil, :full-name "Adam L. Berger"} #search_api.search_api.Author{:id 1463112, :first-name nil, :last-name nil, :full-name "John D. Lafferty"}), :year 1999, :venue "Machine Learning", :ncit 488, :string "Statistical Models for Text Segmentation.. This paper introduces a new statistical approach to automatically partitioning text into coherent segments. The approach is based on a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text. The models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may be domain-specific, that tend to be used near segment boundaries. Assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, Wall Street Journal news articles and television broadcast news story transcripts. Quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. This metric is used to make a quantitative assessment of the relative contributions of the different feature types, as well as a comparison with decision trees and previously proposed text segmentation algorithms.", :doc-id "Statistical Models for Text Segmentation. 1999 Doug Beeferman, Adam L. Berger, John D. Lafferty"}, 952340 #search_api.search_api.Paper{:id 952340, :key "journals/jair/Jaeger05", :title "Ignorability in Statistical and Probabilistic Inference.", :abstract "When dealing with incomplete data in statistical learning, or incomplete observations in probabilistic inference, one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened. Since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive, one asks for conditions under which it can be safely ignored. Such conditions are given by the missing at random (mar) and coarsened at random (car) assumptions. In this paper we provide an in-depth analysis of several questions relating to mar/car assumptions. Main purpose of our study is to provide criteria by which one may evaluate whether a car assumption is reasonable for a particular data collecting or observational process. This question is complicated by the fact that several distinct versions of mar/car assumptions exist. We therefore first provide an overview over these different versions, in which we highlight the distinction between distributional and coarsening variable induced versions. We show that distributional versions are less restrictive and sufficient for most applications. We then address from two different perspectives the question of when the mar/car assumption is warranted. First we provide a \"static\" analysis that characterizes the admissibility of the car assumption in terms of the support structure of the joint probability distribution of complete data and incomplete observations. Here we obtain an equivalence characterization that improves and extends a recent result by Grünwald and Halpern. We then turn to a \"procedural\" analysis that characterizes the admissibility of the car assumption in terms of procedural models for the actual data (or observation) generating process. The main result of this analysis is that the stronger coarsened completely at random (ccar) condition is arguably the most reasonable assumption, as it alone corresponds to data coarsening procedures that satisfy a natural robustness property.", :author (#search_api.search_api.Author{:id 252997, :first-name nil, :last-name nil, :full-name "Manfred Jaeger"}), :year 2005, :venue "J. Artif. Intell. Res. (JAIR)", :ncit 7, :string "Ignorability in Statistical and Probabilistic Inference.. When dealing with incomplete data in statistical learning, or incomplete observations in probabilistic inference, one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened. Since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive, one asks for conditions under which it can be safely ignored. Such conditions are given by the missing at random (mar) and coarsened at random (car) assumptions. In this paper we provide an in-depth analysis of several questions relating to mar/car assumptions. Main purpose of our study is to provide criteria by which one may evaluate whether a car assumption is reasonable for a particular data collecting or observational process. This question is complicated by the fact that several distinct versions of mar/car assumptions exist. We therefore first provide an overview over these different versions, in which we highlight the distinction between distributional and coarsening variable induced versions. We show that distributional versions are less restrictive and sufficient for most applications. We then address from two different perspectives the question of when the mar/car assumption is warranted. First we provide a \"static\" analysis that characterizes the admissibility of the car assumption in terms of the support structure of the joint probability distribution of complete data and incomplete observations. Here we obtain an equivalence characterization that improves and extends a recent result by Grünwald and Halpern. We then turn to a \"procedural\" analysis that characterizes the admissibility of the car assumption in terms of procedural models for the actual data (or observation) generating process. The main result of this analysis is that the stronger coarsened completely at random (ccar) condition is arguably the most reasonable assumption, as it alone corresponds to data coarsening procedures that satisfy a natural robustness property.", :doc-id "Ignorability in Statistical and Probabilistic Inference. 2005 Manfred Jaeger"}, 985236 #search_api.search_api.Paper{:id 985236, :key "journals/jmlr/PasseriniFR06", :title "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.", :abstract "We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :author (#search_api.search_api.Author{:id 1520074, :first-name nil, :last-name nil, :full-name "Andrea Passerini"} #search_api.search_api.Author{:id 34394, :first-name nil, :last-name nil, :full-name "Paolo Frasconi"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2006, :venue "Journal of Machine Learning Research", :ncit 31, :string "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.. We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :doc-id "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting. 2006 Andrea Passerini, Paolo Frasconi, Luc De Raedt"}, 1017236 #search_api.search_api.Paper{:id 1017236, :key "journals/mta/HanLL08", :title "Semantic image classification using statistical local spatial relations model.", :abstract "In this paper, a statistical model called statistical local spatial relations (SLSR) is presented as a novel technique of a learning model with spatial and statistical information for semantic image classification. The model is inspired by probabilistic Latent Semantic Analysis (PLSA) for text mining. In text analysis, PLSA is used to discover topics in a corpus using the bag-of-word document representation. In SLSR, we treat image categories as topics, therefore an image containing instances of multiple categories can be modeled as a mixture of topics. More significantly, SLSR introduces spatial relation information as a factor which is not present in PLSA. SLSR has rotation, scale, translation and affine invariant properties and can solve partial occlusion problems. Using the Dirichlet process and variational Expectation-Maximization learning algorithm, SLSR is developed as an implementation of an image classification algorithm. SLSR uses an unsupervised process which can capture both spatial relations and statistical information simultaneously. The experiments are demonstrated on some standard data sets and show that the SLSR model is a promising model for semantic image classification problems.", :author (#search_api.search_api.Author{:id 51919, :first-name nil, :last-name nil, :full-name "Dongfeng Han"} #search_api.search_api.Author{:id 589651, :first-name nil, :last-name nil, :full-name "Wenhui Li"} #search_api.search_api.Author{:id 1381040, :first-name nil, :last-name nil, :full-name "Zongcheng Li"}), :year 2008, :venue "Multimedia Tools Appl.", :ncit 11, :string "Semantic image classification using statistical local spatial relations model.. In this paper, a statistical model called statistical local spatial relations (SLSR) is presented as a novel technique of a learning model with spatial and statistical information for semantic image classification. The model is inspired by probabilistic Latent Semantic Analysis (PLSA) for text mining. In text analysis, PLSA is used to discover topics in a corpus using the bag-of-word document representation. In SLSR, we treat image categories as topics, therefore an image containing instances of multiple categories can be modeled as a mixture of topics. More significantly, SLSR introduces spatial relation information as a factor which is not present in PLSA. SLSR has rotation, scale, translation and affine invariant properties and can solve partial occlusion problems. Using the Dirichlet process and variational Expectation-Maximization learning algorithm, SLSR is developed as an implementation of an image classification algorithm. SLSR uses an unsupervised process which can capture both spatial relations and statistical information simultaneously. The experiments are demonstrated on some standard data sets and show that the SLSR model is a promising model for semantic image classification problems.", :doc-id "Semantic image classification using statistical local spatial relations model. 2008 Dongfeng Han, Wenhui Li, Zongcheng Li"}, 3623508 #search_api.search_api.Paper{:id 3623508, :key "journals/ijsnm/EspositoFBM12", :title "Social networks and statistical relational learning: a survey.", :abstract nil, :author (#search_api.search_api.Author{:id 769268, :first-name nil, :last-name nil, :full-name "Floriana Esposito"} #search_api.search_api.Author{:id 1126564, :first-name nil, :last-name nil, :full-name "Stefano Ferilli"} #search_api.search_api.Author{:id 1078358, :first-name nil, :last-name nil, :full-name "Teresa Maria Altomare Basile"} #search_api.search_api.Author{:id 1294122, :first-name nil, :last-name nil, :full-name "Nicola Di Mauro"}), :year 2012, :venue "IJSNM", :ncit 0, :string "Social networks and statistical relational learning: a survey.. ", :doc-id "Social networks and statistical relational learning: a survey. 2012 Floriana Esposito, Stefano Ferilli, Teresa Maria Altomare Basile, Nicola Di Mauro"}, 1009524 #search_api.search_api.Paper{:id 1009524, :key "journals/ml/HeckermanGC95", :title "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.", :abstract "We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :author (#search_api.search_api.Author{:id 114083, :first-name nil, :last-name nil, :full-name "David Heckerman"} #search_api.search_api.Author{:id 330355, :first-name nil, :last-name nil, :full-name "Dan Geiger"} #search_api.search_api.Author{:id 1215050, :first-name nil, :last-name nil, :full-name "David Maxwell Chickering"}), :year 1995, :venue "Machine Learning", :ncit 3837, :string "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.. We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :doc-id "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data. 1995 David Heckerman, Dan Geiger, David Maxwell Chickering"}, 192565 #search_api.search_api.Paper{:id 192565, :key "conf/eurocolt/ShamirS95", :title "Learning by extended statistical queries and its relation to PAC learning.", :abstract nil, :author (#search_api.search_api.Author{:id 1191060, :first-name nil, :last-name nil, :full-name "Eli Shamir"} #search_api.search_api.Author{:id 690500, :first-name nil, :last-name nil, :full-name "Clara Shwartzman"}), :year 1995, :venue "EuroCOLT", :ncit 8, :string "Learning by extended statistical queries and its relation to PAC learning.. ", :doc-id "Learning by extended statistical queries and its relation to PAC learning. 1995 Eli Shamir, Clara Shwartzman"}, 1236661 #search_api.search_api.Paper{:id 1236661, :key "conf/RelMiCS/Muller09", :title "Modalities, Relations, and Learning.", :abstract "While the popularity of statistical, probabilistic and exhaustive machine learning techniques still increases, relational and logic approaches are still a niche market in research. While the former approaches focus on predictive accuracy, the latter ones prove to be indispensable in knowledge discovery.In this paper we present a relational description of machine learning problems. We demonstrate how common ensemble learning methods as used in classifier learning can be reformulated in a relational setting. It is shown that multimodal logics and relational data analysis with rough sets are closely related. Finally, we give an interpretation of logic programs as approximations of hypotheses.It is demonstrated that at a certain level of abstraction all these methods unify into one and the same formalisation which nicely connects to multimodal operators. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :author (#search_api.search_api.Author{:id 1161107, :first-name nil, :last-name nil, :full-name "Martin Eric Müller"}), :year 2009, :venue "RelMiCS", :ncit 0, :string "Modalities, Relations, and Learning.. While the popularity of statistical, probabilistic and exhaustive machine learning techniques still increases, relational and logic approaches are still a niche market in research. While the former approaches focus on predictive accuracy, the latter ones prove to be indispensable in knowledge discovery.In this paper we present a relational description of machine learning problems. We demonstrate how common ensemble learning methods as used in classifier learning can be reformulated in a relational setting. It is shown that multimodal logics and relational data analysis with rough sets are closely related. Finally, we give an interpretation of logic programs as approximations of hypotheses.It is demonstrated that at a certain level of abstraction all these methods unify into one and the same formalisation which nicely connects to multimodal operators. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :doc-id "Modalities, Relations, and Learning. 2009 Martin Eric Müller"}, 302837 #search_api.search_api.Paper{:id 302837, :key "conf/icdm/PopesculULP03", :title "Statistical Relational Learning for Document Mining.", :abstract "A major obstacle to fully integrated deployment of manydata mining algorithms is the assumption that data sitsin a single table, even though most real-world databaseshave complex relational structures. We propose an integratedapproach to statistical modeling from relationaldatabases. We structure the search space based on \"refinementgraphs\", which are widely used in inductive logic programmingfor learning logic descriptions. The use of statisticsallows us to extend the search space to include richerset of features, including many which are not boolean.Search and model selection are integrated into a single process,allowing information criteria native to the statisticalmodel, for example logistic regression, to make feature selectiondecisions in a step-wise manner. We present experimentalresults for the task of predicting where scientific paperswill be published based on relational data taken fromCiteSeer. Our approach results in classification accuraciessuperior to those achieved when using classical \"flat\" features.The resulting classifier can be used to recommendwhere to publish articles.", :author (#search_api.search_api.Author{:id 597076, :first-name nil, :last-name nil, :full-name "Alexandrin Popescul"} #search_api.search_api.Author{:id 352656, :first-name nil, :last-name nil, :full-name "Lyle H. Ungar"} #search_api.search_api.Author{:id 599072, :first-name nil, :last-name nil, :full-name "Steve Lawrence"} #search_api.search_api.Author{:id 80443, :first-name nil, :last-name nil, :full-name "David M. Pennock"}), :year 2003, :venue "ICDM", :ncit 51, :string "Statistical Relational Learning for Document Mining.. A major obstacle to fully integrated deployment of manydata mining algorithms is the assumption that data sitsin a single table, even though most real-world databaseshave complex relational structures. We propose an integratedapproach to statistical modeling from relationaldatabases. We structure the search space based on \"refinementgraphs\", which are widely used in inductive logic programmingfor learning logic descriptions. The use of statisticsallows us to extend the search space to include richerset of features, including many which are not boolean.Search and model selection are integrated into a single process,allowing information criteria native to the statisticalmodel, for example logistic regression, to make feature selectiondecisions in a step-wise manner. We present experimentalresults for the task of predicting where scientific paperswill be published based on relational data taken fromCiteSeer. Our approach results in classification accuraciessuperior to those achieved when using classical \"flat\" features.The resulting classifier can be used to recommendwhere to publish articles.", :doc-id "Statistical Relational Learning for Document Mining. 2003 Alexandrin Popescul, Lyle H. Ungar, Steve Lawrence, David M. Pennock"}, 1032117 #search_api.search_api.Paper{:id 1032117, :key "journals/pami/KonishiYCZ03", :title "Statistical Edge Detection: Learning and Evaluating Edge Cues.", :abstract "We formulate edge detection as statistical inference. This statistical edge detection is data driven, unlike standard methods for edge detection which are model based. For any set of edge detection filters (implementing local edge cues), we use presegmented images to learn the probability distributions of filter responses conditioned on whether they are evaluated on or off an edge. Edge detection is formulated as a discrimination task specified by a likelihood ratio test on the filter responses. This approach emphasizes the necessity of modeling the image background (the off-edges). We represent the conditional probability distributions nonparametrically and illustrate them on two different data sets of 100 (Sowerby) and 50 (South Florida) images. Multiple edges cues, including chrominance and multiple-scale, are combined by using their joint distributions. Hence, this cue combination is optimal in the statistical sense. We evaluate the effectiveness of different visual cues using the Chernoff information and Receiver Operator Characteristic (ROC) curves. This shows that our approach gives quantitatively better results than the Canny edge detector when the image background contains significant clutter. In addition, it enables us to determine the effectiveness of different edge cues and gives quantitative measures for the advantages of multilevel processing, for the use of chrominance, and for the relative effectiveness of different detectors. Furthermore, we show that we can learn these conditional distributions on one data set and adapt them to the other with only slight degradation of performance without knowing the ground truth on the second data set. This shows that our results are not purely domain specific. We apply the same approach to the spatial grouping of edge cues and obtain analogies to nonmaximal suppression and hysteresis.", :author (#search_api.search_api.Author{:id 696497, :first-name nil, :last-name nil, :full-name "Scott Konishi"} #search_api.search_api.Author{:id 248255, :first-name nil, :last-name nil, :full-name "Alan L. Yuille"} #search_api.search_api.Author{:id 589169, :first-name nil, :last-name nil, :full-name "James M. Coughlan"} #search_api.search_api.Author{:id 589996, :first-name nil, :last-name nil, :full-name "Song Chun Zhu"}), :year 2003, :venue "IEEE Trans. Pattern Anal. Mach. Intell.", :ncit 187, :string "Statistical Edge Detection: Learning and Evaluating Edge Cues.. We formulate edge detection as statistical inference. This statistical edge detection is data driven, unlike standard methods for edge detection which are model based. For any set of edge detection filters (implementing local edge cues), we use presegmented images to learn the probability distributions of filter responses conditioned on whether they are evaluated on or off an edge. Edge detection is formulated as a discrimination task specified by a likelihood ratio test on the filter responses. This approach emphasizes the necessity of modeling the image background (the off-edges). We represent the conditional probability distributions nonparametrically and illustrate them on two different data sets of 100 (Sowerby) and 50 (South Florida) images. Multiple edges cues, including chrominance and multiple-scale, are combined by using their joint distributions. Hence, this cue combination is optimal in the statistical sense. We evaluate the effectiveness of different visual cues using the Chernoff information and Receiver Operator Characteristic (ROC) curves. This shows that our approach gives quantitatively better results than the Canny edge detector when the image background contains significant clutter. In addition, it enables us to determine the effectiveness of different edge cues and gives quantitative measures for the advantages of multilevel processing, for the use of chrominance, and for the relative effectiveness of different detectors. Furthermore, we show that we can learn these conditional distributions on one data set and adapt them to the other with only slight degradation of performance without knowing the ground truth on the second data set. This shows that our results are not purely domain specific. We apply the same approach to the spatial grouping of edge cues and obtain analogies to nonmaximal suppression and hysteresis.", :doc-id "Statistical Edge Detection: Learning and Evaluating Edge Cues. 2003 Scott Konishi, Alan L. Yuille, James M. Coughlan, Song Chun Zhu"}, 574389 #search_api.search_api.Paper{:id 574389, :key "conf/sbia/Raedt08a", :title "Logic, Probability and Learning, or an Introduction to Statistical Relational Learning.", :abstract "Probabilistic inductive logic programming (PILP), sometimes also called statistical relational learning, addresses one of the central questions of artificial intelligence: the integration of probabilistic reasoning with first order logic representations and machine learning. A rich variety of different formalisms and learning techniques have been developed and they are being applied on applications in network analysis, robotics, bio-informatics, intelligent agents, etc. This tutorial starts with an introduction to probabilistic representations and machine learning, and then continues with an overview of the state-of-the-art in statistical relational learning. We start from classical settings for logic learning (or inductive logic programming) namely learning from entailment, learning from interpretations, and learning from proofs, and show how they can be extended with probabilistic methods. While doing so, we review state-of-the-art statistical relational learning approaches and show how they fit the discussed learning settings for probabilistic inductive logic programming.", :author (#search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2008, :venue "SBIA", :ncit 0, :string "Logic, Probability and Learning, or an Introduction to Statistical Relational Learning.. Probabilistic inductive logic programming (PILP), sometimes also called statistical relational learning, addresses one of the central questions of artificial intelligence: the integration of probabilistic reasoning with first order logic representations and machine learning. A rich variety of different formalisms and learning techniques have been developed and they are being applied on applications in network analysis, robotics, bio-informatics, intelligent agents, etc. This tutorial starts with an introduction to probabilistic representations and machine learning, and then continues with an overview of the state-of-the-art in statistical relational learning. We start from classical settings for logic learning (or inductive logic programming) namely learning from entailment, learning from interpretations, and learning from proofs, and show how they can be extended with probabilistic methods. While doing so, we review state-of-the-art statistical relational learning approaches and show how they fit the discussed learning settings for probabilistic inductive logic programming.", :doc-id "Logic, Probability and Learning, or an Introduction to Statistical Relational Learning. 2008 Luc De Raedt"}, 57463 #search_api.search_api.Paper{:id 57463, :key "conf/atal/RettingerNT08", :title "A statistical relational model for trust learning.", :abstract "We address the learning of trust based on past observations and context information. We argue that from the truster's point of view trust is best expressed as one of several relations that exist between the agent to be trusted (trustee) and the state of the environment. Besides attributes expressing trustworthiness, additional relations might describe commitments made by the trustee with regard to the current situation, like: a seller offers a certain price for a specific product. We show how to implement and learn contextsensitive trust using statistical relational learning in form of the Infinite Hidden Relational Trust Model (IHRTM). The practicability and effectiveness of our approach is evaluated empirically on user-ratings gathered from eBay. Our results suggest that (i) the inherent clustering achieved in the algorithm allows the truster to characterize the structure of a trust-situation and provides meaningful trust assessments; (ii) utilizing the collaborative filtering effect associated with relational data does improve trust assessment performance; (iii) by learning faster and transferring knowledge more effectively we improve cold start performance and can cope better with dynamic behavior in open multiagent systems. The later is demonstrated with interactions recorded from a strategic two-player negotiation scenario.", :author (#search_api.search_api.Author{:id 444383, :first-name nil, :last-name nil, :full-name "Achim Rettinger"} #search_api.search_api.Author{:id 222492, :first-name nil, :last-name nil, :full-name "Matthias Nickles"} #search_api.search_api.Author{:id 680469, :first-name nil, :last-name nil, :full-name "Volker Tresp"}), :year 2008, :venue "AAMAS (2)", :ncit 14, :string "A statistical relational model for trust learning.. We address the learning of trust based on past observations and context information. We argue that from the truster's point of view trust is best expressed as one of several relations that exist between the agent to be trusted (trustee) and the state of the environment. Besides attributes expressing trustworthiness, additional relations might describe commitments made by the trustee with regard to the current situation, like: a seller offers a certain price for a specific product. We show how to implement and learn contextsensitive trust using statistical relational learning in form of the Infinite Hidden Relational Trust Model (IHRTM). The practicability and effectiveness of our approach is evaluated empirically on user-ratings gathered from eBay. Our results suggest that (i) the inherent clustering achieved in the algorithm allows the truster to characterize the structure of a trust-situation and provides meaningful trust assessments; (ii) utilizing the collaborative filtering effect associated with relational data does improve trust assessment performance; (iii) by learning faster and transferring knowledge more effectively we improve cold start performance and can cope better with dynamic behavior in open multiagent systems. The later is demonstrated with interactions recorded from a strategic two-player negotiation scenario.", :doc-id "A statistical relational model for trust learning. 2008 Achim Rettinger, Matthias Nickles, Volker Tresp"}, 746711 #search_api.search_api.Paper{:id 746711, :key "journals/aicom/Kersting06", :title "An inductive logic programming approach to statistical relational learning.", :abstract "It has been a great pleasure to be asked to write the preface for the book based on Kristian Kersting's thesis. There is no doubt in my mind that this is a remarkable and outstanding piece of work. In his thesis Kristian has made an assault on one of the hardest integration problems at the heart of Artificial Intelligence research. This involves taking three disparate major areas of research and attempting a fusion among them. The three areas are: Logic Programming, Uncertainty Reasoning and Machine Learning. Every one of these is a major sub-area of research with its own associated international research conferences. Having taken on such a Herculean task, Kristian has produced a series of widely published results which are now at the core of a newly emerging area: Probabilistic Inductive Logic Programming. The new area is closely tied to, though strictly subsumes, a new field known as “Statistical Relational Learning” which has in the last few years gained major prominence in the American Artificial Intelligence research community. Within his thesis Kristian makes several major contributions, many of which have already been published in refereed conference and journal papers. Firstly, Kristian introduces a series of definitions which circumscribe the new area formed by extending Inductive Logic Programming to the case in which clauses are annotated with probability values. This represents a new and powerful framework which supersedes a number of influential papers and research areas in Artificial Intelligence. Secondly, Kristian introduces Bayesian Logic Programs (BLPs). These represent an elegantly defined lifting of Judea Pearl's Bayesian networks to the logic programming level. Since Kristian's introduction of BLPs, a number of results indicate that BLPs generalise many previously defined representations, not the least of which are Bayesian networks, Logic Programs, Probabilistic Relation Models and Stochastic Logic Programs. Next Kristian investigates the approach of Learning from proofs. This is an interesting new learning framework which is the first to go beyond the two standard semantic frameworks of Inductive Logic Programming. Kristian then looks at the problem of upgrading HMMs to logical HMMs. Hidden Markov Models (HMMs) are one of the most widely used machine learning technologies in Statistical Linguistics and Bioinformatics, and allow the representation of probabilistic finite automata. Kristian has upgraded standard HMMs to allow relational descriptions to be included within the description of the automata. The three standard HMM estimation algorithms are also upgraded. He has demonstrated the power of such representations using biological predictive modelling problems, and shown performance increases over alternative approaches. Kristian next considers the issue of upgrading Fisher Kernels to Relational Fisher kernels. Fisher kernels have been widely used within statistics and more recently in support vector machines. Buildin...", :author (#search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"}), :year 2006, :venue "AI Commun.", :ncit 15, :string "An inductive logic programming approach to statistical relational learning.. It has been a great pleasure to be asked to write the preface for the book based on Kristian Kersting's thesis. There is no doubt in my mind that this is a remarkable and outstanding piece of work. In his thesis Kristian has made an assault on one of the hardest integration problems at the heart of Artificial Intelligence research. This involves taking three disparate major areas of research and attempting a fusion among them. The three areas are: Logic Programming, Uncertainty Reasoning and Machine Learning. Every one of these is a major sub-area of research with its own associated international research conferences. Having taken on such a Herculean task, Kristian has produced a series of widely published results which are now at the core of a newly emerging area: Probabilistic Inductive Logic Programming. The new area is closely tied to, though strictly subsumes, a new field known as “Statistical Relational Learning” which has in the last few years gained major prominence in the American Artificial Intelligence research community. Within his thesis Kristian makes several major contributions, many of which have already been published in refereed conference and journal papers. Firstly, Kristian introduces a series of definitions which circumscribe the new area formed by extending Inductive Logic Programming to the case in which clauses are annotated with probability values. This represents a new and powerful framework which supersedes a number of influential papers and research areas in Artificial Intelligence. Secondly, Kristian introduces Bayesian Logic Programs (BLPs). These represent an elegantly defined lifting of Judea Pearl's Bayesian networks to the logic programming level. Since Kristian's introduction of BLPs, a number of results indicate that BLPs generalise many previously defined representations, not the least of which are Bayesian networks, Logic Programs, Probabilistic Relation Models and Stochastic Logic Programs. Next Kristian investigates the approach of Learning from proofs. This is an interesting new learning framework which is the first to go beyond the two standard semantic frameworks of Inductive Logic Programming. Kristian then looks at the problem of upgrading HMMs to logical HMMs. Hidden Markov Models (HMMs) are one of the most widely used machine learning technologies in Statistical Linguistics and Bioinformatics, and allow the representation of probabilistic finite automata. Kristian has upgraded standard HMMs to allow relational descriptions to be included within the description of the automata. The three standard HMM estimation algorithms are also upgraded. He has demonstrated the power of such representations using biological predictive modelling problems, and shown performance increases over alternative approaches. Kristian next considers the issue of upgrading Fisher Kernels to Relational Fisher kernels. Fisher kernels have been widely used within statistics and more recently in support vector machines. Buildin...", :doc-id "An inductive logic programming approach to statistical relational learning. 2006 Kristian Kersting"}, 388504 #search_api.search_api.Paper{:id 388504, :key "conf/ijcai/FriedmanGKP99", :title "Learning Probabilistic Relational Models.", :abstract "A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with \"flat\" data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases.", :author (#search_api.search_api.Author{:id 61431, :first-name nil, :last-name nil, :full-name "Nir Friedman"} #search_api.search_api.Author{:id 1449720, :first-name nil, :last-name nil, :full-name "Lise Getoor"} #search_api.search_api.Author{:id 560995, :first-name nil, :last-name nil, :full-name "Daphne Koller"} #search_api.search_api.Author{:id 154856, :first-name nil, :last-name nil, :full-name "Avi Pfeffer"}), :year 1999, :venue "IJCAI", :ncit 843, :string "Learning Probabilistic Relational Models.. A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with \"flat\" data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases.", :doc-id "Learning Probabilistic Relational Models. 1999 Nir Friedman, Lise Getoor, Daphne Koller, Avi Pfeffer"}, 644121 #search_api.search_api.Paper{:id 644121, :key "conf/vldb/ZhangHJLZ05", :title "Statistical Learning Techniques for Costing XML Queries.", :abstract "Developing cost models for query optimization is significantly harder for XML queries than for traditional relational queries. The reason is that XML query operators are much more complex than relational operators such as table scans and joins. In this paper, we propose a new approach, called COMET, to modeling the cost of XML operators; to our knowledge, COMET is the first method ever proposed for addressing the XML query costing problem. As in relational cost estimation, COMET exploits a set of system catalog statistics that summarizes the XML data; the set of \"simple path\" statistics that we propose is new, and is well suited to the XML setting. Unlike the traditional approach, COMET uses a new statistical learning technique called \"transform regression\" instead of detailed analytical models to predict the overall cost. Besides rendering the cost estimation problem tractable for XML queries, COMET has the further advantage of enabling the query optimizer to be self-tuning, automatically adapting to changes over time in the query workload and in the system environment. We demonstrate COMET's feasibility by developing a cost model for the recently proposed XNAV navigational operator. Empirical studies with synthetic, benchmark, and real-world data sets show that COMET can quickly obtain accurate cost estimates for a variety of XML queries and data sets.", :author (#search_api.search_api.Author{:id 39154, :first-name nil, :last-name nil, :full-name "Ning Zhang"} #search_api.search_api.Author{:id 832799, :first-name nil, :last-name nil, :full-name "Peter J. Haas"} #search_api.search_api.Author{:id 875466, :first-name nil, :last-name nil, :full-name "Vanja Josifovski"} #search_api.search_api.Author{:id 119924, :first-name nil, :last-name nil, :full-name "Guy M. Lohman"} #search_api.search_api.Author{:id 691076, :first-name nil, :last-name nil, :full-name "Chun Zhang"}), :year 2005, :venue "VLDB", :ncit 56, :string "Statistical Learning Techniques for Costing XML Queries.. Developing cost models for query optimization is significantly harder for XML queries than for traditional relational queries. The reason is that XML query operators are much more complex than relational operators such as table scans and joins. In this paper, we propose a new approach, called COMET, to modeling the cost of XML operators; to our knowledge, COMET is the first method ever proposed for addressing the XML query costing problem. As in relational cost estimation, COMET exploits a set of system catalog statistics that summarizes the XML data; the set of \"simple path\" statistics that we propose is new, and is well suited to the XML setting. Unlike the traditional approach, COMET uses a new statistical learning technique called \"transform regression\" instead of detailed analytical models to predict the overall cost. Besides rendering the cost estimation problem tractable for XML queries, COMET has the further advantage of enabling the query optimizer to be self-tuning, automatically adapting to changes over time in the query workload and in the system environment. We demonstrate COMET's feasibility by developing a cost model for the recently proposed XNAV navigational operator. Empirical studies with synthetic, benchmark, and real-world data sets show that COMET can quickly obtain accurate cost estimates for a variety of XML queries and data sets.", :doc-id "Statistical Learning Techniques for Costing XML Queries. 2005 Ning Zhang, Peter J. Haas, Vanja Josifovski, Guy M. Lohman, Chun Zhang"}, 1009754 #search_api.search_api.Paper{:id 1009754, :key "journals/ml/PerlichP06", :title "Distribution-based aggregation for relational learning with identifier attributes.", :abstract "Identifier attributes--very high-dimensional categorical attributes such as particular product ids or people's names--rarely are incorporated in statistical modeling. However, they can play an important role in relational modeling: it may be informative to have communicated with a particular set of people or to have purchased a particular set of products. A key limitation of existing relational modeling techniques is how they aggregate bags (multisets) of values from related entities. The aggregations used by existing methods are simple summaries of the distributions of features of related entities: e.g., MEAN, MODE, SUM, or COUNT. This paper's main contribution is the introduction of aggregation operators that capture more information about the value distributions, by storing meta-data about value distributions and referencing this meta-data when aggregating--for example by computing class-conditional distributional distances. Such aggregations are particularly important for aggregating values from high-dimensional categorical attributes, for which the simple aggregates provide little information. In the first half of the paper we provide general guidelines for designing aggregation operators, introduce the new aggregators in the context of the relational learning system ACORA (Automated Construction of Relational Attributes), and provide theoretical justification. We also conjecture special properties of identifier attributes, e.g., they proxy for unobserved attributes and for information deeper in the relationship network. In the second half of the paper we provide extensive empirical evidence that the distribution-based aggregators indeed do facilitate modeling with high-dimensional categorical attributes, and in support of the aforementioned conjectures.", :author (#search_api.search_api.Author{:id 445653, :first-name nil, :last-name nil, :full-name "Claudia Perlich"} #search_api.search_api.Author{:id 976323, :first-name nil, :last-name nil, :full-name "Foster J. Provost"}), :year 2006, :venue "Machine Learning", :ncit 54, :string "Distribution-based aggregation for relational learning with identifier attributes.. Identifier attributes--very high-dimensional categorical attributes such as particular product ids or people's names--rarely are incorporated in statistical modeling. However, they can play an important role in relational modeling: it may be informative to have communicated with a particular set of people or to have purchased a particular set of products. A key limitation of existing relational modeling techniques is how they aggregate bags (multisets) of values from related entities. The aggregations used by existing methods are simple summaries of the distributions of features of related entities: e.g., MEAN, MODE, SUM, or COUNT. This paper's main contribution is the introduction of aggregation operators that capture more information about the value distributions, by storing meta-data about value distributions and referencing this meta-data when aggregating--for example by computing class-conditional distributional distances. Such aggregations are particularly important for aggregating values from high-dimensional categorical attributes, for which the simple aggregates provide little information. In the first half of the paper we provide general guidelines for designing aggregation operators, introduce the new aggregators in the context of the relational learning system ACORA (Automated Construction of Relational Attributes), and provide theoretical justification. We also conjecture special properties of identifier attributes, e.g., they proxy for unobserved attributes and for information deeper in the relationship network. In the second half of the paper we provide extensive empirical evidence that the distribution-based aggregators indeed do facilitate modeling with high-dimensional categorical attributes, and in support of the aforementioned conjectures.", :doc-id "Distribution-based aggregation for relational learning with identifier attributes. 2006 Claudia Perlich, Foster J. Provost"}, 1250938 #search_api.search_api.Paper{:id 1250938, :key "conf/ismis/BibaFE09", :title "Efficient MAP Inference for Statistical Relational Models through Hybrid Metaheuristics.", :abstract "Statistical Relational Models are state-of-the-art representation formalisms at the intersection of logical and statistical machine learning. One of the most promising models is Markov Logic (ML) which combines Markov networks (MNs) and first-order logic by attaching weights to first-order formulas and using these as templates for features of MNs. MAP inference in ML is the task of finding the most likely state of a set of output variables given the state of the input variables and this problem is NP-hard. In this paper we present an algorithm for this inference task based on the Iterated Local Search (ILS) and Robust Tabu Search (RoTS) metaheuristics. The algorithm performs a biased sampling of the set of local optima by using RoTS as a local search procedure and repetitively jumping in the search space through a perturbation operator, focusing the search not on the full space of solutions but on a smaller subspace defined by the solutions that are locally optimal for the optimization engine. We show through extensive experiments in real-world domains that it improves over the state-of-the-art algorithm in terms of solution quality and inference time.", :author (#search_api.search_api.Author{:id 1482903, :first-name nil, :last-name nil, :full-name "Marenglen Biba"} #search_api.search_api.Author{:id 1126564, :first-name nil, :last-name nil, :full-name "Stefano Ferilli"} #search_api.search_api.Author{:id 769268, :first-name nil, :last-name nil, :full-name "Floriana Esposito"}), :year 2009, :venue "ISMIS", :ncit 0, :string "Efficient MAP Inference for Statistical Relational Models through Hybrid Metaheuristics.. Statistical Relational Models are state-of-the-art representation formalisms at the intersection of logical and statistical machine learning. One of the most promising models is Markov Logic (ML) which combines Markov networks (MNs) and first-order logic by attaching weights to first-order formulas and using these as templates for features of MNs. MAP inference in ML is the task of finding the most likely state of a set of output variables given the state of the input variables and this problem is NP-hard. In this paper we present an algorithm for this inference task based on the Iterated Local Search (ILS) and Robust Tabu Search (RoTS) metaheuristics. The algorithm performs a biased sampling of the set of local optima by using RoTS as a local search procedure and repetitively jumping in the search space through a perturbation operator, focusing the search not on the full space of solutions but on a smaller subspace defined by the solutions that are locally optimal for the optimization engine. We show through extensive experiments in real-world domains that it improves over the state-of-the-art algorithm in terms of solution quality and inference time.", :doc-id "Efficient MAP Inference for Statistical Relational Models through Hybrid Metaheuristics. 2009 Marenglen Biba, Stefano Ferilli, Floriana Esposito"}, 294586 #search_api.search_api.Paper{:id 294586, :key "conf/icdar/CeciBM05", :title "Relational Learning techniques for Document Image Understanding: Comparing Statistical and Logical approaches.", :abstract "In this paper, we evaluate and systematically compare two different (multi-)relational learning methods based on a statistical approach and a logical approach for the task of document image understanding. For a fair comparison, both methods are tested on the same real world dataset consisting of multipage articles published in an international journal. An analysis of pros and cons of both approaches is reported.", :author (#search_api.search_api.Author{:id 1294148, :first-name nil, :last-name nil, :full-name "Michelangelo Ceci"} #search_api.search_api.Author{:id 1385113, :first-name nil, :last-name nil, :full-name "Margherita Berardi"} #search_api.search_api.Author{:id 282116, :first-name nil, :last-name nil, :full-name "Donato Malerba"}), :year 2005, :venue "ICDAR", :ncit 3, :string "Relational Learning techniques for Document Image Understanding: Comparing Statistical and Logical approaches.. In this paper, we evaluate and systematically compare two different (multi-)relational learning methods based on a statistical approach and a logical approach for the task of document image understanding. For a fair comparison, both methods are tested on the same real world dataset consisting of multipage articles published in an international journal. An analysis of pros and cons of both approaches is reported.", :doc-id "Relational Learning techniques for Document Image Understanding: Comparing Statistical and Logical approaches. 2005 Michelangelo Ceci, Margherita Berardi, Donato Malerba"}, 1026778 #search_api.search_api.Paper{:id 1026778, :key "journals/nn/Linsker05", :title "Improved local learning rule for information maximization and related applications.", :abstract "For a neural network comprising feedforward and lateral connections, a local learning rule is proposed that causes the lateral connections to learn directly the inverse of a covariance matrix. In contrast to earlier work, the rule involves just one processing pass through the lateral connections for each input presentation, and consists of a simple anti-Hebbian term. This provides an effective and simple method for online network learning algorithms that implement optimization principles, drawn from statistics or from information or control theory, for which a running estimate of the covariance matrix inverse is useful. An application to infomax learning (mutual information maximization) in the presence of input and output noise is used to illustrate the method.", :author (#search_api.search_api.Author{:id 321062, :first-name nil, :last-name nil, :full-name "Ralph Linsker"}), :year 2005, :venue "Neural Networks", :ncit 19, :string "Improved local learning rule for information maximization and related applications.. For a neural network comprising feedforward and lateral connections, a local learning rule is proposed that causes the lateral connections to learn directly the inverse of a covariance matrix. In contrast to earlier work, the rule involves just one processing pass through the lateral connections for each input presentation, and consists of a simple anti-Hebbian term. This provides an effective and simple method for online network learning algorithms that implement optimization principles, drawn from statistics or from information or control theory, for which a running estimate of the covariance matrix inverse is useful. An application to infomax learning (mutual information maximization) in the presence of input and output noise is used to illustrate the method.", :doc-id "Improved local learning rule for information maximization and related applications. 2005 Ralph Linsker"}, 3341018 #search_api.search_api.Paper{:id 3341018, :key "phd/de/Kersting2006", :title "An inductive logic programming approach to statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"}), :year 2006, :venue nil, :ncit 15, :string "An inductive logic programming approach to statistical relational learning.. ", :doc-id "An inductive logic programming approach to statistical relational learning. 2006 Kristian Kersting"}, 3402586 #search_api.search_api.Paper{:id 3402586, :key "series/faia/2005-148", :title "An Inductive Logic Programming Approach to Statistical Relational Learning", :abstract nil, :author (#search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"}), :year 2005, :venue "Frontiers in Artificial Intelligence and Applications", :ncit 15, :string "An Inductive Logic Programming Approach to Statistical Relational Learning. ", :doc-id "An Inductive Logic Programming Approach to Statistical Relational Learning 2005 Kristian Kersting"}, 395451 #search_api.search_api.Paper{:id 395451, :key "conf/ilp/SaittaV08", :title "A Comparison between Two Statistical Relational Models.", :abstract "Statistical Relational Learning has received much attention this last decade. In the ILP community, several models have emerged for modelling and learning uncertain knowledge, expressed in subset of first order logics. Nevertheless, no deep comparisons have been made among them and, given an application, determining which model must be chosen is difficult. In this paper, we compare two of them, namely Markov Logic Networks and Bayesian Programs, especially with respect to their representation ability and inference methods. The comparison shows that the two models are substantially different, from the point of view of the user, so that choosing one really means choosing a different philosophy to look at the problem. In order to make the comparison more concrete, we have used a running example, which shows most of the interesting points of the approaches, yet remaining exactly tractable.", :author (#search_api.search_api.Author{:id 176408, :first-name nil, :last-name nil, :full-name "Lorenza Saitta"} #search_api.search_api.Author{:id 99042, :first-name nil, :last-name nil, :full-name "Christel Vrain"}), :year 2008, :venue "ILP", :ncit 1, :string "A Comparison between Two Statistical Relational Models.. Statistical Relational Learning has received much attention this last decade. In the ILP community, several models have emerged for modelling and learning uncertain knowledge, expressed in subset of first order logics. Nevertheless, no deep comparisons have been made among them and, given an application, determining which model must be chosen is difficult. In this paper, we compare two of them, namely Markov Logic Networks and Bayesian Programs, especially with respect to their representation ability and inference methods. The comparison shows that the two models are substantially different, from the point of view of the user, so that choosing one really means choosing a different philosophy to look at the problem. In order to make the comparison more concrete, we have used a running example, which shows most of the interesting points of the approaches, yet remaining exactly tractable.", :doc-id "A Comparison between Two Statistical Relational Models. 2008 Lorenza Saitta, Christel Vrain"}, 937563 #search_api.search_api.Paper{:id 937563, :key "journals/ir/Yang99", :title "An Evaluation of Statistical Approaches to Text Categorization.", :abstract "This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance&semi; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.", :author (#search_api.search_api.Author{:id 1066064, :first-name nil, :last-name nil, :full-name "Yiming Yang"}), :year 1999, :venue "Inf. Retr.", :ncit 1871, :string "An Evaluation of Statistical Approaches to Text Categorization.. This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance&semi; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.", :doc-id "An Evaluation of Statistical Approaches to Text Categorization. 1999 Yiming Yang"}, 391548 #search_api.search_api.Paper{:id 391548, :key "conf/ijcai/DavisOSBPC07", :title "Change of Representation for Statistical Relational Learning.", :abstract "Statistical relational learning (SRL) algorithms learn statistical models from relational data, such as that stored in a relational database. We previously introduced view learning for SRL, in which the view of a relational database can be automatically modified, yielding more accurate statistical models. The present paper presents SAYU-VISTA, an algorithm which advances beyond the initial view learning approach in three ways. First, it learns views that introduce new relational tables, rather than merely new fields for an existing table of the database. Second, new tables or new fields are not limited to being approximations to some target concept; instead, the new approach performs a type of predicate invention. The new approach avoids the classical problem with predicate invention, of learning many useless predicates, by keeping only new fields or tables (i.e., new predicates) that immediately improve the performance of the statistical model. Third, retained fields or tables can then be used in the definitions of further new fields or tables. We evaluate the new view learning approach on three relational classification tasks.", :author (#search_api.search_api.Author{:id 747351, :first-name nil, :last-name nil, :full-name "Jesse Davis"} #search_api.search_api.Author{:id 629180, :first-name nil, :last-name nil, :full-name "Irene M. Ong"} #search_api.search_api.Author{:id 1227127, :first-name nil, :last-name nil, :full-name "Jan Struyf"} #search_api.search_api.Author{:id 750446, :first-name nil, :last-name nil, :full-name "Elizabeth S. Burnside"} #search_api.search_api.Author{:id 138183, :first-name nil, :last-name nil, :full-name "David Page"} #search_api.search_api.Author{:id 1352436, :first-name nil, :last-name nil, :full-name "Vítor Santos Costa"}), :year 2007, :venue "IJCAI", :ncit 28, :string "Change of Representation for Statistical Relational Learning.. Statistical relational learning (SRL) algorithms learn statistical models from relational data, such as that stored in a relational database. We previously introduced view learning for SRL, in which the view of a relational database can be automatically modified, yielding more accurate statistical models. The present paper presents SAYU-VISTA, an algorithm which advances beyond the initial view learning approach in three ways. First, it learns views that introduce new relational tables, rather than merely new fields for an existing table of the database. Second, new tables or new fields are not limited to being approximations to some target concept; instead, the new approach performs a type of predicate invention. The new approach avoids the classical problem with predicate invention, of learning many useless predicates, by keeping only new fields or tables (i.e., new predicates) that immediately improve the performance of the statistical model. Third, retained fields or tables can then be used in the definitions of further new fields or tables. We evaluate the new view learning approach on three relational classification tasks.", :doc-id "Change of Representation for Statistical Relational Learning. 2007 Jesse Davis, Irene M. Ong, Jan Struyf, Elizabeth S. Burnside, David Page, Vítor Santos Costa"}, 271964 #search_api.search_api.Paper{:id 271964, :key "conf/icann/KopeczM97", :title "Relative Time Scales in the Self-Organization of Pattern Classification: From One-Shot to Statistical Learning.", :abstract nil, :author (#search_api.search_api.Author{:id 840288, :first-name nil, :last-name nil, :full-name "Klaus Kopecz"} #search_api.search_api.Author{:id 379366, :first-name nil, :last-name nil, :full-name "Karim Mohraz"}), :year 1997, :venue "ICANN", :ncit 0, :string "Relative Time Scales in the Self-Organization of Pattern Classification: From One-Shot to Statistical Learning.. ", :doc-id "Relative Time Scales in the Self-Organization of Pattern Classification: From One-Shot to Statistical Learning. 1997 Klaus Kopecz, Karim Mohraz"}, 1009341 #search_api.search_api.Paper{:id 1009341, :key "journals/ml/BlockeelJK06", :title "Introduction to the special issue on multi-relational data mining and statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 882341, :first-name nil, :last-name nil, :full-name "Hendrik Blockeel"} #search_api.search_api.Author{:id 408384, :first-name nil, :last-name nil, :full-name "David Jensen"} #search_api.search_api.Author{:id 954022, :first-name nil, :last-name nil, :full-name "Stefan Kramer"}), :year 2006, :venue "Machine Learning", :ncit 2, :string "Introduction to the special issue on multi-relational data mining and statistical relational learning.. ", :doc-id "Introduction to the special issue on multi-relational data mining and statistical relational learning. 2006 Hendrik Blockeel, David Jensen, Stefan Kramer"}, 622461 #search_api.search_api.Paper{:id 622461, :key "conf/stoc/Kearns93", :title "Efficient noise-tolerant learning from statistical queries.", :abstract "In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :author (#search_api.search_api.Author{:id 1249307, :first-name nil, :last-name nil, :full-name "Michael J. Kearns"}), :year 1993, :venue "STOC", :ncit 437, :string "Efficient noise-tolerant learning from statistical queries.. In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :doc-id "Efficient noise-tolerant learning from statistical queries. 1993 Michael J. Kearns"}, 1254526 #search_api.search_api.Paper{:id 1254526, :key "conf/pkdd/RettingerNT09", :title "Statistical Relational Learning with Formal Ontologies.", :abstract "We propose a learning approach for integrating formal knowledge into statistical inference by exploiting ontologies as a semantically rich and fully formal representation of prior knowledge. The logical constraints deduced from ontologies can be utilized to enhance and control the learning task by enforcing description logic satisfiability in a latent multi-relational graphical model. To demonstrate the feasibility of our approach we provide experiments using real world social network data in form of a $\\mathcal{SHOIN}(D)$ ontology. The results illustrate two main practical advancements: First, entities and entity relationships can be analyzed via the latent model structure. Second, enforcing the ontological constraints guarantees that the learned model does not predict inconsistent relations. In our experiments, this leads to an improved predictive performance.", :author (#search_api.search_api.Author{:id 444383, :first-name nil, :last-name nil, :full-name "Achim Rettinger"} #search_api.search_api.Author{:id 222492, :first-name nil, :last-name nil, :full-name "Matthias Nickles"} #search_api.search_api.Author{:id 680469, :first-name nil, :last-name nil, :full-name "Volker Tresp"}), :year 2009, :venue "ECML/PKDD (2)", :ncit 13, :string "Statistical Relational Learning with Formal Ontologies.. We propose a learning approach for integrating formal knowledge into statistical inference by exploiting ontologies as a semantically rich and fully formal representation of prior knowledge. The logical constraints deduced from ontologies can be utilized to enhance and control the learning task by enforcing description logic satisfiability in a latent multi-relational graphical model. To demonstrate the feasibility of our approach we provide experiments using real world social network data in form of a $\\mathcal{SHOIN}(D)$ ontology. The results illustrate two main practical advancements: First, entities and entity relationships can be analyzed via the latent model structure. Second, enforcing the ontological constraints guarantees that the learned model does not predict inconsistent relations. In our experiments, this leads to an improved predictive performance.", :doc-id "Statistical Relational Learning with Formal Ontologies. 2009 Achim Rettinger, Matthias Nickles, Volker Tresp"}, 3782751 #search_api.search_api.Paper{:id 3782751, :key "conf/ilp/SahaSR12", :title "What Kinds of Relational Features Are Useful for Statistical Learning?", :abstract nil, :author (#search_api.search_api.Author{:id 914315, :first-name nil, :last-name nil, :full-name "Amrita Saha"} #search_api.search_api.Author{:id 1118213, :first-name nil, :last-name nil, :full-name "Ashwin Srinivasan"} #search_api.search_api.Author{:id 376403, :first-name nil, :last-name nil, :full-name "Ganesh Ramakrishnan"}), :year 2012, :venue "ILP", :ncit 0, :string "What Kinds of Relational Features Are Useful for Statistical Learning?. ", :doc-id "What Kinds of Relational Features Are Useful for Statistical Learning? 2012 Amrita Saha, Ashwin Srinivasan, Ganesh Ramakrishnan"}}}}, :json #utils.core.Success{:value {:topic-titles {3424576 #{190192}, 797088 #{294586}, 470752 #{1009537 303622 952368}, 4094720 #{1009537}, 331680 #{1032117}, 336897 #{950858 622461}, 871681 #{952368}, 22532673 #{1288331}, 2883137 #{1280035}, 17306305 #{1009230 1017236}, 1911810 #{1009299}, 8330403 #{1017236}, 160995 #{1009218}, 1028771 #{1026778}, 22691076 #{1254526}, 8050180 #{57463}, 31176997 #{675364 190192 746711}, 18462661 #{473123}, 22589574 #{190192}, 24104134 #{1032117}, 30876902 #{1319504}, 140806 #{1280035 1279856}, 19667111 #{687201 3074945 336067 388102 303622 3258022 746727 2927016 2927018 3258026 3074924 473199 190192 3623508 574389 57463 746711 3341018 391548 1009341}, 226631 #{302837}, 6759 #{20109}, 10983 #{138052 985236 574389 1250938 395451}, 318439 #{1017236}, 380008 #{192565}, 1126536 #{675364}, 191752 #{1026778}, 357672 #{472783 1009524}, 31024872 #{1280035}, 3985352 #{1236661}, 27955209 #{1033319 302837}, 1587689 #{388504}, 140841 #{473095}, 5206601 #{574400 473123 2775184 1319504 190192 1009341}, 694025 #{1009218}, 27593 #{473123 1319504}, 504458 #{1279856 1032117}, 17594154 #{1009299}, 87339 #{190192 937563}, 45035 #{1032117}, 897612 #{797516}, 1817228 #{473123 952368 1279856}, 31195693 #{1032117}, 472877 #{1280035}, 22187885 #{91401}, 8398 #{498833}, 1019150 #{797516 20109}, 1179950 #{473199}, 22958 #{950858 622461}, 9588430 #{1028237}, 183503 #{1254526}, 2023695 #{473321}, 8495 #{190192 1032117}, 801135 #{472783 1009524}, 17917391 #{952340}, 14343887 #{1009299}, 706543 #{1185824 1323591 3382280 472937 950858 644121 271964 622461 3782751}, 536080 #{1319504}, 1331441 #{937563}, 17193265 #{952340 1254526}, 1792433 #{1250938}, 434897 #{893160}, 1323985 #{1250938}, 427282 #{1026778}, 21523 #{1026778 937563}, 8707155 #{498833}, 689427 #{1009537 1017236}, 7699923 #{1185901}, 36425555 #{1009537}, 954323 #{1033319}, 29003796 #{574400 3074945 138052 541902 1010126 985236 574389 746711 3341018 3402586}, 6890644 #{1009537}, 172244 #{472783 1009524}, 76340 #{1288331}, 879637 #{952340 1032117}, 5657877 #{1319504}, 3670357 #{2784531}, 960021 #{952368}, 313942 #{472783 1009524 1250938}, 66294 #{574400}, 8190902 #{3786794}, 15690807 #{1009537}, 2276471 #{574400 687201 388102 388504}, 2538775 #{746711}, 1053303 #{1033319 952368}, 6533911 #{473095}, 30319607 #{91401}, 405944 #{2775184}, 16920 #{1254526}, 598776 #{2866177}, 233497 #{2881512}, 1171513 #{893160 1028237}, 1406201 #{797516 472783 1009524 302837 1250938}, 160986 #{1028237}, 4890 #{472783 1009524}, 598971 #{1280035}, 14482748 #{1236661}, 699964 #{746711}, 1775388 #{893160 1009230}, 6075324 #{797516}, 1188828 #{1236661}, 31813917 #{746711}, 18956829 #{1288331}, 20926 #{1009218 138052 985236}, 1966814 #{893160}, 693727 #{952340}, 17193471 #{3027908 1009754}, 3190431 #{473095 1017236}, 1718975 #{472783 1009524 746711 388504}, 3515391 #{644121}}, :rel (["962413" "763505"] ["1587689" "763505"] ["1188828" "5486694"] ["2023695" "693702"] ["36425555" "1228702"] ["8398" "690672"] ["31195693" "2675045"] ["5175143" "17228412"] ["8707155" "17228412"] ["4289067" "10690420"] ["14482748" "1034006"] ["36312376" "1188828"] ["29003796" "1188828"] ["30319607" "22820037"] ["36163957" "767343"] ["8330403" "767343"] ["226631" "17179027"] ["26308484" "17179027"] ["4094720" "17179027"] ["172244" "23173987"] ["5206601" "1009204"] ["31454449" "1009204"] ["796635" "1009204"] ["958652" "787876"] ["22718453" "24059390"] ["11737376" "24059390"] ["66294" "24059390"] ["31813917" "24059390"] ["405944" "3234221"] ["472877" "22705265"] ["8495" "6538378"] ["1792433" "32549775"] ["1323985" "22718453"] ["3670357" "22718453"] ["172244" "34044100"] ["470752" "34044100"] ["313942" "34044100"] ["6075324" "25304497"] ["7699923" "958652"] ["960021" "958652"] ["1718975" "744360"] ["1171513" "744360"] ["8190902" "5206601"] ["22187885" "5206601"] ["1331441" "5206601"] ["22532673" "5206601"] ["318439" "5206601"] ["29549713" "5206601"] ["24104134" "700074"] ["700355" "17503782"] ["1028771" "17503782"] ["5657877" "4861714"] ["3697698" "4861714"] ["160995" "4861714"] ["693727" "990361"] ["34310097" "990361"] ["694025" "990361"] ["699134" "990361"] ["3670357" "990361"] ["966983" "25825321"] ["191752" "6535800"] ["5657877" "1195726"] ["3234221" "693727"] ["4628120" "693727"] ["4594748" "693727"] ["20189596" "693727"] ["536080" "3697698"] ["45035" "3697698"] ["8190902" "4842680"] ["405944" "4628120"] ["2023695" "8135339"] ["318439" "11737376"] ["1911810" "11737376"] ["18462661" "9387252"] ["21936671" "9387252"] ["699964" "31229429"] ["470752" "30982373"] ["17917391" "30982373"] ["22476294" "692453"] ["821959" "692453"] ["694025" "709243"] ["699964" "7484211"] ["22820037" "772270"] ["34379506" "772270"] ["4890" "772270"] ["6759" "12469844"] ["958652" "30319607"] ["700355" "2157920"] ["6075324" "21053327"] ["871681" "22532673"] ["15325165" "22532673"] ["31454449" "709574"] ["32190039" "709574"] ["8190902" "24960643"] ["140841" "22042655"] ["4822234" "10978278"] ["1171513" "5065063"] ["30319607" "31454449"] ["5657877" "31195693"] ["598971" "31195693"] ["17193265" "22991469"] ["6759" "30774561"] ["3224825" "26743152"] ["32549775" "6533911"] ["28979098" "6533911"] ["357672" "6533911"] ["4890" "6533911"] ["22691076" "6533911"] ["87339" "6533911"] ["21936671" "6533911"] ["28978911" "6533911"] ["472877" "6533911"] ["66294" "36312376"] ["17228412" "17193061"] ["767343" "17193061"] ["17179027" "17193061"] ["30982373" "17193061"] ["24960643" "17193061"] ["6539078" "17193061"] ["5490673" "17193061"] ["15690807" "17193061"] ["28979098" "772545"] ["35718126" "772545"] ["17923612" "772545"] ["2210772" "772545"] ["17990732" "772545"] ["28978911" "772545"] ["1009209" "16989227"] ["951835" "16989227"] ["183503" "23890667"] ["699964" "5232797"] ["1019150" "21384136"] ["1775388" "1406201"] ["32549775" "17193265"] ["4861714" "17193265"] ["27955209" "17193265"] ["1587689" "17193265"] ["6890644" "691866"] ["31024872" "28979098"] ["160986" "3864201"] ["31024872" "35718126"] ["4861714" "946892"] ["1171513" "946892"] ["8050180" "17612277"] ["3190431" "716309"] ["10983" "693979"] ["1045088" "1754736"] ["1028771" "1754736"] ["4289067" "1754736"] ["6535800" "17652827"] ["801135" "17652827"] ["27593" "17652827"] ["804551" "2871217"] ["18956829" "697506"] ["5200150" "697506"] ["1770656" "697506"] ["504458" "17923612"] ["879637" "17923612"] ["8050180" "34698963"] ["1487877" "34698963"] ["1126536" "4594748"] ["5200150" "5673643"] ["897612" "1045088"] ["871681" "28927577"] ["22691076" "28927577"] ["30319607" "700355"] ["1406201" "700355"] ["708266" "700355"] ["1532663" "700355"] ["1855180" "700355"] ["31176997" "12535256"] ["3424576" "12535256"] ["796635" "5038508"] ["191752" "6537978"] ["26308484" "6537978"] ["160986" "6537978"] ["9588430" "6537978"] ["797088" "26263822"] ["946892" "4822234"] ["18956829" "4822234"] ["694008" "4822234"] ["6890644" "22476294"] ["76340" "22476294"] ["34044100" "18956829"] ["699964" "8439989"] ["380008" "34310097"] ["1179950" "29549713"] ["76340" "29549713"] ["24104134" "21764485"] ["45035" "21764485"] ["536080" "8156101"] ["31195693" "8156101"] ["694008" "26339806"] ["6075324" "1069987"] ["5200150" "36163957"] ["699134" "733122"] ["897612" "733122"] ["7279789" "28335948"] ["31195693" "6539521"] ["17193265" "6539521"] ["140841" "6539521"] ["17735029" "6539521"] ["17684887" "6539521"] ["694025" "6539521"] ["17193471" "6539521"] ["946910" "1028771"] ["1587689" "1028771"] ["18070174" "4873906"] ["22121360" "708266"] ["8398" "27248985"] ["5200150" "792595"] ["36425555" "6539078"] ["22532673" "6539078"] ["29549713" "6539078"] ["17306305" "6539078"] ["31024872" "2210772"] ["18956829" "1008581"] ["946910" "1008581"] ["18070174" "1008581"] ["2538775" "17306305"] ["87339" "17306305"] ["31176997" "17306305"] ["6535800" "17990732"] ["879637" "17990732"] ["4890" "17735029"] ["797088" "962413"] ["1966814" "17594154"] ["31195693" "5490673"] ["6538378" "694008"] ["9387252" "694008"] ["22042655" "694008"] ["6533911" "694008"] ["17193061" "694008"] ["17652827" "694008"] ["5038508" "694008"] ["6537978" "694008"] ["21764485" "694008"] ["8156101" "694008"] ["6539521" "694008"] ["9387237" "694008"] ["946910" "694008"] ["17506498" "694008"] ["9272793" "694008"] ["183503" "22166736"] ["958652" "1532663"] ["1179950" "27955209"] ["21523" "1991254"] ["17306305" "1991254"] ["3985352" "1991254"] ["1775388" "1991254"] ["1966814" "1991254"] ["3424576" "1991254"] ["14343887" "26308484"] ["470752" "6536797"] ["5200150" "6536797"] ["6759" "30787370"] ["1188828" "706543"] ["24059390" "706543"] ["1718975" "706543"] ["958652" "706543"] ["990361" "706543"] ["20926" "706543"] ["12535256" "706543"] ["22589574" "706543"] ["17594154" "706543"] ["1991254" "706543"] ["1171513" "706543"] ["726312" "706543"] ["797088" "706543"] ["233497" "706543"] ["33547228" "706543"] ["801135" "706543"] ["1053303" "706543"] ["19667111" "706543"] ["20924581" "706543"] ["14482748" "706543"] ["879637" "706543"] ["3175294" "33748657"] ["8398" "3175294"] ["29549713" "3175294"] ["226631" "35997105"] ["36425555" "804551"] ["17193265" "804551"] ["17917391" "804551"] ["699964" "3782398"] ["21523" "1171513"] ["31813917" "726312"] ["14343887" "726312"] ["5657877" "916810"] ["18070174" "11034627"] ["331680" "797088"] ["434897" "797088"] ["8707155" "797088"] ["3175294" "36477012"] ["14426697" "784409"] ["699964" "5646263"] ["772270" "772240"] ["536080" "9387237"] ["5200150" "17684887"] ["336897" "17684887"] ["6075324" "878280"] ["767343" "693985"] ["1323985" "693985"] ["24104134" "693985"] ["772545" "693985"] ["30876902" "693985"] ["17735029" "693985"] ["22958" "693985"] ["946910" "693985"] ["801135" "693985"] ["27593" "693985"] ["11737376" "960021"] ["1331441" "960021"] ["7279789" "960021"] ["20941685" "960021"] ["3515391" "10678854"] ["5673643" "946910"] ["17594154" "946910"] ["958652" "32190039"] ["700292" "1855180"] ["10978278" "1557538"] ["1487877" "1557538"] ["700355" "4289067"] ["946910" "34379506"] ["3224825" "34379506"] ["30319607" "36286862"] ["17193265" "17506498"] ["1045088" "744038"] ["598971" "694025"] ["427282" "694025"] ["2023695" "699134"] ["6759" "699134"] ["763505" "966983"] ["1754736" "966983"] ["1008581" "966983"] ["693985" "966983"] ["598776" "1587689"] ["598971" "1587689"] ["954323" "1587689"] ["470752" "1587689"] ["5200150" "1587689"] ["1792433" "1587689"] ["29421852" "1587689"] ["22532673" "5175143"] ["3190431" "5175143"] ["3424576" "5175143"] ["22187885" "33547228"] ["66294" "33547228"] ["470752" "33547228"] ["1775388" "33547228"] ["1188828" "914736"] ["16920" "914736"] ["140841" "11472332"] ["914736" "5699671"] ["6759" "5699671"] ["24104134" "21981503"] ["10974311" "21981503"] ["2023695" "10974311"] ["24059390" "22691076"] ["1718975" "22691076"] ["699964" "1686106"] ["4890" "22712867"] ["14343887" "7279789"] ["689427" "7279789"] ["22991469" "26478223"] ["16920" "26478223"] ["37832157" "26478223"] ["699964" "972730"] ["1009204" "1009209"] ["709243" "1009209"] ["694008" "1009209"] ["3190431" "693800"] ["1557538" "700292"] ["3224825" "700292"] ["9500290" "700292"] ["10978278" "4003390"] ["14426697" "4003390"] ["1487877" "4003390"] ["10983" "15522913"] ["692453" "690777"] ["954323" "690777"] ["1770656" "690777"] ["2883137" "17193471"] ["30876902" "17193471"] ["27955209" "17193471"] ["35997105" "17193471"] ["2538775" "17193471"] ["22691076" "17193471"] ["336897" "17193471"] ["19667111" "17193471"] ["20924581" "17193471"] ["18462661" "5200150"] ["172244" "5200150"] ["8330403" "21936671"] ["699964" "22121360"] ["871681" "20924581"] ["36425555" "20924581"] ["4094720" "20924581"] ["689427" "20924581"] ["22476294" "1770656"] ["694025" "21917434"] ["17228412" "14482748"] ["5206601" "14482748"] ["18462661" "14482748"] ["191752" "14482748"] ["1817228" "14482748"] ["17193265" "37832157"] ["4890" "1152426"] ["5200150" "20189596"] ["36425555" "14426697"] ["772545" "3515391"] ["1171513" "3515391"] ["31195693" "18070174"] ["31024872" "28978911"] ["3175294" "1098276"] ["3515391" "1098276"] ["22991469" "1055691"] ["706543" "1055691"] ["140806" "29421852"] ["15522913" "11504647"] ["191752" "821959"] ["22991469" "3224825"] ["16920" "3224825"] ["17193471" "3224825"] ["3515391" "3224825"] ["5200150" "1487877"] ["6075324" "29313810"] ["31195693" "9500290"] ["6536797" "9272793"] ["706543" "9272793"] ["2538775" "1228459"] ["1019150" "20941685"] ["4842680" "951835"] ["2276471" "951835"] ["8495" "951835"] ["1331441" "796635"] ["36312376" "796635"] ["23890667" "796635"] ["8050180" "796635"] ["16920" "796635"] ["3515391" "796635"] ["5200150" "958609"] ["700355" "32611713"] ["699964" "4183228"] ["470752" "15325165"] ["700355" "25174161"]), :title-results {1185824 #search_api.search_api.Paper{:id 1185824, :key "journals/jocn/AblaKO08", :title "On-line Assessment of Statistical Learning by Event-related Potentials.", :abstract "We investigated the neural processes involved in on-line statistical learning and word segmentation. Auditory event-related potentials (ERPs) were recorded while participants were exposed to continuous, nonlinguistic auditory sequences, the elements of which were organized into “tritone words” that were sequenced in random order, with no silent spaces between them. After listening to three 6.6-min sessions of sequences, the participants performed a behavioral choice test, in which they were instructed to indicate the most familiar tone sequence in each test trial by pressing buttons. The participants were divided into three groups (high, middle, and low learners) based on their behavioral performance. The overall mean performance was 74.4%, indicating that the tone sequence was segmented and that the participants learned the tone words statistically. Grand-averaged ERPs showed that word onset (initial tone) elicited the largest N100 and N400 in the early learning session of high learners, but in middle learners, the word-onset effect was elicited in a later session, and there was no effect in low learners. The N400 amplitudes significantly differed between the three learning sessions in the high-and middle-learner groups. The results suggest that the N400 effect indicates not only on-line word segmentation but also the degree of statistical learning. This study provides insight into the neural mechanisms underlying on-line statistical learning processes.", :author (#search_api.search_api.Author{:id 85341, :first-name nil, :last-name nil, :full-name "Dilshat Abla"} #search_api.search_api.Author{:id 167451, :first-name nil, :last-name nil, :full-name "Kentaro Katahira"} #search_api.search_api.Author{:id 883499, :first-name nil, :last-name nil, :full-name "Kazuo Okanoya"}), :year 2008, :venue "J. Cognitive Neuroscience", :ncit 0, :string "On-line Assessment of Statistical Learning by Event-related Potentials.. We investigated the neural processes involved in on-line statistical learning and word segmentation. Auditory event-related potentials (ERPs) were recorded while participants were exposed to continuous, nonlinguistic auditory sequences, the elements of which were organized into “tritone words” that were sequenced in random order, with no silent spaces between them. After listening to three 6.6-min sessions of sequences, the participants performed a behavioral choice test, in which they were instructed to indicate the most familiar tone sequence in each test trial by pressing buttons. The participants were divided into three groups (high, middle, and low learners) based on their behavioral performance. The overall mean performance was 74.4%, indicating that the tone sequence was segmented and that the participants learned the tone words statistically. Grand-averaged ERPs showed that word onset (initial tone) elicited the largest N100 and N400 in the early learning session of high learners, but in middle learners, the word-onset effect was elicited in a later session, and there was no effect in low learners. The N400 amplitudes significantly differed between the three learning sessions in the high-and middle-learner groups. The results suggest that the N400 effect indicates not only on-line word segmentation but also the degree of statistical learning. This study provides insight into the neural mechanisms underlying on-line statistical learning processes.", :doc-id "On-line Assessment of Statistical Learning by Event-related Potentials. 2008 Dilshat Abla, Kentaro Katahira, Kazuo Okanoya"}, 574400 #search_api.search_api.Paper{:id 574400, :key "conf/sbia/Raedt08", :title "Logical and Relational Learning.", :abstract "I use the term logical and relational learning (LRL) to refer to the subfield of machine learning and data mining that is concerned with learning in expressive logical or relational representations. It is the union of inductive logic programming, (statistical) relational learning and multi-relational data mining and constitutes a general class of techniques and methodology for learning from structured data (such as graphs, networks, relational databases) and background knowledge. During the course of its existence, logical and relational learning has changed dramatically. Whereas early work was mainly concerned with logical issues (and even program synthesis from examples), in the 90s its focus was on the discovery of new and interpretable knowledge from structured data, often in the form of rules or patterns. Since then the range of tasks to which logical and relational learning has been applied has significantly broadened and now covers almost all machine learning problems and settings. Today, there exist logical and relational learning methods for reinforcement learning, statistical learning, distance- and kernel-based learning in addition to traditional symbolic machine learning approaches. At the same time, logical and relational learning problems are appearing everywhere. Advances in intelligent systems are enabling the generation of high-level symbolic and structured data in a wide variety of domains, including the semantic web, robotics, vision, social networks, and the life sciences, which in turn raises new challenges and opportunities for logical and relational learning. These developments have led to a new view on logical and relational learning and its role in machine learning and artificial intelligence. In this talk, I shall reflect on this view by identifying some of the lessons learned in logical and relational learning and formulating some challenges for future developments.", :author (#search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2008, :venue "SBIA", :ncit 173, :string "Logical and Relational Learning.. I use the term logical and relational learning (LRL) to refer to the subfield of machine learning and data mining that is concerned with learning in expressive logical or relational representations. It is the union of inductive logic programming, (statistical) relational learning and multi-relational data mining and constitutes a general class of techniques and methodology for learning from structured data (such as graphs, networks, relational databases) and background knowledge. During the course of its existence, logical and relational learning has changed dramatically. Whereas early work was mainly concerned with logical issues (and even program synthesis from examples), in the 90s its focus was on the discovery of new and interpretable knowledge from structured data, often in the form of rules or patterns. Since then the range of tasks to which logical and relational learning has been applied has significantly broadened and now covers almost all machine learning problems and settings. Today, there exist logical and relational learning methods for reinforcement learning, statistical learning, distance- and kernel-based learning in addition to traditional symbolic machine learning approaches. At the same time, logical and relational learning problems are appearing everywhere. Advances in intelligent systems are enabling the generation of high-level symbolic and structured data in a wide variety of domains, including the semantic web, robotics, vision, social networks, and the life sciences, which in turn raises new challenges and opportunities for logical and relational learning. These developments have led to a new view on logical and relational learning and its role in machine learning and artificial intelligence. In this talk, I shall reflect on this view by identifying some of the lessons learned in logical and relational learning and formulating some challenges for future developments.", :doc-id "Logical and Relational Learning. 2008 Luc De Raedt"}, 2866177 #search_api.search_api.Paper{:id 2866177, :key "journals/ml/LandwehrPRF10", :title "Fast learning of relational kernels.", :abstract "We develop a general theoretical framework for statistical logical learning with kernels based on dynamic propositionalization, where structure learning corresponds to inferring a suitable kernel on logical objects, and parameter learning corresponds to function learning in the resulting reproducing kernel Hilbert space. In particular, we study the case where structure learning is performed by a simple FOIL-like algorithm, and propose alternative scoring functions for guiding the search process. We present an empirical evaluation on several data sets in the single-task as well as in the multi-task setting.", :author (#search_api.search_api.Author{:id 127590, :first-name nil, :last-name nil, :full-name "Niels Landwehr"} #search_api.search_api.Author{:id 1520074, :first-name nil, :last-name nil, :full-name "Andrea Passerini"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"} #search_api.search_api.Author{:id 34394, :first-name nil, :last-name nil, :full-name "Paolo Frasconi"}), :year 2010, :venue "Machine Learning", :ncit 12, :string "Fast learning of relational kernels.. We develop a general theoretical framework for statistical logical learning with kernels based on dynamic propositionalization, where structure learning corresponds to inferring a suitable kernel on logical objects, and parameter learning corresponds to function learning in the resulting reproducing kernel Hilbert space. In particular, we study the case where structure learning is performed by a simple FOIL-like algorithm, and propose alternative scoring functions for guiding the search process. We present an empirical evaluation on several data sets in the single-task as well as in the multi-task setting.", :doc-id "Fast learning of relational kernels. 2010 Niels Landwehr, Andrea Passerini, Luc De Raedt, Paolo Frasconi"}, 687201 #search_api.search_api.Paper{:id 687201, :key "conf/grc/Chen07", :title "Research on Statistical Relational Learning and Rough Set in SRL.", :abstract "Statistical relational learning constructs statistical mod- els from relational databases, combining the powers of re- lational learning and statistical learning. Its strong abil- ity and special property make statistical relational learn- ing become one of the important areas in machine learn- ing. In this paper, the general concepts and characteris- tics of statistical relational learning are presented firstly. Then some major branches of this newly emerging field are discussed, including logic and rule-based approaches, frame and object-oriented approaches, and several other important approaches. After that some methods of apply- ing rough set in statistical relational learning are described, such as gRS-ILP and VPRSILP. Finally applications of sta- tistical relational learning are briefly introduced and some future directions of statistical relational learning and the prospects of rough set in this area are pointed out.", :author (#search_api.search_api.Author{:id 74552, :first-name nil, :last-name nil, :full-name "Fei Chen"}), :year 2007, :venue "GrC", :ncit 0, :string "Research on Statistical Relational Learning and Rough Set in SRL.. Statistical relational learning constructs statistical mod- els from relational databases, combining the powers of re- lational learning and statistical learning. Its strong abil- ity and special property make statistical relational learn- ing become one of the important areas in machine learn- ing. In this paper, the general concepts and characteris- tics of statistical relational learning are presented firstly. Then some major branches of this newly emerging field are discussed, including logic and rule-based approaches, frame and object-oriented approaches, and several other important approaches. After that some methods of apply- ing rough set in statistical relational learning are described, such as gRS-ILP and VPRSILP. Finally applications of sta- tistical relational learning are briefly introduced and some future directions of statistical relational learning and the prospects of rough set in this area are pointed out.", :doc-id "Research on Statistical Relational Learning and Rough Set in SRL. 2007 Fei Chen"}, 1009537 #search_api.search_api.Paper{:id 1009537, :key "journals/ml/Hofmann01", :title "Unsupervised Learning by Probabilistic Latent Semantic Analysis.", :abstract "This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.", :author (#search_api.search_api.Author{:id 8394, :first-name nil, :last-name nil, :full-name "Thomas Hofmann"}), :year 2001, :venue "Machine Learning", :ncit 2463, :string "Unsupervised Learning by Probabilistic Latent Semantic Analysis.. This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.", :doc-id "Unsupervised Learning by Probabilistic Latent Semantic Analysis. 2001 Thomas Hofmann"}, 3074945 #search_api.search_api.Paper{:id 3074945, :key "journals/ml/BlockeelBRDKY11", :title "Guest editorial to the special issue on inductive logic programming, mining and learning in graphs and statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 882341, :first-name nil, :last-name nil, :full-name "Hendrik Blockeel"} #search_api.search_api.Author{:id 272602, :first-name nil, :last-name nil, :full-name "Karsten M. Borgwardt"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"} #search_api.search_api.Author{:id 668085, :first-name nil, :last-name nil, :full-name "Pedro Domingos"} #search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"} #search_api.search_api.Author{:id 324907, :first-name nil, :last-name nil, :full-name "Xifeng Yan"}), :year 2011, :venue "Machine Learning", :ncit 0, :string "Guest editorial to the special issue on inductive logic programming, mining and learning in graphs and statistical relational learning.. ", :doc-id "Guest editorial to the special issue on inductive logic programming, mining and learning in graphs and statistical relational learning. 2011 Hendrik Blockeel, Karsten M. Borgwardt, Luc De Raedt, Pedro Domingos, Kristian Kersting, Xifeng Yan"}, 1009218 #search_api.search_api.Paper{:id 1009218, :key "journals/ml/Boulle04", :title "Khiops: A Statistical Discretization Method of Continuous Attributes.", :abstract "In supervised machine learning, some algorithms are restricted to discrete data and have to discretize continuous attributes. Many discretization methods, based on statistical criteria, information content, or other specialized criteria, have been studied in the past. In this paper, we propose the discretization method Khiops,1 based on the chi-square statistic. In contrast with related methods ChiMerge and ChiSplit, this method optimizes the chi-square criterion in a global manner on the whole discretization domain and does not require any stopping criterion. A theoretical study followed by experiments demonstrates the robustness and the good predictive performance of the method.", :author (#search_api.search_api.Author{:id 1526635, :first-name nil, :last-name nil, :full-name "Marc Boullé"}), :year 2004, :venue "Machine Learning", :ncit 0, :string "Khiops: A Statistical Discretization Method of Continuous Attributes.. In supervised machine learning, some algorithms are restricted to discrete data and have to discretize continuous attributes. Many discretization methods, based on statistical criteria, information content, or other specialized criteria, have been studied in the past. In this paper, we propose the discretization method Khiops,1 based on the chi-square statistic. In contrast with related methods ChiMerge and ChiSplit, this method optimizes the chi-square criterion in a global manner on the whole discretization domain and does not require any stopping criterion. A theoretical study followed by experiments demonstrates the robustness and the good predictive performance of the method.", :doc-id "Khiops: A Statistical Discretization Method of Continuous Attributes. 2004 Marc Boullé"}, 1280035 #search_api.search_api.Paper{:id 1280035, :key "journals/nn/Watanabe10", :title "Equations of states in singular statistical estimation.", :abstract "Learning machines that have hierarchical structures or hidden variables are singular statistical models because they are nonidentifiable and their Fisher information matrices are singular. In singular statistical models, neither does the Bayes a posteriori distribution converge to the normal distribution nor does the maximum likelihood estimator satisfy asymptotic normality. This is the main reason that it has been difficult to predict their generalization performance from trained states. In this paper, we study four errors, (1) the Bayes generalization error, (2) the Bayes training error, (3) the Gibbs generalization error, and (4) the Gibbs training error, and prove that there are universal mathematical relations among these errors. The formulas proved in this paper are equations of states in statistical estimation because they hold for any true distribution, any parametric model, and any a priori distribution. Also we show that the Bayes and Gibbs generalization errors can be estimated by Bayes and Gibbs training errors, and we propose widely applicable information criteria that can be applied to both regular and singular statistical models.", :author (#search_api.search_api.Author{:id 976510, :first-name nil, :last-name nil, :full-name "Sumio Watanabe"}), :year 2010, :venue "Neural Networks", :ncit 13, :string "Equations of states in singular statistical estimation.. Learning machines that have hierarchical structures or hidden variables are singular statistical models because they are nonidentifiable and their Fisher information matrices are singular. In singular statistical models, neither does the Bayes a posteriori distribution converge to the normal distribution nor does the maximum likelihood estimator satisfy asymptotic normality. This is the main reason that it has been difficult to predict their generalization performance from trained states. In this paper, we study four errors, (1) the Bayes generalization error, (2) the Bayes training error, (3) the Gibbs generalization error, and (4) the Gibbs training error, and prove that there are universal mathematical relations among these errors. The formulas proved in this paper are equations of states in statistical estimation because they hold for any true distribution, any parametric model, and any a priori distribution. Also we show that the Bayes and Gibbs generalization errors can be estimated by Bayes and Gibbs training errors, and we propose widely applicable information criteria that can be applied to both regular and singular statistical models.", :doc-id "Equations of states in singular statistical estimation. 2010 Sumio Watanabe"}, 473123 #search_api.search_api.Paper{:id 473123, :key "conf/kdd/NevilleJFH03", :title "Learning relational probability trees.", :abstract "Classification trees are widely used in the machine learning and data mining communities for modeling propositional data. Recent work has extended this basic paradigm to probability estimation trees. Traditional tree learning algorithms assume that instances in the training data are homogenous and independently distributed. Relational probability trees (RPTs) extend standard probability estimation trees to a relational setting in which data instances are heterogeneous and interdependent. Our algorithm for learning the structure and parameters of an RPT searches over a space of relational features that use aggregation functions (e.g. AVERAGE, MODE, COUNT) to dynamically propositionalize relational data and create binary splits within the RPT. Previous work has identified a number of statistical biases due to characteristics of relational data such as autocorrelation and degree disparity. The RPT algorithm uses a novel form of randomization test to adjust for these biases. On a variety of relational learning tasks, RPTs built using randomization tests are significantly smaller than other models and achieve equivalent, or better, performance.", :author (#search_api.search_api.Author{:id 150768, :first-name nil, :last-name nil, :full-name "Jennifer Neville"} #search_api.search_api.Author{:id 408381, :first-name nil, :last-name nil, :full-name "David Jensen"} #search_api.search_api.Author{:id 1434755, :first-name nil, :last-name nil, :full-name "Lisa Friedland"} #search_api.search_api.Author{:id 575750, :first-name nil, :last-name nil, :full-name "Michael Hay"}), :year 2003, :venue "KDD", :ncit 191, :string "Learning relational probability trees.. Classification trees are widely used in the machine learning and data mining communities for modeling propositional data. Recent work has extended this basic paradigm to probability estimation trees. Traditional tree learning algorithms assume that instances in the training data are homogenous and independently distributed. Relational probability trees (RPTs) extend standard probability estimation trees to a relational setting in which data instances are heterogeneous and interdependent. Our algorithm for learning the structure and parameters of an RPT searches over a space of relational features that use aggregation functions (e.g. AVERAGE, MODE, COUNT) to dynamically propositionalize relational data and create binary splits within the RPT. Previous work has identified a number of statistical biases due to characteristics of relational data such as autocorrelation and degree disparity. The RPT algorithm uses a novel form of randomization test to adjust for these biases. On a variety of relational learning tasks, RPTs built using randomization tests are significantly smaller than other models and achieve equivalent, or better, performance.", :doc-id "Learning relational probability trees. 2003 Jennifer Neville, David Jensen, Lisa Friedland, Michael Hay"}, 336067 #search_api.search_api.Paper{:id 336067, :key "conf/icml/KokD07", :title "Statistical predicate invention.", :abstract "We propose statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models and predicate invention in ILP. We propose an initial model for SPI based on second-order Markov logic, in which predicates as well as arguments can be variables, and the domain of discourse is not fully known in advance. Our approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with (e.g., it clusters relations by the clusters of the objects they relate). Since different clusterings are better for predicting different subsets of the atoms, we allow multiple cross-cutting clusterings. We show that this approach outperforms Markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets.", :author (#search_api.search_api.Author{:id 801078, :first-name nil, :last-name nil, :full-name "Stanley Kok"} #search_api.search_api.Author{:id 668085, :first-name nil, :last-name nil, :full-name "Pedro Domingos"}), :year 2007, :venue "ICML", :ncit 59, :string "Statistical predicate invention.. We propose statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models and predicate invention in ILP. We propose an initial model for SPI based on second-order Markov logic, in which predicates as well as arguments can be variables, and the domain of discourse is not fully known in advance. Our approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with (e.g., it clusters relations by the clusters of the objects they relate). Since different clusterings are better for predicting different subsets of the atoms, we allow multiple cross-cutting clusterings. We show that this approach outperforms Markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets.", :doc-id "Statistical predicate invention. 2007 Stanley Kok, Pedro Domingos"}, 675364 #search_api.search_api.Paper{:id 675364, :key "conf/www/QinLZWXL08", :title "Learning to rank relational objects and its application to web search.", :abstract "Learning to rank is a new statistical learning technology on creating a ranking model for sorting objects. The technology has been successfully applied to web search, and is becoming one of the key machineries for building search engines. Existing approaches to learning to rank, however, did not consider the cases in which there exists relationship between the objects to be ranked, despite of the fact that such situations are very common in practice. For example, in web search, given a query certain relationships usually exist among the the retrieved documents, e.g., URL hierarchy, similarity, etc., and sometimes it is necessary to utilize the information in ranking of the documents. This paper addresses the issue and formulates it as a novel learning problem, referred to as, 'learning to rank relational objects'. In the new learning task, the ranking model is defined as a function of not only the contents (features) of objects but also the relations between objects. The paper further focuses on one setting of the learning problem in which the way of using relation information is predetermined. It formalizes the learning task as an optimization problem in the setting. The paper then proposes a new method to perform the optimization task, particularly an implementation based on SVM. Experimental results show that the proposed method outperforms the baseline methods for two ranking tasks (Pseudo Relevance Feedback and Topic Distillation) in web search, indicating that the proposed method can indeed make effective use of relation information and content information in ranking.", :author (#search_api.search_api.Author{:id 17439, :first-name nil, :last-name nil, :full-name "Tao Qin"} #search_api.search_api.Author{:id 1411681, :first-name nil, :last-name nil, :full-name "Tie-Yan Liu"} #search_api.search_api.Author{:id 981048, :first-name nil, :last-name nil, :full-name "Xu-Dong Zhang"} #search_api.search_api.Author{:id 566222, :first-name nil, :last-name nil, :full-name "De-Sheng Wang"} #search_api.search_api.Author{:id 1482892, :first-name nil, :last-name nil, :full-name "Wen-Ying Xiong"} #search_api.search_api.Author{:id 742456, :first-name nil, :last-name nil, :full-name "Hang Li"}), :year 2008, :venue "WWW", :ncit 62, :string "Learning to rank relational objects and its application to web search.. Learning to rank is a new statistical learning technology on creating a ranking model for sorting objects. The technology has been successfully applied to web search, and is becoming one of the key machineries for building search engines. Existing approaches to learning to rank, however, did not consider the cases in which there exists relationship between the objects to be ranked, despite of the fact that such situations are very common in practice. For example, in web search, given a query certain relationships usually exist among the the retrieved documents, e.g., URL hierarchy, similarity, etc., and sometimes it is necessary to utilize the information in ranking of the documents. This paper addresses the issue and formulates it as a novel learning problem, referred to as, 'learning to rank relational objects'. In the new learning task, the ranking model is defined as a function of not only the contents (features) of objects but also the relations between objects. The paper further focuses on one setting of the learning problem in which the way of using relation information is predetermined. It formalizes the learning task as an optimization problem in the setting. The paper then proposes a new method to perform the optimization task, particularly an implementation based on SVM. Experimental results show that the proposed method outperforms the baseline methods for two ranking tasks (Pseudo Relevance Feedback and Topic Distillation) in web search, indicating that the proposed method can indeed make effective use of relation information and content information in ranking.", :doc-id "Learning to rank relational objects and its application to web search. 2008 Tao Qin, Tie-Yan Liu, Xu-Dong Zhang, De-Sheng Wang, Wen-Ying Xiong, Hang Li"}, 138052 #search_api.search_api.Paper{:id 138052, :key "conf/dagstuhl/PasseriniFR05", :title "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.", :abstract "We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :author (#search_api.search_api.Author{:id 1520074, :first-name nil, :last-name nil, :full-name "Andrea Passerini"} #search_api.search_api.Author{:id 34394, :first-name nil, :last-name nil, :full-name "Paolo Frasconi"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2005, :venue "Probabilistic, Logical and Relational Learning", :ncit 31, :string "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.. We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :doc-id "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting. 2005 Andrea Passerini, Paolo Frasconi, Luc De Raedt"}, 3027908 #search_api.search_api.Paper{:id 3027908, :key "conf/sigmod/GetoorM11", :title "Learning statistical models from relational data.", :abstract nil, :author (#search_api.search_api.Author{:id 1449720, :first-name nil, :last-name nil, :full-name "Lise Getoor"} #search_api.search_api.Author{:id 421571, :first-name nil, :last-name nil, :full-name "Lilyana Mihalkova"}), :year 2011, :venue "SIGMOD Conference", :ncit 87, :string "Learning statistical models from relational data.. ", :doc-id "Learning statistical models from relational data. 2011 Lise Getoor, Lilyana Mihalkova"}, 388102 #search_api.search_api.Paper{:id 388102, :key "conf/ijcai/DavisBDPRCS05", :title "View Learning for Statistical Relational Learning: With an Application to Mammography.", :abstract "Statistical relational learning (SRL) constructs probabilistic models from relational databases. A key capability of SRL is the learning of arcs (in the Bayes net sense) connecting entries in different rows of a relational table, or in different tables. Nevertheless, SRL approaches currently are constrained to use the existing database schema. For many database applications, users find it profitable to define alternative \"views\" of the database, in effect defining new fields or tables. Such new fields or tables can also be highly useful in learning. We provide SRL with the capability of learning new views.", :author (#search_api.search_api.Author{:id 747351, :first-name nil, :last-name nil, :full-name "Jesse Davis"} #search_api.search_api.Author{:id 750446, :first-name nil, :last-name nil, :full-name "Elizabeth S. Burnside"} #search_api.search_api.Author{:id 1157247, :first-name nil, :last-name nil, :full-name "Inês de Castro Dutra"} #search_api.search_api.Author{:id 138183, :first-name nil, :last-name nil, :full-name "David Page"} #search_api.search_api.Author{:id 464920, :first-name nil, :last-name nil, :full-name "Raghu Ramakrishnan"} #search_api.search_api.Author{:id 1352436, :first-name nil, :last-name nil, :full-name "Vítor Santos Costa"} #search_api.search_api.Author{:id 7, :first-name nil, :last-name nil, :full-name "Jude W. Shavlik"}), :year 2005, :venue "IJCAI", :ncit 33, :string "View Learning for Statistical Relational Learning: With an Application to Mammography.. Statistical relational learning (SRL) constructs probabilistic models from relational databases. A key capability of SRL is the learning of arcs (in the Bayes net sense) connecting entries in different rows of a relational table, or in different tables. Nevertheless, SRL approaches currently are constrained to use the existing database schema. For many database applications, users find it profitable to define alternative \"views\" of the database, in effect defining new fields or tables. Such new fields or tables can also be highly useful in learning. We provide SRL with the capability of learning new views.", :doc-id "View Learning for Statistical Relational Learning: With an Application to Mammography. 2005 Jesse Davis, Elizabeth S. Burnside, Inês de Castro Dutra, David Page, Raghu Ramakrishnan, Vítor Santos Costa, Jude W. Shavlik"}, 303622 #search_api.search_api.Paper{:id 303622, :key "conf/icdm/XiangN08", :title "Pseudolikelihood EM for Within-network Relational Learning.", :abstract "In this work, we study the problem of \\emph{within-network} relational learning and inference, where models are learned on a partially labeled relational dataset and then are applied to predict the classes of unlabeled instances in the same graph. We categorize recent work in statistical relational learning into three alternative approaches for this setting: disjoint learning with disjoint inference, disjoint learning with collective inference, and collective learning with collective inference. Models from each of these categories has been employed previously in different settings, but to our knowledge there has been no systematic comparison of models from all three categories. In this paper, we develop a novel pseudolikelihood EM method that facilitates more general \\emph{collective learning} and \\emph{collective inference} on partially labeled relational networks. We then compare this method to competing methods from the other categories on both synthetic and real-world data. We show that collective learning and inference with the pseudolikelihood EM approach achieves significantly higher accuracy than the other types of models when there are a moderate number of labeled examples in the data graph.", :author (#search_api.search_api.Author{:id 819343, :first-name nil, :last-name nil, :full-name "Rongjing Xiang"} #search_api.search_api.Author{:id 150768, :first-name nil, :last-name nil, :full-name "Jennifer Neville"}), :year 2008, :venue "ICDM", :ncit 9, :string "Pseudolikelihood EM for Within-network Relational Learning.. In this work, we study the problem of \\emph{within-network} relational learning and inference, where models are learned on a partially labeled relational dataset and then are applied to predict the classes of unlabeled instances in the same graph. We categorize recent work in statistical relational learning into three alternative approaches for this setting: disjoint learning with disjoint inference, disjoint learning with collective inference, and collective learning with collective inference. Models from each of these categories has been employed previously in different settings, but to our knowledge there has been no systematic comparison of models from all three categories. In this paper, we develop a novel pseudolikelihood EM method that facilitates more general \\emph{collective learning} and \\emph{collective inference} on partially labeled relational networks. We then compare this method to competing methods from the other categories on both synthetic and real-world data. We show that collective learning and inference with the pseudolikelihood EM approach achieves significantly higher accuracy than the other types of models when there are a moderate number of labeled examples in the data graph.", :doc-id "Pseudolikelihood EM for Within-network Relational Learning. 2008 Rongjing Xiang, Jennifer Neville"}, 3258022 #search_api.search_api.Paper{:id 3258022, :key "journals/ml/NatarajanKKGS12", :title "Gradient-based boosting for statistical relational learning: The relational dependency network case.", :abstract nil, :author (#search_api.search_api.Author{:id 883462, :first-name nil, :last-name nil, :full-name "Sriraam Natarajan"} #search_api.search_api.Author{:id 905516, :first-name nil, :last-name nil, :full-name "Tushar Khot"} #search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"} #search_api.search_api.Author{:id 1351938, :first-name nil, :last-name nil, :full-name "Bernd Gutmann"} #search_api.search_api.Author{:id 7, :first-name nil, :last-name nil, :full-name "Jude W. Shavlik"}), :year 2012, :venue "Machine Learning", :ncit 20, :string "Gradient-based boosting for statistical relational learning: The relational dependency network case.. ", :doc-id "Gradient-based boosting for statistical relational learning: The relational dependency network case. 2012 Sriraam Natarajan, Tushar Khot, Kristian Kersting, Bernd Gutmann, Jude W. Shavlik"}, 473095 #search_api.search_api.Paper{:id 473095, :key "conf/kdd/Moore06", :title "New cached-sufficient statistics algorithms for quickly answering statistical questions.", :abstract "This talk is about recent work on new ways to exploit preprocessed views of data tables for tractably solving big statistical queries. We'll describe deployments of these new algorithms in the realms of detecting killer asteroids and unnatural disease outbreaks.In recent years, several groups have looked at methods for pre-storing general sufficient statistics of the data in spatial data structures such as kd-trees and ball-trees so that both frequentist and Bayesian statistical operations become fast for large datasets. In this talk we will look at two other classes of optimization required in important statistical queries.The first involves iterating over all spatial regions (big and small). The second involves detection of tracks from noisy intermittent observations separated far apart in time and space. We will also discuss the implications that have arisen from making these operations tractable. We will focus particularly onDetecting all asteroids in the solar system larger than Pittsburgh's Cathedral of Learning (data to be collected over 2006-2010). Early detection of emerging diseases based on national monitoring of health-related transactions..", :author (#search_api.search_api.Author{:id 685289, :first-name nil, :last-name nil, :full-name "Andrew Moore"}), :year 2006, :venue "KDD", :ncit 0, :string "New cached-sufficient statistics algorithms for quickly answering statistical questions.. This talk is about recent work on new ways to exploit preprocessed views of data tables for tractably solving big statistical queries. We'll describe deployments of these new algorithms in the realms of detecting killer asteroids and unnatural disease outbreaks.In recent years, several groups have looked at methods for pre-storing general sufficient statistics of the data in spatial data structures such as kd-trees and ball-trees so that both frequentist and Bayesian statistical operations become fast for large datasets. In this talk we will look at two other classes of optimization required in important statistical queries.The first involves iterating over all spatial regions (big and small). The second involves detection of tracks from noisy intermittent observations separated far apart in time and space. We will also discuss the implications that have arisen from making these operations tractable. We will focus particularly onDetecting all asteroids in the solar system larger than Pittsburgh's Cathedral of Learning (data to be collected over 2006-2010). Early detection of emerging diseases based on national monitoring of health-related transactions..", :doc-id "New cached-sufficient statistics algorithms for quickly answering statistical questions. 2006 Andrew Moore"}, 1033319 #search_api.search_api.Paper{:id 1033319, :key "journals/pami/WechslerDLC04", :title "Motion Estimation Using Statistical Learning Theory.", :abstract "Abstract--This paper describes a novel application of Statistical Learning Theory (SLT) to single motion estimation and tracking. The problem of motion estimation can be related to statistical model selection, where the goal is to select one (correct) motion model from several possible motion models, given finite noisy samples. SLT, also known as Vapnik-Chervonenkis (VC), theory provides analytic generalization bounds for model selection, which have been used successfully for practical model selection. This paper describes a successful application of an SLT-based model selection approach to the challenging problem of estimating optimal motion models from small data sets of image measurements (flow). We present results of experiments on both synthetic and real image sequences for motion interpolation and extrapolation; these results demonstrate the feasibility and strength of our approach. Our experimental results show that for motion estimation applications, SLT-based model selection compares favorably against alternative model selection methods, such as the Akaike's fpe, Schwartz' criterion (sc), Generalized Cross-Validation (gcv), and Shibata's Model Selector (sms). The paper also shows how to address the aperture problem using SLT-based model selection for penalized linear (ridge regression) formulation.", :author (#search_api.search_api.Author{:id 1499994, :first-name nil, :last-name nil, :full-name "Harry Wechsler"} #search_api.search_api.Author{:id 398169, :first-name nil, :last-name nil, :full-name "Zoran Duric"} #search_api.search_api.Author{:id 1493163, :first-name nil, :last-name nil, :full-name "Fayin Li"} #search_api.search_api.Author{:id 1104310, :first-name nil, :last-name nil, :full-name "Vladimir Cherkassky"}), :year 2004, :venue "IEEE Trans. Pattern Anal. Mach. Intell.", :ncit 50, :string "Motion Estimation Using Statistical Learning Theory.. Abstract--This paper describes a novel application of Statistical Learning Theory (SLT) to single motion estimation and tracking. The problem of motion estimation can be related to statistical model selection, where the goal is to select one (correct) motion model from several possible motion models, given finite noisy samples. SLT, also known as Vapnik-Chervonenkis (VC), theory provides analytic generalization bounds for model selection, which have been used successfully for practical model selection. This paper describes a successful application of an SLT-based model selection approach to the challenging problem of estimating optimal motion models from small data sets of image measurements (flow). We present results of experiments on both synthetic and real image sequences for motion interpolation and extrapolation; these results demonstrate the feasibility and strength of our approach. Our experimental results show that for motion estimation applications, SLT-based model selection compares favorably against alternative model selection methods, such as the Akaike's fpe, Schwartz' criterion (sc), Generalized Cross-Validation (gcv), and Shibata's Model Selector (sms). The paper also shows how to address the aperture problem using SLT-based model selection for penalized linear (ridge regression) formulation.", :doc-id "Motion Estimation Using Statistical Learning Theory. 2004 Harry Wechsler, Zoran Duric, Fayin Li, Vladimir Cherkassky"}, 746727 #search_api.search_api.Paper{:id 746727, :key "journals/aicom/Tian06", :title "Context-based statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 260142, :first-name nil, :last-name nil, :full-name "YongHong Tian"}), :year 2006, :venue "AI Commun.", :ncit 0, :string "Context-based statistical relational learning.. ", :doc-id "Context-based statistical relational learning. 2006 YongHong Tian"}, 1323591 #search_api.search_api.Paper{:id 1323591, :key "conf/icc/TiwanaSA09", :title "Statistical Learning for Automated RRM: Application to eUTRAN Mobility.", :abstract "Self Organizing Network (SON) functionalities are currently developed to improve network performance and management tasks. SON functionalities require efficient utilization of data extracted from the network. In this context, the paper has two objectives. First it is shown that one can use simple statistical learning techniques such as regression to extract a model from data. The model comprises closed form expressions that approximate the functional relations between measured Key Performance Indicators (KPIs) and Radio Resource Management (RRM) parameters. Second, it is shown how the model can be integrated in a monitoring process and be used to devise an efficient auto-tuning algorithm. To this end, two case studies of handover monitoring and handover auto-tuning in a LTE network are described and illustrate the application of the proposed approach.", :author (#search_api.search_api.Author{:id -1, :first-name nil, :last-name nil, :full-name "Moazzam Islam Tiwana"} #search_api.search_api.Author{:id 1094720, :first-name nil, :last-name nil, :full-name "Berna Sayraç"} #search_api.search_api.Author{:id 223882, :first-name nil, :last-name nil, :full-name "Zwi Altman"}), :year 2009, :venue "ICC", :ncit 6, :string "Statistical Learning for Automated RRM: Application to eUTRAN Mobility.. Self Organizing Network (SON) functionalities are currently developed to improve network performance and management tasks. SON functionalities require efficient utilization of data extracted from the network. In this context, the paper has two objectives. First it is shown that one can use simple statistical learning techniques such as regression to extract a model from data. The model comprises closed form expressions that approximate the functional relations between measured Key Performance Indicators (KPIs) and Radio Resource Management (RRM) parameters. Second, it is shown how the model can be integrated in a monitoring process and be used to devise an efficient auto-tuning algorithm. To this end, two case studies of handover monitoring and handover auto-tuning in a LTE network are described and illustrate the application of the proposed approach.", :doc-id "Statistical Learning for Automated RRM: Application to eUTRAN Mobility. 2009 Moazzam Islam Tiwana, Berna Sayraç, Zwi Altman"}, 3382280 #search_api.search_api.Paper{:id 3382280, :key "journals/cogsci/ArciuliS12", :title "Statistical Learning Is Related to Reading Ability in Children and Adults.", :abstract nil, :author (#search_api.search_api.Author{:id 14233998, :first-name nil, :last-name nil, :full-name "Joanne Arciuli"} #search_api.search_api.Author{:id 14270159, :first-name nil, :last-name nil, :full-name "Ian C. Simpson"}), :year 2012, :venue "Cognitive Science", :ncit 0, :string "Statistical Learning Is Related to Reading Ability in Children and Adults.. ", :doc-id "Statistical Learning Is Related to Reading Ability in Children and Adults. 2012 Joanne Arciuli, Ian C. Simpson"}, 893160 #search_api.search_api.Paper{:id 893160, :key "journals/ida/CucchiaraMPR01", :title "An application of machine learning and statistics to defect detection.", :abstract "We present an application of machine learning and statistics to the problem of distinguishing between defective and non-defective industrial workpieces, where the defect takes the form of a long and thin crack on the surface of the piece. From the images of pieces a number of features are extracted by using the Hough transform and the Correlated Hough transform. Two datasets are considered, one containing only features related to the Hough transform and the other containing also features related to the Correlated Hough transform. On these datasets we have compared six different learning algorithms: an attribute-value learner, C4.5, a backpropagation neural network, NeuralWorks Predict, a k-nearest neighbour algorithm, and three statistical techniques, linear, logistic and quadratic discriminant. The experiments show that C4.5 performs best for both feature sets and gives an average accuracy of 93.3% for the first dataset and 95.9% for the second dataset.", :author (#search_api.search_api.Author{:id 394534, :first-name nil, :last-name nil, :full-name "Rita Cucchiara"} #search_api.search_api.Author{:id 1065974, :first-name nil, :last-name nil, :full-name "Paola Mello"} #search_api.search_api.Author{:id 893047, :first-name nil, :last-name nil, :full-name "Massimo Piccardi"} #search_api.search_api.Author{:id 1316705, :first-name nil, :last-name nil, :full-name "Fabrizio Riguzzi"}), :year 2001, :venue "Intell. Data Anal.", :ncit 3, :string "An application of machine learning and statistics to defect detection.. We present an application of machine learning and statistics to the problem of distinguishing between defective and non-defective industrial workpieces, where the defect takes the form of a long and thin crack on the surface of the piece. From the images of pieces a number of features are extracted by using the Hough transform and the Correlated Hough transform. Two datasets are considered, one containing only features related to the Hough transform and the other containing also features related to the Correlated Hough transform. On these datasets we have compared six different learning algorithms: an attribute-value learner, C4.5, a backpropagation neural network, NeuralWorks Predict, a k-nearest neighbour algorithm, and three statistical techniques, linear, logistic and quadratic discriminant. The experiments show that C4.5 performs best for both feature sets and gives an average accuracy of 93.3% for the first dataset and 95.9% for the second dataset.", :doc-id "An application of machine learning and statistics to defect detection. 2001 Rita Cucchiara, Paola Mello, Massimo Piccardi, Fabrizio Riguzzi"}, 2927016 #search_api.search_api.Paper{:id 2927016, :key "journals/sigkdd/Landwehr09", :title "Trading expressivity for efficiency in statistical relational learning: Ph.D. thesis abstract.", :abstract nil, :author (#search_api.search_api.Author{:id 127590, :first-name nil, :last-name nil, :full-name "Niels Landwehr"}), :year 2009, :venue "SIGKDD Explorations", :ncit 0, :string "Trading expressivity for efficiency in statistical relational learning: Ph.D. thesis abstract.. ", :doc-id "Trading expressivity for efficiency in statistical relational learning: Ph.D. thesis abstract. 2009 Niels Landwehr"}, 2881512 #search_api.search_api.Paper{:id 2881512, :key "journals/jocn/Turk-BrowneSCJ09", :title "Neural Evidence of Statistical Learning: Efficient Detection of Visual Regularities Without Awareness.", :abstract "Our environment contains regularities distributed in space and time that can be detected by way of statistical learning. This unsupervised learning occurs without intent or awareness, but little is known about how it relates to other types of learning, how it affects perceptual processing, and how quickly it can occur. Here we use fMRI during statistical learning to explore these questions. Participants viewed statistically structured versus unstructured sequences of shapes while performing a task unrelated to the structure. Robust neural responses to statistical structure were observed, and these responses were notable in four ways: First, responses to structure were observed in the striatum and medial temporal lobe, suggesting that statistical learning may be related to other forms of associative learning and relational memory. Second, statistical regularities yielded greater activation in category-specific visual regions (object-selective lateral occipital cortex and word-selective ventral occipito-temporal cortex), demonstrating that these regions are sensitive to information distributed in time. Third, evidence of learning emerged early during familiarization, showing that statistical learning can operate very quickly and with little exposure. Finally, neural signatures of learning were dissociable from subsequent explicit familiarity, suggesting that learning can occur in the absence of awareness. Overall, our findings help elucidate the underlying nature of statistical learning.", :author (#search_api.search_api.Author{:id 795446, :first-name nil, :last-name nil, :full-name "Nicholas B. Turk-Browne"} #search_api.search_api.Author{:id 1336480, :first-name nil, :last-name nil, :full-name "Brian J. Scholl"} #search_api.search_api.Author{:id 1113401, :first-name nil, :last-name nil, :full-name "Marvin M. Chun"} #search_api.search_api.Author{:id 778152, :first-name nil, :last-name nil, :full-name "Marcia K. Johnson"}), :year 2009, :venue "J. Cognitive Neuroscience", :ncit 25, :string "Neural Evidence of Statistical Learning: Efficient Detection of Visual Regularities Without Awareness.. Our environment contains regularities distributed in space and time that can be detected by way of statistical learning. This unsupervised learning occurs without intent or awareness, but little is known about how it relates to other types of learning, how it affects perceptual processing, and how quickly it can occur. Here we use fMRI during statistical learning to explore these questions. Participants viewed statistically structured versus unstructured sequences of shapes while performing a task unrelated to the structure. Robust neural responses to statistical structure were observed, and these responses were notable in four ways: First, responses to structure were observed in the striatum and medial temporal lobe, suggesting that statistical learning may be related to other forms of associative learning and relational memory. Second, statistical regularities yielded greater activation in category-specific visual regions (object-selective lateral occipital cortex and word-selective ventral occipito-temporal cortex), demonstrating that these regions are sensitive to information distributed in time. Third, evidence of learning emerged early during familiarization, showing that statistical learning can operate very quickly and with little exposure. Finally, neural signatures of learning were dissociable from subsequent explicit familiarity, suggesting that learning can occur in the absence of awareness. Overall, our findings help elucidate the underlying nature of statistical learning.", :doc-id "Neural Evidence of Statistical Learning: Efficient Detection of Visual Regularities Without Awareness. 2009 Nicholas B. Turk-Browne, Brian J. Scholl, Marvin M. Chun, Marcia K. Johnson"}, 473321 #search_api.search_api.Paper{:id 473321, :key "conf/kdd/SuchanekIW06", :title "Combining linguistic and statistical analysis to extract relations from web documents.", :abstract "The World Wide Web provides a nearly endless source of knowledge, which is mostly given in natural language. A first step towards exploiting this data automatically could be to extract pairs of a given semantic relation from text documents - for example all pairs of a person and her birthdate. One strategy for this task is to find text patterns that express the semantic relation, to generalize these patterns, and to apply them to a corpus to find new pairs. In this paper, we show that this approach profits significantly when deep linguistic structures are used instead of surface text patterns. We demonstrate how linguistic structures can be represented for machine learning, and we provide a theoretical analysis of the pattern matching approach. We show the benefits of our approach by extensive experiments with our prototype system LEILA.", :author (#search_api.search_api.Author{:id 793922, :first-name nil, :last-name nil, :full-name "Fabian M. Suchanek"} #search_api.search_api.Author{:id 528047, :first-name nil, :last-name nil, :full-name "Georgiana Ifrim"} #search_api.search_api.Author{:id 1015097, :first-name nil, :last-name nil, :full-name "Gerhard Weikum"}), :year 2006, :venue "KDD", :ncit 97, :string "Combining linguistic and statistical analysis to extract relations from web documents.. The World Wide Web provides a nearly endless source of knowledge, which is mostly given in natural language. A first step towards exploiting this data automatically could be to extract pairs of a given semantic relation from text documents - for example all pairs of a person and her birthdate. One strategy for this task is to find text patterns that express the semantic relation, to generalize these patterns, and to apply them to a corpus to find new pairs. In this paper, we show that this approach profits significantly when deep linguistic structures are used instead of surface text patterns. We demonstrate how linguistic structures can be represented for machine learning, and we provide a theoretical analysis of the pattern matching approach. We show the benefits of our approach by extensive experiments with our prototype system LEILA.", :doc-id "Combining linguistic and statistical analysis to extract relations from web documents. 2006 Fabian M. Suchanek, Georgiana Ifrim, Gerhard Weikum"}, 91401 #search_api.search_api.Paper{:id 91401, :key "conf/chi/PatelFLH08", :title "Investigating statistical machine learning as a tool for software development.", :abstract "As statistical machine learning algorithms and techniques continue to mature, many researchers and developers see statistical machine learning not only as a topic of expert study, but also as a tool for software development. Extensive prior work has studied software development, but little prior work has studied software developers applying statistical machine learning. This paper presents interviews of eleven researchers experienced in applying statistical machine learning algorithms and techniques to human-computer interaction problems, as well as a study of ten participants working during a five-hour study to apply statistical machine learning algorithms and techniques to a realistic problem. We distill three related categories of difficulties that arise in applying statistical machine learning as a tool for software development: (1) difficulty pursuing statistical machine learning as an iterative and exploratory process, (2) difficulty understanding relationships between data and the behavior of statistical machine learning algorithms, and (3) difficulty evaluating the performance of statistical machine learning algorithms and techniques in the context of applications. This paper provides important new insight into these difficulties and the need for development tools that better support the application of statistical machine learning.", :author (#search_api.search_api.Author{:id 939325, :first-name nil, :last-name nil, :full-name "Kayur Patel"} #search_api.search_api.Author{:id 1505625, :first-name nil, :last-name nil, :full-name "James Fogarty"} #search_api.search_api.Author{:id 82771, :first-name nil, :last-name nil, :full-name "James A. Landay"} #search_api.search_api.Author{:id 907542, :first-name nil, :last-name nil, :full-name "Beverly L. Harrison"}), :year 2008, :venue "CHI", :ncit 22, :string "Investigating statistical machine learning as a tool for software development.. As statistical machine learning algorithms and techniques continue to mature, many researchers and developers see statistical machine learning not only as a topic of expert study, but also as a tool for software development. Extensive prior work has studied software development, but little prior work has studied software developers applying statistical machine learning. This paper presents interviews of eleven researchers experienced in applying statistical machine learning algorithms and techniques to human-computer interaction problems, as well as a study of ten participants working during a five-hour study to apply statistical machine learning algorithms and techniques to a realistic problem. We distill three related categories of difficulties that arise in applying statistical machine learning as a tool for software development: (1) difficulty pursuing statistical machine learning as an iterative and exploratory process, (2) difficulty understanding relationships between data and the behavior of statistical machine learning algorithms, and (3) difficulty evaluating the performance of statistical machine learning algorithms and techniques in the context of applications. This paper provides important new insight into these difficulties and the need for development tools that better support the application of statistical machine learning.", :doc-id "Investigating statistical machine learning as a tool for software development. 2008 Kayur Patel, James Fogarty, James A. Landay, Beverly L. Harrison"}, 472937 #search_api.search_api.Paper{:id 472937, :key "conf/kdd/Koller03", :title "Statistical learning from relational data.", :abstract nil, :author (#search_api.search_api.Author{:id 560995, :first-name nil, :last-name nil, :full-name "Daphne Koller"}), :year 2003, :venue "KDD", :ncit 0, :string "Statistical learning from relational data.. ", :doc-id "Statistical learning from relational data. 2003 Daphne Koller"}, 3786794 #search_api.search_api.Paper{:id 3786794, :key "conf/fusion/JandelSW12", :title "Online learnability of Statistical Relational Learning in anomaly detection.", :abstract nil, :author (#search_api.search_api.Author{:id 1344508, :first-name nil, :last-name nil, :full-name "Magnus Jändel"} #search_api.search_api.Author{:id 1319563, :first-name nil, :last-name nil, :full-name "Pontus Svenson"} #search_api.search_api.Author{:id 14343624, :first-name nil, :last-name nil, :full-name "Niclas Wadströmer"}), :year 2012, :venue "FUSION", :ncit 0, :string "Online learnability of Statistical Relational Learning in anomaly detection.. ", :doc-id "Online learnability of Statistical Relational Learning in anomaly detection. 2012 Magnus Jändel, Pontus Svenson, Niclas Wadströmer"}, 2927018 #search_api.search_api.Paper{:id 2927018, :key "journals/sigkdd/RamonCFK09", :title "StReBio'09: statistical relational learning and mining in bioinformatics.", :abstract nil, :author (#search_api.search_api.Author{:id 1015109, :first-name nil, :last-name nil, :full-name "Jan Ramon"} #search_api.search_api.Author{:id 892400, :first-name nil, :last-name nil, :full-name "Fabrizio Costa"} #search_api.search_api.Author{:id 981793, :first-name nil, :last-name nil, :full-name "Christophe Costa Florêncio"} #search_api.search_api.Author{:id 19675, :first-name nil, :last-name nil, :full-name "Joost N. Kok"}), :year 2009, :venue "SIGKDD Explorations", :ncit 0, :string "StReBio'09: statistical relational learning and mining in bioinformatics.. ", :doc-id "StReBio'09: statistical relational learning and mining in bioinformatics. 2009 Jan Ramon, Fabrizio Costa, Christophe Costa Florêncio, Joost N. Kok"}, 950858 #search_api.search_api.Paper{:id 950858, :key "journals/jacm/Kearns98", :title "Efficient Noise-Tolerant Learning from Statistical Queries.", :abstract "In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :author (#search_api.search_api.Author{:id 1249307, :first-name nil, :last-name nil, :full-name "Michael J. Kearns"}), :year 1998, :venue "J. ACM", :ncit 456, :string "Efficient Noise-Tolerant Learning from Statistical Queries.. In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :doc-id "Efficient Noise-Tolerant Learning from Statistical Queries. 1998 Michael J. Kearns"}, 3258026 #search_api.search_api.Paper{:id 3258026, :key "journals/ml/RiguzziM12", :title "Applying the information bottleneck to statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 1316705, :first-name nil, :last-name nil, :full-name "Fabrizio Riguzzi"} #search_api.search_api.Author{:id 1294122, :first-name nil, :last-name nil, :full-name "Nicola Di Mauro"}), :year 2012, :venue "Machine Learning", :ncit 14, :string "Applying the information bottleneck to statistical relational learning.. ", :doc-id "Applying the information bottleneck to statistical relational learning. 2012 Fabrizio Riguzzi, Nicola Di Mauro"}, 1288331 #search_api.search_api.Paper{:id 1288331, :key "journals/sadm/SundararaghavanZ09", :title "A statistical learning approach for the design of polycrystalline materials.", :abstract "Important physical properties such as yield strength, elastic modulus, and thermal conductivity depend on the material microstructure. Realization of optimal microstructures is important for hardware components in aerospace applications where there is a need to optimize material properties for improved performance. Microstructures can be tailored through controlled deformation or heat treatment. However, identification of the optimal processing path is a non-trivial (and non-unique) problem. Data-mining techniques are eminently suitable for process design since optimal processing paths can be selected based on available information from a large database-relating processes, properties, and microstructures. In this paper, the problem of designing processing stages that lead to a desired microstructure or material property is addressed by mining over a database of microstructural signatures. A hierarchical X-means classifier is designed to match crystallographic orientation features to a class of microstructural signatures within a database. Instead of the conventional distortion minimization algorithm of k-means, X-means maximizes a Bayesian information measure for calculating cluster centers which allows automatic detection of number of classes. Using the microstructural database, an adaptive data-compression technique based on proper orthogonal decomposition (POD) has been designed to accelerate materials design. In this technique, reduced modes selected adaptively from the database are used to speed up auxiliary microstructure optimization algorithms built over the database. Copyright &copy; 2009 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 1: 000-000, 2009", :author (#search_api.search_api.Author{:id 1415860, :first-name nil, :last-name nil, :full-name "Veera Sundararaghavan"} #search_api.search_api.Author{:id 585807, :first-name nil, :last-name nil, :full-name "Nicholas Zabaras"}), :year 2009, :venue "Statistical Analysis and Data Mining", :ncit 3, :string "A statistical learning approach for the design of polycrystalline materials.. Important physical properties such as yield strength, elastic modulus, and thermal conductivity depend on the material microstructure. Realization of optimal microstructures is important for hardware components in aerospace applications where there is a need to optimize material properties for improved performance. Microstructures can be tailored through controlled deformation or heat treatment. However, identification of the optimal processing path is a non-trivial (and non-unique) problem. Data-mining techniques are eminently suitable for process design since optimal processing paths can be selected based on available information from a large database-relating processes, properties, and microstructures. In this paper, the problem of designing processing stages that lead to a desired microstructure or material property is addressed by mining over a database of microstructural signatures. A hierarchical X-means classifier is designed to match crystallographic orientation features to a class of microstructural signatures within a database. Instead of the conventional distortion minimization algorithm of k-means, X-means maximizes a Bayesian information measure for calculating cluster centers which allows automatic detection of number of classes. Using the microstructural database, an adaptive data-compression technique based on proper orthogonal decomposition (POD) has been designed to accelerate materials design. In this technique, reduced modes selected adaptively from the database are used to speed up auxiliary microstructure optimization algorithms built over the database. Copyright &copy; 2009 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 1: 000-000, 2009", :doc-id "A statistical learning approach for the design of polycrystalline materials. 2009 Veera Sundararaghavan, Nicholas Zabaras"}, 797516 #search_api.search_api.Paper{:id 797516, :key "journals/coling/OchN04", :title "The Alignment Template Approach to Statistical Machine Translation.", :abstract "A phrase-based statistical machine translation approach &mdash; the alignment template approach &mdash; is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source&ndash;channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German&ndash;English speech VERBMOBIL task, we analyze the effect of various system components. On the French&ndash;English Canadian HANSARDS task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese&ndash;English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.", :author (#search_api.search_api.Author{:id 1493982, :first-name nil, :last-name nil, :full-name "Franz Josef Och"} #search_api.search_api.Author{:id 526052, :first-name nil, :last-name nil, :full-name "Hermann Ney"}), :year 2004, :venue "Computational Linguistics", :ncit 644, :string "The Alignment Template Approach to Statistical Machine Translation.. A phrase-based statistical machine translation approach &mdash; the alignment template approach &mdash; is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source&ndash;channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German&ndash;English speech VERBMOBIL task, we analyze the effect of various system components. On the French&ndash;English Canadian HANSARDS task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese&ndash;English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.", :doc-id "The Alignment Template Approach to Statistical Machine Translation. 2004 Franz Josef Och, Hermann Ney"}, 3074924 #search_api.search_api.Paper{:id 3074924, :key "journals/ml/RettingerNT11", :title "Statistical relational learning of trust.", :abstract nil, :author (#search_api.search_api.Author{:id 444383, :first-name nil, :last-name nil, :full-name "Achim Rettinger"} #search_api.search_api.Author{:id 222492, :first-name nil, :last-name nil, :full-name "Matthias Nickles"} #search_api.search_api.Author{:id 680469, :first-name nil, :last-name nil, :full-name "Volker Tresp"}), :year 2011, :venue "Machine Learning", :ncit 6, :string "Statistical relational learning of trust.. ", :doc-id "Statistical relational learning of trust. 2011 Achim Rettinger, Matthias Nickles, Volker Tresp"}, 1185901 #search_api.search_api.Paper{:id 1185901, :key "journals/jocn/MuellerBF08", :title "The Role of Pause Cues in Language Learning: The Emergence of Event-related Potentials Related to Sequence Processing.", :abstract "Humans can derive sequential dependencies from unfamiliar artificial speech within several of minutes of exposure. However, there is an ongoing debate about the nature of the underlying learning processes. In a widely discussed study Pea et al. [Pea, M., Bonatti, L. L., Nespor, M., & Mehler, J. Signal-driven computations in speech processing. Science, 298, 604607, 2002] argued for the importance of subtle acoustic cues in the signal, such as pauses, in order to switch between two computational mechanisms, which are conceptualized as rule-based versus statistical. The present study was aimed to approach this problem by recording event-related potentials in response to correct and incorrect phrases consisting of bisyllabics after short exposure to either rule-based or random artificial speech streams. Rule-based streams contained dependencies of the form AXC, whereby A elements reliably predicted the C elements and X elements were variable. Participants were exposed to four input and test phases. Two of the input streams were rule-based and contained either only probabilistic information related to the distribution of the AXC stimuli or an additional acoustic cue indicating the boundaries of relevant units. The other two streams were random variations of the rule-based streams. During the test phase in the condition with pause cues, an early negativity and a later positivity emerged for correct and incorrect items in comparison to their acoustically identical counterparts, which were presented after the random control condition. In the noncued condition, only negativities were seen. The timing and the scalp distribution of the negativities were different for correct and incorrect sequences in both the cued and the noncued conditions. The results are interpreted in support of a view of grammatical learning in which both distributional and acoustic cues may contribute to different aspects of syntactic learning.", :author (#search_api.search_api.Author{:id 983626, :first-name nil, :last-name nil, :full-name "Jutta L. Mueller"} #search_api.search_api.Author{:id -1, :first-name nil, :last-name nil, :full-name "Jörg Bahlmann"} #search_api.search_api.Author{:id 258614, :first-name nil, :last-name nil, :full-name "Angela D. Friederici"}), :year 2008, :venue "J. Cognitive Neuroscience", :ncit 16, :string "The Role of Pause Cues in Language Learning: The Emergence of Event-related Potentials Related to Sequence Processing.. Humans can derive sequential dependencies from unfamiliar artificial speech within several of minutes of exposure. However, there is an ongoing debate about the nature of the underlying learning processes. In a widely discussed study Pea et al. [Pea, M., Bonatti, L. L., Nespor, M., & Mehler, J. Signal-driven computations in speech processing. Science, 298, 604607, 2002] argued for the importance of subtle acoustic cues in the signal, such as pauses, in order to switch between two computational mechanisms, which are conceptualized as rule-based versus statistical. The present study was aimed to approach this problem by recording event-related potentials in response to correct and incorrect phrases consisting of bisyllabics after short exposure to either rule-based or random artificial speech streams. Rule-based streams contained dependencies of the form AXC, whereby A elements reliably predicted the C elements and X elements were variable. Participants were exposed to four input and test phases. Two of the input streams were rule-based and contained either only probabilistic information related to the distribution of the AXC stimuli or an additional acoustic cue indicating the boundaries of relevant units. The other two streams were random variations of the rule-based streams. During the test phase in the condition with pause cues, an early negativity and a later positivity emerged for correct and incorrect items in comparison to their acoustically identical counterparts, which were presented after the random control condition. In the noncued condition, only negativities were seen. The timing and the scalp distribution of the negativities were different for correct and incorrect sequences in both the cued and the noncued conditions. The results are interpreted in support of a view of grammatical learning in which both distributional and acoustic cues may contribute to different aspects of syntactic learning.", :doc-id "The Role of Pause Cues in Language Learning: The Emergence of Event-related Potentials Related to Sequence Processing. 2008 Jutta L. Mueller, Jörg Bahlmann, Angela D. Friederici"}, 1028237 #search_api.search_api.Paper{:id 1028237, :key "journals/npl/LuoUN99", :title "Unsupervised Learning of Higher-Order Statistics.", :abstract "This paper deals with the adaptive extraction of higher-order statistics of related signals. We will show how to use higher-order neural networks to adaptively extract the higher-order cumulant matrices and tensors with an invariant weight norm. This proposed scheme can serve as an alternative tool in many application fields with higher-order statistics.", :author (#search_api.search_api.Author{:id 658006, :first-name nil, :last-name nil, :full-name "Fa-Long Luo"} #search_api.search_api.Author{:id 416876, :first-name nil, :last-name nil, :full-name "Rolf Unbehauen"} #search_api.search_api.Author{:id 1370072, :first-name nil, :last-name nil, :full-name "Tertulien Ndjountche"}), :year 1999, :venue "Neural Processing Letters", :ncit 0, :string "Unsupervised Learning of Higher-Order Statistics.. This paper deals with the adaptive extraction of higher-order statistics of related signals. We will show how to use higher-order neural networks to adaptively extract the higher-order cumulant matrices and tensors with an invariant weight norm. This proposed scheme can serve as an alternative tool in many application fields with higher-order statistics.", :doc-id "Unsupervised Learning of Higher-Order Statistics. 1999 Fa-Long Luo, Rolf Unbehauen, Tertulien Ndjountche"}, 20109 #search_api.search_api.Paper{:id 20109, :key "conf/acl/Chiang05", :title "A Hierarchical Phrase-Based Model for Statistical Machine Translation.", :abstract "We present a statistical phrase-based translation model that uses hierarchical phrases---phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.", :author (#search_api.search_api.Author{:id 415922, :first-name nil, :last-name nil, :full-name "David Chiang"}), :year 2005, :venue "ACL", :ncit 739, :string "A Hierarchical Phrase-Based Model for Statistical Machine Translation.. We present a statistical phrase-based translation model that uses hierarchical phrases---phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.", :doc-id "A Hierarchical Phrase-Based Model for Statistical Machine Translation. 2005 David Chiang"}, 541902 #search_api.search_api.Paper{:id 541902, :key "conf/pkdd/Raedt05", :title "Statistical Relational Learning: An Inductive Logic Programming Perspective.", :abstract nil, :author (#search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2005, :venue "PKDD", :ncit 3, :string "Statistical Relational Learning: An Inductive Logic Programming Perspective.. ", :doc-id "Statistical Relational Learning: An Inductive Logic Programming Perspective. 2005 Luc De Raedt"}, 1010126 #search_api.search_api.Paper{:id 1010126, :key "journals/ml/DietterichDGMT08", :title "Structured machine learning: the next ten years.", :abstract "The field of inductive logic programming (ILP) has made steady progress, since the first ILP workshop in 1991, based on a balance of developments in theory, implementations and applications. More recently there has been an increased emphasis on Probabilistic ILP and the related fields of Statistical Relational Learning (SRL) and Structured Prediction. The goal of the current paper is to consider these emerging trends and chart out the strategic directions and open problems for the broader area of structured machine learning for the next 10 years.", :author (#search_api.search_api.Author{:id 737377, :first-name nil, :last-name nil, :full-name "Thomas G. Dietterich"} #search_api.search_api.Author{:id 668085, :first-name nil, :last-name nil, :full-name "Pedro Domingos"} #search_api.search_api.Author{:id 1449720, :first-name nil, :last-name nil, :full-name "Lise Getoor"} #search_api.search_api.Author{:id 120131, :first-name nil, :last-name nil, :full-name "Stephen Muggleton"} #search_api.search_api.Author{:id 435675, :first-name nil, :last-name nil, :full-name "Prasad Tadepalli"}), :year 2008, :venue "Machine Learning", :ncit 42, :string "Structured machine learning: the next ten years.. The field of inductive logic programming (ILP) has made steady progress, since the first ILP workshop in 1991, based on a balance of developments in theory, implementations and applications. More recently there has been an increased emphasis on Probabilistic ILP and the related fields of Statistical Relational Learning (SRL) and Structured Prediction. The goal of the current paper is to consider these emerging trends and chart out the strategic directions and open problems for the broader area of structured machine learning for the next 10 years.", :doc-id "Structured machine learning: the next ten years. 2008 Thomas G. Dietterich, Pedro Domingos, Lise Getoor, Stephen Muggleton, Prasad Tadepalli"}, 1009230 #search_api.search_api.Paper{:id 1009230, :key "journals/ml/BrazdilSC03", :title "Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results.", :abstract "We present a meta-learning method to support selection of candidate learning algorithms. It uses a k-Nearest Neighbor algorithm to identify the datasets that are most similar to the one at hand. The distance between datasets is assessed using a relatively small set of data characteristics, which was selected to represent properties that affect algorithm performance. The performance of the candidate algorithms on those datasets is used to generate a recommendation to the user in the form of a ranking. The performance is assessed using a multicriteria evaluation measure that takes not only accuracy, but also time into account. As it is not common in Machine Learning to work with rankings, we had to identify and adapt existing statistical techniques to devise an appropriate evaluation methodology. Using that methodology, we show that the meta-learning method presented leads to significantly better rankings than the baseline ranking method. The evaluation methodology is general and can be adapted to other ranking problems. Although here we have concentrated on ranking classification algorithms, the meta-learning framework presented can provide assistance in the selection of combinations of methods or more complex problem solving strategies.", :author (#search_api.search_api.Author{:id 1157783, :first-name nil, :last-name nil, :full-name "Pavel Brazdil"} #search_api.search_api.Author{:id 791367, :first-name nil, :last-name nil, :full-name "Carlos Soares"} #search_api.search_api.Author{:id 212848, :first-name nil, :last-name nil, :full-name "Joaquim Pinto da Costa"}), :year 2003, :venue "Machine Learning", :ncit 200, :string "Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results.. We present a meta-learning method to support selection of candidate learning algorithms. It uses a k-Nearest Neighbor algorithm to identify the datasets that are most similar to the one at hand. The distance between datasets is assessed using a relatively small set of data characteristics, which was selected to represent properties that affect algorithm performance. The performance of the candidate algorithms on those datasets is used to generate a recommendation to the user in the form of a ranking. The performance is assessed using a multicriteria evaluation measure that takes not only accuracy, but also time into account. As it is not common in Machine Learning to work with rankings, we had to identify and adapt existing statistical techniques to devise an appropriate evaluation methodology. Using that methodology, we show that the meta-learning method presented leads to significantly better rankings than the baseline ranking method. The evaluation methodology is general and can be adapted to other ranking problems. Although here we have concentrated on ranking classification algorithms, the meta-learning framework presented can provide assistance in the selection of combinations of methods or more complex problem solving strategies.", :doc-id "Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results. 2003 Pavel Brazdil, Carlos Soares, Joaquim Pinto da Costa"}, 473199 #search_api.search_api.Paper{:id 473199, :key "conf/kdd/PopesculU04", :title "Cluster-based concept invention for statistical relational learning.", :abstract "We use clustering to derive new relations which augment database schema used in automatic generation of predictive features in statistical relational learning. Entities derived from clusters increase the expressivity of feature spaces by creating new first-class concepts which contribute to the creation of new features. For example, in CiteSeer, papers can be clustered based on words or citations giving \"topics\", and authors can be clustered based on documents they co-author giving \"communities\". Such cluster-derived concepts become part of more complex feature expressions. Out of the large number of generated features, those which improve predictive accuracy are kept in the model, as decided by statistical feature selection criteria. We present results demonstrating improved accuracy on two tasks, venue prediction and link prediction, using CiteSeer data.", :author (#search_api.search_api.Author{:id 597076, :first-name nil, :last-name nil, :full-name "Alexandrin Popescul"} #search_api.search_api.Author{:id 352656, :first-name nil, :last-name nil, :full-name "Lyle H. Ungar"}), :year 2004, :venue "KDD", :ncit 31, :string "Cluster-based concept invention for statistical relational learning.. We use clustering to derive new relations which augment database schema used in automatic generation of predictive features in statistical relational learning. Entities derived from clusters increase the expressivity of feature spaces by creating new first-class concepts which contribute to the creation of new features. For example, in CiteSeer, papers can be clustered based on words or citations giving \"topics\", and authors can be clustered based on documents they co-author giving \"communities\". Such cluster-derived concepts become part of more complex feature expressions. Out of the large number of generated features, those which improve predictive accuracy are kept in the model, as decided by statistical feature selection criteria. We present results demonstrating improved accuracy on two tasks, venue prediction and link prediction, using CiteSeer data.", :doc-id "Cluster-based concept invention for statistical relational learning. 2004 Alexandrin Popescul, Lyle H. Ungar"}, 472783 #search_api.search_api.Paper{:id 472783, :key "conf/kdd/HeckermanGC94", :title "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.", :abstract "We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :author (#search_api.search_api.Author{:id 114083, :first-name nil, :last-name nil, :full-name "David Heckerman"} #search_api.search_api.Author{:id 330355, :first-name nil, :last-name nil, :full-name "Dan Geiger"} #search_api.search_api.Author{:id 1215050, :first-name nil, :last-name nil, :full-name "David Maxwell Chickering"}), :year 1994, :venue "KDD Workshop", :ncit 3837, :string "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.. We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :doc-id "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data. 1994 David Heckerman, Dan Geiger, David Maxwell Chickering"}, 952368 #search_api.search_api.Paper{:id 952368, :key "journals/jair/DaumeM06", :title "Domain Adaptation for Statistical Classifiers.", :abstract "The most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution. Unfortunately, in many applications, the \"in-domain\" test data is drawn from a distribution that is related, but not identical, to the \"out-of-domain\" distribution of the training data. We consider the common case in which labeled out-of-domain data is plentiful, but labeled in-domain data is scarce. We introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts. We present efficient inference algorithms for this special case based on the technique of conditional expectation maximization. Our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain.", :author (#search_api.search_api.Author{:id 820633, :first-name nil, :last-name nil, :full-name "Hal Daumé III"} #search_api.search_api.Author{:id 911330, :first-name nil, :last-name nil, :full-name "Daniel Marcu"}), :year 2006, :venue "J. Artif. Intell. Res. (JAIR)", :ncit 250, :string "Domain Adaptation for Statistical Classifiers.. The most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution. Unfortunately, in many applications, the \"in-domain\" test data is drawn from a distribution that is related, but not identical, to the \"out-of-domain\" distribution of the training data. We consider the common case in which labeled out-of-domain data is plentiful, but labeled in-domain data is scarce. We introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts. We present efficient inference algorithms for this special case based on the technique of conditional expectation maximization. Our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain.", :doc-id "Domain Adaptation for Statistical Classifiers. 2006 Hal Daumé III, Daniel Marcu"}, 2775184 #search_api.search_api.Paper{:id 2775184, :key "conf/ai/KhosraviB10", :title "A Survey on Statistical Relational Learning.", :abstract "Frequent subgraph mining is an active research topic in the data mining community. A graph is a general model to represent data and has been used in many domains like cheminformatics and bioinformatics. Mining patterns from graph databases is challenging since graph related operations, such as subgraph testing, generally have higher time complexity than the corresponding operations on itemsets, sequences, and trees, which have been studied extensively. We propose a novel frequent subgraph mining algorithm: FFSM, which employs a vertical search scheme within an algebraic graph framework we have developed to reduce the number of redundant candidates proposed. Our empirical study on synthetic and real datasets demonstrates that FFSM achieves a substantial performance gain over the current start-of-the-art subgraph mining algorithm gSpan.", :author (#search_api.search_api.Author{:id 133910, :first-name nil, :last-name nil, :full-name "Hassan Khosravi"} #search_api.search_api.Author{:id 84842, :first-name nil, :last-name nil, :full-name "Bahareh Bina"}), :year 2010, :venue "Canadian Conference on AI", :ncit 1, :string "A Survey on Statistical Relational Learning.. Frequent subgraph mining is an active research topic in the data mining community. A graph is a general model to represent data and has been used in many domains like cheminformatics and bioinformatics. Mining patterns from graph databases is challenging since graph related operations, such as subgraph testing, generally have higher time complexity than the corresponding operations on itemsets, sequences, and trees, which have been studied extensively. We propose a novel frequent subgraph mining algorithm: FFSM, which employs a vertical search scheme within an algebraic graph framework we have developed to reduce the number of redundant candidates proposed. Our empirical study on synthetic and real datasets demonstrates that FFSM achieves a substantial performance gain over the current start-of-the-art subgraph mining algorithm gSpan.", :doc-id "A Survey on Statistical Relational Learning. 2010 Hassan Khosravi, Bahareh Bina"}, 1319504 #search_api.search_api.Paper{:id 1319504, :key "conf/icdm/NevilleGE09", :title "Evaluating Statistical Tests for Within-Network Classifiers of Relational Data.", :abstract "Recently a number of modeling techniques have been developed for data mining and machine learning in relational and network domains where the instances are not independent and identically distributed (i.i.d.). These methods specifically exploit the statistical dependencies among instances in order to improve classification accuracy. However, there has been little focus on how these same dependencies affect our ability to draw accurate conclusions about the performance of the models. More specifically, the complex link structure and attribute dependencies in network data violate the assumptions of many conventional statistical tests and make it difficult to use these tests to assess the models in an unbiased manner. In this work, we examine the task of within-network classification and the question of whether two algorithms will learn models which will result in significantly different levels of performance. We show that the commonly-used form of evaluation (paired t-test on overlapping network samples) can result in an unacceptable level of Type I error. Furthermore we show that Type I error increases as (1) the correlation among instances increases and (2) the size of the evaluation set increases (i.e., the proportion of labeled nodes in the network decreases). We propose a method for network cross-validation that combined with paired t-tests produces more acceptable levels of Type I error while still providing reasonable levels of statistical power (i.e., Type II error).", :author (#search_api.search_api.Author{:id 150768, :first-name nil, :last-name nil, :full-name "Jennifer Neville"} #search_api.search_api.Author{:id 335801, :first-name nil, :last-name nil, :full-name "Brian Gallagher"} #search_api.search_api.Author{:id 806969, :first-name nil, :last-name nil, :full-name "Tina Eliassi-Rad"}), :year 2009, :venue "ICDM", :ncit 12, :string "Evaluating Statistical Tests for Within-Network Classifiers of Relational Data.. Recently a number of modeling techniques have been developed for data mining and machine learning in relational and network domains where the instances are not independent and identically distributed (i.i.d.). These methods specifically exploit the statistical dependencies among instances in order to improve classification accuracy. However, there has been little focus on how these same dependencies affect our ability to draw accurate conclusions about the performance of the models. More specifically, the complex link structure and attribute dependencies in network data violate the assumptions of many conventional statistical tests and make it difficult to use these tests to assess the models in an unbiased manner. In this work, we examine the task of within-network classification and the question of whether two algorithms will learn models which will result in significantly different levels of performance. We show that the commonly-used form of evaluation (paired t-test on overlapping network samples) can result in an unacceptable level of Type I error. Furthermore we show that Type I error increases as (1) the correlation among instances increases and (2) the size of the evaluation set increases (i.e., the proportion of labeled nodes in the network decreases). We propose a method for network cross-validation that combined with paired t-tests produces more acceptable levels of Type I error while still providing reasonable levels of statistical power (i.e., Type II error).", :doc-id "Evaluating Statistical Tests for Within-Network Classifiers of Relational Data. 2009 Jennifer Neville, Brian Gallagher, Tina Eliassi-Rad"}, 190192 #search_api.search_api.Paper{:id 190192, :key "conf/esws/KieferBL08", :title "Adding Data Mining Support to SPARQL Via Statistical Relational Learning Methods.", :abstract "Exploiting the complex structure of relational data enables to build better models by taking into account the additional information provided by the links between objects. We extend this idea to the Semantic Web by introducing our novel SPARQL-ML approach to perform data mining for Semantic Web data. Our approach is based on traditional SPARQL and statistical relational learning methods, such as Relational Probability Trees and Relational Bayesian Classifiers. We analyze our approach thoroughly conducting three sets of experiments on synthetic as well as real-world data sets. Our analytical results show that our approach can be used for any Semantic Web data set to perform instance-based learning and classification. A comparison to kernel methods used in Support Vector Machines shows that our approach is superior in terms of classification accuracy.", :author (#search_api.search_api.Author{:id 1506617, :first-name nil, :last-name nil, :full-name "Christoph Kiefer"} #search_api.search_api.Author{:id 798825, :first-name nil, :last-name nil, :full-name "Abraham Bernstein"} #search_api.search_api.Author{:id 322034, :first-name nil, :last-name nil, :full-name "André Locher"}), :year 2008, :venue "ESWC", :ncit 26, :string "Adding Data Mining Support to SPARQL Via Statistical Relational Learning Methods.. Exploiting the complex structure of relational data enables to build better models by taking into account the additional information provided by the links between objects. We extend this idea to the Semantic Web by introducing our novel SPARQL-ML approach to perform data mining for Semantic Web data. Our approach is based on traditional SPARQL and statistical relational learning methods, such as Relational Probability Trees and Relational Bayesian Classifiers. We analyze our approach thoroughly conducting three sets of experiments on synthetic as well as real-world data sets. Our analytical results show that our approach can be used for any Semantic Web data set to perform instance-based learning and classification. A comparison to kernel methods used in Support Vector Machines shows that our approach is superior in terms of classification accuracy.", :doc-id "Adding Data Mining Support to SPARQL Via Statistical Relational Learning Methods. 2008 Christoph Kiefer, Abraham Bernstein, André Locher"}, 1279856 #search_api.search_api.Paper{:id 1279856, :key "journals/neco/AmariM93", :title "Statistical Theory of Learning Curves under Entropic Loss Criterion.", :abstract "The present paper elucidates a universal property of learning curves, which shows how the generalization error, training error, and the complexity of the underlying stochastic machine are related and how the behavior of a stochastic machine is improved as the number of training examples increases. The error is measured by the entropic loss. It is proved that the generalization error converges to H0, the entropy of the conditional distribution of the true machine, as H0 + m*/(2t), while the training error converges as H0-m*/(2t), where t is the number of examples and m* shows the complexity of the network. When the model is faithful, implying that the true machine is in the model, m* is reduced to m, the number of modifiable parameters. This is a universal law because it holds for any regular machine irrespective of its structure under the maximum likelihood estimator. Similar relations are obtained for the Bayes and Gibbs learning algorithms. These learning curves show the relation among the accuracy of learning, the complexity of a model, and the number of training examples.", :author (#search_api.search_api.Author{:id 99365, :first-name nil, :last-name nil, :full-name "Shun-ichi Amari"} #search_api.search_api.Author{:id 918942, :first-name nil, :last-name nil, :full-name "Noboru Murata"}), :year 1993, :venue "Neural Computation", :ncit 143, :string "Statistical Theory of Learning Curves under Entropic Loss Criterion.. The present paper elucidates a universal property of learning curves, which shows how the generalization error, training error, and the complexity of the underlying stochastic machine are related and how the behavior of a stochastic machine is improved as the number of training examples increases. The error is measured by the entropic loss. It is proved that the generalization error converges to H0, the entropy of the conditional distribution of the true machine, as H0 + m*/(2t), while the training error converges as H0-m*/(2t), where t is the number of examples and m* shows the complexity of the network. When the model is faithful, implying that the true machine is in the model, m* is reduced to m, the number of modifiable parameters. This is a universal law because it holds for any regular machine irrespective of its structure under the maximum likelihood estimator. Similar relations are obtained for the Bayes and Gibbs learning algorithms. These learning curves show the relation among the accuracy of learning, the complexity of a model, and the number of training examples.", :doc-id "Statistical Theory of Learning Curves under Entropic Loss Criterion. 1993 Shun-ichi Amari, Noboru Murata"}, 498833 #search_api.search_api.Paper{:id 498833, :key "conf/miccai/UnalNSF08", :title "Customized Design of Hearing Aids Using Statistical Shape Learning.", :abstract "3D shape modeling is a crucial component of rapid prototyping systems that customize shapes of implants and prosthetic devices to a patient's anatomy. In this paper, we present a solution to the problem of customized 3D shape modeling using a statistical shape analysis framework. We design a novel method to learn the relationship between two classes of shapes, which are related by certain operations or transformation. The two associated shape classes are represented in a lower dimensional manifold, and the reduced set of parameters obtained in this subspace is utilized in an estimation, which is exemplified by a multivariate regression in this paper. We demonstrate our method with a felicitous application to estimation of customized hearing aid devices.", :author (#search_api.search_api.Author{:id 987788, :first-name nil, :last-name nil, :full-name "Gozde B. Unal"} #search_api.search_api.Author{:id 1388067, :first-name nil, :last-name nil, :full-name "Delphine Nain"} #search_api.search_api.Author{:id 174234, :first-name nil, :last-name nil, :full-name "Gregory G. Slabaugh"} #search_api.search_api.Author{:id 303194, :first-name nil, :last-name nil, :full-name "Tong Fang"}), :year 2008, :venue "MICCAI (1)", :ncit 6, :string "Customized Design of Hearing Aids Using Statistical Shape Learning.. 3D shape modeling is a crucial component of rapid prototyping systems that customize shapes of implants and prosthetic devices to a patient's anatomy. In this paper, we present a solution to the problem of customized 3D shape modeling using a statistical shape analysis framework. We design a novel method to learn the relationship between two classes of shapes, which are related by certain operations or transformation. The two associated shape classes are represented in a lower dimensional manifold, and the reduced set of parameters obtained in this subspace is utilized in an estimation, which is exemplified by a multivariate regression in this paper. We demonstrate our method with a felicitous application to estimation of customized hearing aid devices.", :doc-id "Customized Design of Hearing Aids Using Statistical Shape Learning. 2008 Gozde B. Unal, Delphine Nain, Gregory G. Slabaugh, Tong Fang"}, 2784531 #search_api.search_api.Paper{:id 2784531, :key "conf/ecai/JainBB10", :title "Adaptive Markov Logic Networks: Learning Statistical Relational Models with Dynamic Parameters.", :abstract "Statistical relational models, such as Markov logic networks, seek to compactly describe properties of relational domains by representing general principles about objects belonging to particular classes. Models are intended to be independent of the set of objects to which these principles can be applied, and it is assumed that the principles will soundly generalize across arbitrary sets of objects. In this paper, we point out limitations of models that seek to represent the corresponding principles with a fixed set of parameters and discuss the conditions under which the soundness of fixed parameters is indeed questionable. We propose a novel representation formalism called adaptive Markov logic networks to allow more flexible representations of relational domains, which involve parameters that are dynamically adjusted to fit the properties of an instantiation by phrasing the model's parameters as functions over attributes of the instantiation at hand. We empirically demonstrate the value of our learning and representation system on a simple but well-motivated example domain. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :author (#search_api.search_api.Author{:id 639688, :first-name nil, :last-name nil, :full-name "Dominik Jain"} #search_api.search_api.Author{:id 1005331, :first-name nil, :last-name nil, :full-name "Andreas Barthels"} #search_api.search_api.Author{:id 361314, :first-name nil, :last-name nil, :full-name "Michael Beetz"}), :year 2010, :venue "ECAI", :ncit 8, :string "Adaptive Markov Logic Networks: Learning Statistical Relational Models with Dynamic Parameters.. Statistical relational models, such as Markov logic networks, seek to compactly describe properties of relational domains by representing general principles about objects belonging to particular classes. Models are intended to be independent of the set of objects to which these principles can be applied, and it is assumed that the principles will soundly generalize across arbitrary sets of objects. In this paper, we point out limitations of models that seek to represent the corresponding principles with a fixed set of parameters and discuss the conditions under which the soundness of fixed parameters is indeed questionable. We propose a novel representation formalism called adaptive Markov logic networks to allow more flexible representations of relational domains, which involve parameters that are dynamically adjusted to fit the properties of an instantiation by phrasing the model's parameters as functions over attributes of the instantiation at hand. We empirically demonstrate the value of our learning and representation system on a simple but well-motivated example domain. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :doc-id "Adaptive Markov Logic Networks: Learning Statistical Relational Models with Dynamic Parameters. 2010 Dominik Jain, Andreas Barthels, Michael Beetz"}, 1009299 #search_api.search_api.Paper{:id 1009299, :key "journals/ml/BeefermanBL99", :title "Statistical Models for Text Segmentation.", :abstract "This paper introduces a new statistical approach to automatically partitioning text into coherent segments. The approach is based on a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text. The models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may be domain-specific, that tend to be used near segment boundaries. Assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, Wall Street Journal news articles and television broadcast news story transcripts. Quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. This metric is used to make a quantitative assessment of the relative contributions of the different feature types, as well as a comparison with decision trees and previously proposed text segmentation algorithms.", :author (#search_api.search_api.Author{:id 1914, :first-name nil, :last-name nil, :full-name "Doug Beeferman"} #search_api.search_api.Author{:id 109818, :first-name nil, :last-name nil, :full-name "Adam L. Berger"} #search_api.search_api.Author{:id 1463112, :first-name nil, :last-name nil, :full-name "John D. Lafferty"}), :year 1999, :venue "Machine Learning", :ncit 488, :string "Statistical Models for Text Segmentation.. This paper introduces a new statistical approach to automatically partitioning text into coherent segments. The approach is based on a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text. The models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may be domain-specific, that tend to be used near segment boundaries. Assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, Wall Street Journal news articles and television broadcast news story transcripts. Quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. This metric is used to make a quantitative assessment of the relative contributions of the different feature types, as well as a comparison with decision trees and previously proposed text segmentation algorithms.", :doc-id "Statistical Models for Text Segmentation. 1999 Doug Beeferman, Adam L. Berger, John D. Lafferty"}, 952340 #search_api.search_api.Paper{:id 952340, :key "journals/jair/Jaeger05", :title "Ignorability in Statistical and Probabilistic Inference.", :abstract "When dealing with incomplete data in statistical learning, or incomplete observations in probabilistic inference, one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened. Since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive, one asks for conditions under which it can be safely ignored. Such conditions are given by the missing at random (mar) and coarsened at random (car) assumptions. In this paper we provide an in-depth analysis of several questions relating to mar/car assumptions. Main purpose of our study is to provide criteria by which one may evaluate whether a car assumption is reasonable for a particular data collecting or observational process. This question is complicated by the fact that several distinct versions of mar/car assumptions exist. We therefore first provide an overview over these different versions, in which we highlight the distinction between distributional and coarsening variable induced versions. We show that distributional versions are less restrictive and sufficient for most applications. We then address from two different perspectives the question of when the mar/car assumption is warranted. First we provide a \"static\" analysis that characterizes the admissibility of the car assumption in terms of the support structure of the joint probability distribution of complete data and incomplete observations. Here we obtain an equivalence characterization that improves and extends a recent result by Grünwald and Halpern. We then turn to a \"procedural\" analysis that characterizes the admissibility of the car assumption in terms of procedural models for the actual data (or observation) generating process. The main result of this analysis is that the stronger coarsened completely at random (ccar) condition is arguably the most reasonable assumption, as it alone corresponds to data coarsening procedures that satisfy a natural robustness property.", :author (#search_api.search_api.Author{:id 252997, :first-name nil, :last-name nil, :full-name "Manfred Jaeger"}), :year 2005, :venue "J. Artif. Intell. Res. (JAIR)", :ncit 7, :string "Ignorability in Statistical and Probabilistic Inference.. When dealing with incomplete data in statistical learning, or incomplete observations in probabilistic inference, one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened. Since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive, one asks for conditions under which it can be safely ignored. Such conditions are given by the missing at random (mar) and coarsened at random (car) assumptions. In this paper we provide an in-depth analysis of several questions relating to mar/car assumptions. Main purpose of our study is to provide criteria by which one may evaluate whether a car assumption is reasonable for a particular data collecting or observational process. This question is complicated by the fact that several distinct versions of mar/car assumptions exist. We therefore first provide an overview over these different versions, in which we highlight the distinction between distributional and coarsening variable induced versions. We show that distributional versions are less restrictive and sufficient for most applications. We then address from two different perspectives the question of when the mar/car assumption is warranted. First we provide a \"static\" analysis that characterizes the admissibility of the car assumption in terms of the support structure of the joint probability distribution of complete data and incomplete observations. Here we obtain an equivalence characterization that improves and extends a recent result by Grünwald and Halpern. We then turn to a \"procedural\" analysis that characterizes the admissibility of the car assumption in terms of procedural models for the actual data (or observation) generating process. The main result of this analysis is that the stronger coarsened completely at random (ccar) condition is arguably the most reasonable assumption, as it alone corresponds to data coarsening procedures that satisfy a natural robustness property.", :doc-id "Ignorability in Statistical and Probabilistic Inference. 2005 Manfred Jaeger"}, 985236 #search_api.search_api.Paper{:id 985236, :key "journals/jmlr/PasseriniFR06", :title "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.", :abstract "We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :author (#search_api.search_api.Author{:id 1520074, :first-name nil, :last-name nil, :full-name "Andrea Passerini"} #search_api.search_api.Author{:id 34394, :first-name nil, :last-name nil, :full-name "Paolo Frasconi"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2006, :venue "Journal of Machine Learning Research", :ncit 31, :string "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.. We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :doc-id "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting. 2006 Andrea Passerini, Paolo Frasconi, Luc De Raedt"}, 1017236 #search_api.search_api.Paper{:id 1017236, :key "journals/mta/HanLL08", :title "Semantic image classification using statistical local spatial relations model.", :abstract "In this paper, a statistical model called statistical local spatial relations (SLSR) is presented as a novel technique of a learning model with spatial and statistical information for semantic image classification. The model is inspired by probabilistic Latent Semantic Analysis (PLSA) for text mining. In text analysis, PLSA is used to discover topics in a corpus using the bag-of-word document representation. In SLSR, we treat image categories as topics, therefore an image containing instances of multiple categories can be modeled as a mixture of topics. More significantly, SLSR introduces spatial relation information as a factor which is not present in PLSA. SLSR has rotation, scale, translation and affine invariant properties and can solve partial occlusion problems. Using the Dirichlet process and variational Expectation-Maximization learning algorithm, SLSR is developed as an implementation of an image classification algorithm. SLSR uses an unsupervised process which can capture both spatial relations and statistical information simultaneously. The experiments are demonstrated on some standard data sets and show that the SLSR model is a promising model for semantic image classification problems.", :author (#search_api.search_api.Author{:id 51919, :first-name nil, :last-name nil, :full-name "Dongfeng Han"} #search_api.search_api.Author{:id 589651, :first-name nil, :last-name nil, :full-name "Wenhui Li"} #search_api.search_api.Author{:id 1381040, :first-name nil, :last-name nil, :full-name "Zongcheng Li"}), :year 2008, :venue "Multimedia Tools Appl.", :ncit 11, :string "Semantic image classification using statistical local spatial relations model.. In this paper, a statistical model called statistical local spatial relations (SLSR) is presented as a novel technique of a learning model with spatial and statistical information for semantic image classification. The model is inspired by probabilistic Latent Semantic Analysis (PLSA) for text mining. In text analysis, PLSA is used to discover topics in a corpus using the bag-of-word document representation. In SLSR, we treat image categories as topics, therefore an image containing instances of multiple categories can be modeled as a mixture of topics. More significantly, SLSR introduces spatial relation information as a factor which is not present in PLSA. SLSR has rotation, scale, translation and affine invariant properties and can solve partial occlusion problems. Using the Dirichlet process and variational Expectation-Maximization learning algorithm, SLSR is developed as an implementation of an image classification algorithm. SLSR uses an unsupervised process which can capture both spatial relations and statistical information simultaneously. The experiments are demonstrated on some standard data sets and show that the SLSR model is a promising model for semantic image classification problems.", :doc-id "Semantic image classification using statistical local spatial relations model. 2008 Dongfeng Han, Wenhui Li, Zongcheng Li"}, 3623508 #search_api.search_api.Paper{:id 3623508, :key "journals/ijsnm/EspositoFBM12", :title "Social networks and statistical relational learning: a survey.", :abstract nil, :author (#search_api.search_api.Author{:id 769268, :first-name nil, :last-name nil, :full-name "Floriana Esposito"} #search_api.search_api.Author{:id 1126564, :first-name nil, :last-name nil, :full-name "Stefano Ferilli"} #search_api.search_api.Author{:id 1078358, :first-name nil, :last-name nil, :full-name "Teresa Maria Altomare Basile"} #search_api.search_api.Author{:id 1294122, :first-name nil, :last-name nil, :full-name "Nicola Di Mauro"}), :year 2012, :venue "IJSNM", :ncit 0, :string "Social networks and statistical relational learning: a survey.. ", :doc-id "Social networks and statistical relational learning: a survey. 2012 Floriana Esposito, Stefano Ferilli, Teresa Maria Altomare Basile, Nicola Di Mauro"}, 1009524 #search_api.search_api.Paper{:id 1009524, :key "journals/ml/HeckermanGC95", :title "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.", :abstract "We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :author (#search_api.search_api.Author{:id 114083, :first-name nil, :last-name nil, :full-name "David Heckerman"} #search_api.search_api.Author{:id 330355, :first-name nil, :last-name nil, :full-name "Dan Geiger"} #search_api.search_api.Author{:id 1215050, :first-name nil, :last-name nil, :full-name "David Maxwell Chickering"}), :year 1995, :venue "Machine Learning", :ncit 3837, :string "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.. We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :doc-id "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data. 1995 David Heckerman, Dan Geiger, David Maxwell Chickering"}, 192565 #search_api.search_api.Paper{:id 192565, :key "conf/eurocolt/ShamirS95", :title "Learning by extended statistical queries and its relation to PAC learning.", :abstract nil, :author (#search_api.search_api.Author{:id 1191060, :first-name nil, :last-name nil, :full-name "Eli Shamir"} #search_api.search_api.Author{:id 690500, :first-name nil, :last-name nil, :full-name "Clara Shwartzman"}), :year 1995, :venue "EuroCOLT", :ncit 8, :string "Learning by extended statistical queries and its relation to PAC learning.. ", :doc-id "Learning by extended statistical queries and its relation to PAC learning. 1995 Eli Shamir, Clara Shwartzman"}, 1236661 #search_api.search_api.Paper{:id 1236661, :key "conf/RelMiCS/Muller09", :title "Modalities, Relations, and Learning.", :abstract "While the popularity of statistical, probabilistic and exhaustive machine learning techniques still increases, relational and logic approaches are still a niche market in research. While the former approaches focus on predictive accuracy, the latter ones prove to be indispensable in knowledge discovery.In this paper we present a relational description of machine learning problems. We demonstrate how common ensemble learning methods as used in classifier learning can be reformulated in a relational setting. It is shown that multimodal logics and relational data analysis with rough sets are closely related. Finally, we give an interpretation of logic programs as approximations of hypotheses.It is demonstrated that at a certain level of abstraction all these methods unify into one and the same formalisation which nicely connects to multimodal operators. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :author (#search_api.search_api.Author{:id 1161107, :first-name nil, :last-name nil, :full-name "Martin Eric Müller"}), :year 2009, :venue "RelMiCS", :ncit 0, :string "Modalities, Relations, and Learning.. While the popularity of statistical, probabilistic and exhaustive machine learning techniques still increases, relational and logic approaches are still a niche market in research. While the former approaches focus on predictive accuracy, the latter ones prove to be indispensable in knowledge discovery.In this paper we present a relational description of machine learning problems. We demonstrate how common ensemble learning methods as used in classifier learning can be reformulated in a relational setting. It is shown that multimodal logics and relational data analysis with rough sets are closely related. Finally, we give an interpretation of logic programs as approximations of hypotheses.It is demonstrated that at a certain level of abstraction all these methods unify into one and the same formalisation which nicely connects to multimodal operators. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :doc-id "Modalities, Relations, and Learning. 2009 Martin Eric Müller"}, 302837 #search_api.search_api.Paper{:id 302837, :key "conf/icdm/PopesculULP03", :title "Statistical Relational Learning for Document Mining.", :abstract "A major obstacle to fully integrated deployment of manydata mining algorithms is the assumption that data sitsin a single table, even though most real-world databaseshave complex relational structures. We propose an integratedapproach to statistical modeling from relationaldatabases. We structure the search space based on \"refinementgraphs\", which are widely used in inductive logic programmingfor learning logic descriptions. The use of statisticsallows us to extend the search space to include richerset of features, including many which are not boolean.Search and model selection are integrated into a single process,allowing information criteria native to the statisticalmodel, for example logistic regression, to make feature selectiondecisions in a step-wise manner. We present experimentalresults for the task of predicting where scientific paperswill be published based on relational data taken fromCiteSeer. Our approach results in classification accuraciessuperior to those achieved when using classical \"flat\" features.The resulting classifier can be used to recommendwhere to publish articles.", :author (#search_api.search_api.Author{:id 597076, :first-name nil, :last-name nil, :full-name "Alexandrin Popescul"} #search_api.search_api.Author{:id 352656, :first-name nil, :last-name nil, :full-name "Lyle H. Ungar"} #search_api.search_api.Author{:id 599072, :first-name nil, :last-name nil, :full-name "Steve Lawrence"} #search_api.search_api.Author{:id 80443, :first-name nil, :last-name nil, :full-name "David M. Pennock"}), :year 2003, :venue "ICDM", :ncit 51, :string "Statistical Relational Learning for Document Mining.. A major obstacle to fully integrated deployment of manydata mining algorithms is the assumption that data sitsin a single table, even though most real-world databaseshave complex relational structures. We propose an integratedapproach to statistical modeling from relationaldatabases. We structure the search space based on \"refinementgraphs\", which are widely used in inductive logic programmingfor learning logic descriptions. The use of statisticsallows us to extend the search space to include richerset of features, including many which are not boolean.Search and model selection are integrated into a single process,allowing information criteria native to the statisticalmodel, for example logistic regression, to make feature selectiondecisions in a step-wise manner. We present experimentalresults for the task of predicting where scientific paperswill be published based on relational data taken fromCiteSeer. Our approach results in classification accuraciessuperior to those achieved when using classical \"flat\" features.The resulting classifier can be used to recommendwhere to publish articles.", :doc-id "Statistical Relational Learning for Document Mining. 2003 Alexandrin Popescul, Lyle H. Ungar, Steve Lawrence, David M. Pennock"}, 1032117 #search_api.search_api.Paper{:id 1032117, :key "journals/pami/KonishiYCZ03", :title "Statistical Edge Detection: Learning and Evaluating Edge Cues.", :abstract "We formulate edge detection as statistical inference. This statistical edge detection is data driven, unlike standard methods for edge detection which are model based. For any set of edge detection filters (implementing local edge cues), we use presegmented images to learn the probability distributions of filter responses conditioned on whether they are evaluated on or off an edge. Edge detection is formulated as a discrimination task specified by a likelihood ratio test on the filter responses. This approach emphasizes the necessity of modeling the image background (the off-edges). We represent the conditional probability distributions nonparametrically and illustrate them on two different data sets of 100 (Sowerby) and 50 (South Florida) images. Multiple edges cues, including chrominance and multiple-scale, are combined by using their joint distributions. Hence, this cue combination is optimal in the statistical sense. We evaluate the effectiveness of different visual cues using the Chernoff information and Receiver Operator Characteristic (ROC) curves. This shows that our approach gives quantitatively better results than the Canny edge detector when the image background contains significant clutter. In addition, it enables us to determine the effectiveness of different edge cues and gives quantitative measures for the advantages of multilevel processing, for the use of chrominance, and for the relative effectiveness of different detectors. Furthermore, we show that we can learn these conditional distributions on one data set and adapt them to the other with only slight degradation of performance without knowing the ground truth on the second data set. This shows that our results are not purely domain specific. We apply the same approach to the spatial grouping of edge cues and obtain analogies to nonmaximal suppression and hysteresis.", :author (#search_api.search_api.Author{:id 696497, :first-name nil, :last-name nil, :full-name "Scott Konishi"} #search_api.search_api.Author{:id 248255, :first-name nil, :last-name nil, :full-name "Alan L. Yuille"} #search_api.search_api.Author{:id 589169, :first-name nil, :last-name nil, :full-name "James M. Coughlan"} #search_api.search_api.Author{:id 589996, :first-name nil, :last-name nil, :full-name "Song Chun Zhu"}), :year 2003, :venue "IEEE Trans. Pattern Anal. Mach. Intell.", :ncit 187, :string "Statistical Edge Detection: Learning and Evaluating Edge Cues.. We formulate edge detection as statistical inference. This statistical edge detection is data driven, unlike standard methods for edge detection which are model based. For any set of edge detection filters (implementing local edge cues), we use presegmented images to learn the probability distributions of filter responses conditioned on whether they are evaluated on or off an edge. Edge detection is formulated as a discrimination task specified by a likelihood ratio test on the filter responses. This approach emphasizes the necessity of modeling the image background (the off-edges). We represent the conditional probability distributions nonparametrically and illustrate them on two different data sets of 100 (Sowerby) and 50 (South Florida) images. Multiple edges cues, including chrominance and multiple-scale, are combined by using their joint distributions. Hence, this cue combination is optimal in the statistical sense. We evaluate the effectiveness of different visual cues using the Chernoff information and Receiver Operator Characteristic (ROC) curves. This shows that our approach gives quantitatively better results than the Canny edge detector when the image background contains significant clutter. In addition, it enables us to determine the effectiveness of different edge cues and gives quantitative measures for the advantages of multilevel processing, for the use of chrominance, and for the relative effectiveness of different detectors. Furthermore, we show that we can learn these conditional distributions on one data set and adapt them to the other with only slight degradation of performance without knowing the ground truth on the second data set. This shows that our results are not purely domain specific. We apply the same approach to the spatial grouping of edge cues and obtain analogies to nonmaximal suppression and hysteresis.", :doc-id "Statistical Edge Detection: Learning and Evaluating Edge Cues. 2003 Scott Konishi, Alan L. Yuille, James M. Coughlan, Song Chun Zhu"}, 574389 #search_api.search_api.Paper{:id 574389, :key "conf/sbia/Raedt08a", :title "Logic, Probability and Learning, or an Introduction to Statistical Relational Learning.", :abstract "Probabilistic inductive logic programming (PILP), sometimes also called statistical relational learning, addresses one of the central questions of artificial intelligence: the integration of probabilistic reasoning with first order logic representations and machine learning. A rich variety of different formalisms and learning techniques have been developed and they are being applied on applications in network analysis, robotics, bio-informatics, intelligent agents, etc. This tutorial starts with an introduction to probabilistic representations and machine learning, and then continues with an overview of the state-of-the-art in statistical relational learning. We start from classical settings for logic learning (or inductive logic programming) namely learning from entailment, learning from interpretations, and learning from proofs, and show how they can be extended with probabilistic methods. While doing so, we review state-of-the-art statistical relational learning approaches and show how they fit the discussed learning settings for probabilistic inductive logic programming.", :author (#search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2008, :venue "SBIA", :ncit 0, :string "Logic, Probability and Learning, or an Introduction to Statistical Relational Learning.. Probabilistic inductive logic programming (PILP), sometimes also called statistical relational learning, addresses one of the central questions of artificial intelligence: the integration of probabilistic reasoning with first order logic representations and machine learning. A rich variety of different formalisms and learning techniques have been developed and they are being applied on applications in network analysis, robotics, bio-informatics, intelligent agents, etc. This tutorial starts with an introduction to probabilistic representations and machine learning, and then continues with an overview of the state-of-the-art in statistical relational learning. We start from classical settings for logic learning (or inductive logic programming) namely learning from entailment, learning from interpretations, and learning from proofs, and show how they can be extended with probabilistic methods. While doing so, we review state-of-the-art statistical relational learning approaches and show how they fit the discussed learning settings for probabilistic inductive logic programming.", :doc-id "Logic, Probability and Learning, or an Introduction to Statistical Relational Learning. 2008 Luc De Raedt"}, 57463 #search_api.search_api.Paper{:id 57463, :key "conf/atal/RettingerNT08", :title "A statistical relational model for trust learning.", :abstract "We address the learning of trust based on past observations and context information. We argue that from the truster's point of view trust is best expressed as one of several relations that exist between the agent to be trusted (trustee) and the state of the environment. Besides attributes expressing trustworthiness, additional relations might describe commitments made by the trustee with regard to the current situation, like: a seller offers a certain price for a specific product. We show how to implement and learn contextsensitive trust using statistical relational learning in form of the Infinite Hidden Relational Trust Model (IHRTM). The practicability and effectiveness of our approach is evaluated empirically on user-ratings gathered from eBay. Our results suggest that (i) the inherent clustering achieved in the algorithm allows the truster to characterize the structure of a trust-situation and provides meaningful trust assessments; (ii) utilizing the collaborative filtering effect associated with relational data does improve trust assessment performance; (iii) by learning faster and transferring knowledge more effectively we improve cold start performance and can cope better with dynamic behavior in open multiagent systems. The later is demonstrated with interactions recorded from a strategic two-player negotiation scenario.", :author (#search_api.search_api.Author{:id 444383, :first-name nil, :last-name nil, :full-name "Achim Rettinger"} #search_api.search_api.Author{:id 222492, :first-name nil, :last-name nil, :full-name "Matthias Nickles"} #search_api.search_api.Author{:id 680469, :first-name nil, :last-name nil, :full-name "Volker Tresp"}), :year 2008, :venue "AAMAS (2)", :ncit 14, :string "A statistical relational model for trust learning.. We address the learning of trust based on past observations and context information. We argue that from the truster's point of view trust is best expressed as one of several relations that exist between the agent to be trusted (trustee) and the state of the environment. Besides attributes expressing trustworthiness, additional relations might describe commitments made by the trustee with regard to the current situation, like: a seller offers a certain price for a specific product. We show how to implement and learn contextsensitive trust using statistical relational learning in form of the Infinite Hidden Relational Trust Model (IHRTM). The practicability and effectiveness of our approach is evaluated empirically on user-ratings gathered from eBay. Our results suggest that (i) the inherent clustering achieved in the algorithm allows the truster to characterize the structure of a trust-situation and provides meaningful trust assessments; (ii) utilizing the collaborative filtering effect associated with relational data does improve trust assessment performance; (iii) by learning faster and transferring knowledge more effectively we improve cold start performance and can cope better with dynamic behavior in open multiagent systems. The later is demonstrated with interactions recorded from a strategic two-player negotiation scenario.", :doc-id "A statistical relational model for trust learning. 2008 Achim Rettinger, Matthias Nickles, Volker Tresp"}, 746711 #search_api.search_api.Paper{:id 746711, :key "journals/aicom/Kersting06", :title "An inductive logic programming approach to statistical relational learning.", :abstract "It has been a great pleasure to be asked to write the preface for the book based on Kristian Kersting's thesis. There is no doubt in my mind that this is a remarkable and outstanding piece of work. In his thesis Kristian has made an assault on one of the hardest integration problems at the heart of Artificial Intelligence research. This involves taking three disparate major areas of research and attempting a fusion among them. The three areas are: Logic Programming, Uncertainty Reasoning and Machine Learning. Every one of these is a major sub-area of research with its own associated international research conferences. Having taken on such a Herculean task, Kristian has produced a series of widely published results which are now at the core of a newly emerging area: Probabilistic Inductive Logic Programming. The new area is closely tied to, though strictly subsumes, a new field known as “Statistical Relational Learning” which has in the last few years gained major prominence in the American Artificial Intelligence research community. Within his thesis Kristian makes several major contributions, many of which have already been published in refereed conference and journal papers. Firstly, Kristian introduces a series of definitions which circumscribe the new area formed by extending Inductive Logic Programming to the case in which clauses are annotated with probability values. This represents a new and powerful framework which supersedes a number of influential papers and research areas in Artificial Intelligence. Secondly, Kristian introduces Bayesian Logic Programs (BLPs). These represent an elegantly defined lifting of Judea Pearl's Bayesian networks to the logic programming level. Since Kristian's introduction of BLPs, a number of results indicate that BLPs generalise many previously defined representations, not the least of which are Bayesian networks, Logic Programs, Probabilistic Relation Models and Stochastic Logic Programs. Next Kristian investigates the approach of Learning from proofs. This is an interesting new learning framework which is the first to go beyond the two standard semantic frameworks of Inductive Logic Programming. Kristian then looks at the problem of upgrading HMMs to logical HMMs. Hidden Markov Models (HMMs) are one of the most widely used machine learning technologies in Statistical Linguistics and Bioinformatics, and allow the representation of probabilistic finite automata. Kristian has upgraded standard HMMs to allow relational descriptions to be included within the description of the automata. The three standard HMM estimation algorithms are also upgraded. He has demonstrated the power of such representations using biological predictive modelling problems, and shown performance increases over alternative approaches. Kristian next considers the issue of upgrading Fisher Kernels to Relational Fisher kernels. Fisher kernels have been widely used within statistics and more recently in support vector machines. Buildin...", :author (#search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"}), :year 2006, :venue "AI Commun.", :ncit 15, :string "An inductive logic programming approach to statistical relational learning.. It has been a great pleasure to be asked to write the preface for the book based on Kristian Kersting's thesis. There is no doubt in my mind that this is a remarkable and outstanding piece of work. In his thesis Kristian has made an assault on one of the hardest integration problems at the heart of Artificial Intelligence research. This involves taking three disparate major areas of research and attempting a fusion among them. The three areas are: Logic Programming, Uncertainty Reasoning and Machine Learning. Every one of these is a major sub-area of research with its own associated international research conferences. Having taken on such a Herculean task, Kristian has produced a series of widely published results which are now at the core of a newly emerging area: Probabilistic Inductive Logic Programming. The new area is closely tied to, though strictly subsumes, a new field known as “Statistical Relational Learning” which has in the last few years gained major prominence in the American Artificial Intelligence research community. Within his thesis Kristian makes several major contributions, many of which have already been published in refereed conference and journal papers. Firstly, Kristian introduces a series of definitions which circumscribe the new area formed by extending Inductive Logic Programming to the case in which clauses are annotated with probability values. This represents a new and powerful framework which supersedes a number of influential papers and research areas in Artificial Intelligence. Secondly, Kristian introduces Bayesian Logic Programs (BLPs). These represent an elegantly defined lifting of Judea Pearl's Bayesian networks to the logic programming level. Since Kristian's introduction of BLPs, a number of results indicate that BLPs generalise many previously defined representations, not the least of which are Bayesian networks, Logic Programs, Probabilistic Relation Models and Stochastic Logic Programs. Next Kristian investigates the approach of Learning from proofs. This is an interesting new learning framework which is the first to go beyond the two standard semantic frameworks of Inductive Logic Programming. Kristian then looks at the problem of upgrading HMMs to logical HMMs. Hidden Markov Models (HMMs) are one of the most widely used machine learning technologies in Statistical Linguistics and Bioinformatics, and allow the representation of probabilistic finite automata. Kristian has upgraded standard HMMs to allow relational descriptions to be included within the description of the automata. The three standard HMM estimation algorithms are also upgraded. He has demonstrated the power of such representations using biological predictive modelling problems, and shown performance increases over alternative approaches. Kristian next considers the issue of upgrading Fisher Kernels to Relational Fisher kernels. Fisher kernels have been widely used within statistics and more recently in support vector machines. Buildin...", :doc-id "An inductive logic programming approach to statistical relational learning. 2006 Kristian Kersting"}, 388504 #search_api.search_api.Paper{:id 388504, :key "conf/ijcai/FriedmanGKP99", :title "Learning Probabilistic Relational Models.", :abstract "A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with \"flat\" data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases.", :author (#search_api.search_api.Author{:id 61431, :first-name nil, :last-name nil, :full-name "Nir Friedman"} #search_api.search_api.Author{:id 1449720, :first-name nil, :last-name nil, :full-name "Lise Getoor"} #search_api.search_api.Author{:id 560995, :first-name nil, :last-name nil, :full-name "Daphne Koller"} #search_api.search_api.Author{:id 154856, :first-name nil, :last-name nil, :full-name "Avi Pfeffer"}), :year 1999, :venue "IJCAI", :ncit 843, :string "Learning Probabilistic Relational Models.. A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with \"flat\" data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases.", :doc-id "Learning Probabilistic Relational Models. 1999 Nir Friedman, Lise Getoor, Daphne Koller, Avi Pfeffer"}, 644121 #search_api.search_api.Paper{:id 644121, :key "conf/vldb/ZhangHJLZ05", :title "Statistical Learning Techniques for Costing XML Queries.", :abstract "Developing cost models for query optimization is significantly harder for XML queries than for traditional relational queries. The reason is that XML query operators are much more complex than relational operators such as table scans and joins. In this paper, we propose a new approach, called COMET, to modeling the cost of XML operators; to our knowledge, COMET is the first method ever proposed for addressing the XML query costing problem. As in relational cost estimation, COMET exploits a set of system catalog statistics that summarizes the XML data; the set of \"simple path\" statistics that we propose is new, and is well suited to the XML setting. Unlike the traditional approach, COMET uses a new statistical learning technique called \"transform regression\" instead of detailed analytical models to predict the overall cost. Besides rendering the cost estimation problem tractable for XML queries, COMET has the further advantage of enabling the query optimizer to be self-tuning, automatically adapting to changes over time in the query workload and in the system environment. We demonstrate COMET's feasibility by developing a cost model for the recently proposed XNAV navigational operator. Empirical studies with synthetic, benchmark, and real-world data sets show that COMET can quickly obtain accurate cost estimates for a variety of XML queries and data sets.", :author (#search_api.search_api.Author{:id 39154, :first-name nil, :last-name nil, :full-name "Ning Zhang"} #search_api.search_api.Author{:id 832799, :first-name nil, :last-name nil, :full-name "Peter J. Haas"} #search_api.search_api.Author{:id 875466, :first-name nil, :last-name nil, :full-name "Vanja Josifovski"} #search_api.search_api.Author{:id 119924, :first-name nil, :last-name nil, :full-name "Guy M. Lohman"} #search_api.search_api.Author{:id 691076, :first-name nil, :last-name nil, :full-name "Chun Zhang"}), :year 2005, :venue "VLDB", :ncit 56, :string "Statistical Learning Techniques for Costing XML Queries.. Developing cost models for query optimization is significantly harder for XML queries than for traditional relational queries. The reason is that XML query operators are much more complex than relational operators such as table scans and joins. In this paper, we propose a new approach, called COMET, to modeling the cost of XML operators; to our knowledge, COMET is the first method ever proposed for addressing the XML query costing problem. As in relational cost estimation, COMET exploits a set of system catalog statistics that summarizes the XML data; the set of \"simple path\" statistics that we propose is new, and is well suited to the XML setting. Unlike the traditional approach, COMET uses a new statistical learning technique called \"transform regression\" instead of detailed analytical models to predict the overall cost. Besides rendering the cost estimation problem tractable for XML queries, COMET has the further advantage of enabling the query optimizer to be self-tuning, automatically adapting to changes over time in the query workload and in the system environment. We demonstrate COMET's feasibility by developing a cost model for the recently proposed XNAV navigational operator. Empirical studies with synthetic, benchmark, and real-world data sets show that COMET can quickly obtain accurate cost estimates for a variety of XML queries and data sets.", :doc-id "Statistical Learning Techniques for Costing XML Queries. 2005 Ning Zhang, Peter J. Haas, Vanja Josifovski, Guy M. Lohman, Chun Zhang"}, 1009754 #search_api.search_api.Paper{:id 1009754, :key "journals/ml/PerlichP06", :title "Distribution-based aggregation for relational learning with identifier attributes.", :abstract "Identifier attributes--very high-dimensional categorical attributes such as particular product ids or people's names--rarely are incorporated in statistical modeling. However, they can play an important role in relational modeling: it may be informative to have communicated with a particular set of people or to have purchased a particular set of products. A key limitation of existing relational modeling techniques is how they aggregate bags (multisets) of values from related entities. The aggregations used by existing methods are simple summaries of the distributions of features of related entities: e.g., MEAN, MODE, SUM, or COUNT. This paper's main contribution is the introduction of aggregation operators that capture more information about the value distributions, by storing meta-data about value distributions and referencing this meta-data when aggregating--for example by computing class-conditional distributional distances. Such aggregations are particularly important for aggregating values from high-dimensional categorical attributes, for which the simple aggregates provide little information. In the first half of the paper we provide general guidelines for designing aggregation operators, introduce the new aggregators in the context of the relational learning system ACORA (Automated Construction of Relational Attributes), and provide theoretical justification. We also conjecture special properties of identifier attributes, e.g., they proxy for unobserved attributes and for information deeper in the relationship network. In the second half of the paper we provide extensive empirical evidence that the distribution-based aggregators indeed do facilitate modeling with high-dimensional categorical attributes, and in support of the aforementioned conjectures.", :author (#search_api.search_api.Author{:id 445653, :first-name nil, :last-name nil, :full-name "Claudia Perlich"} #search_api.search_api.Author{:id 976323, :first-name nil, :last-name nil, :full-name "Foster J. Provost"}), :year 2006, :venue "Machine Learning", :ncit 54, :string "Distribution-based aggregation for relational learning with identifier attributes.. Identifier attributes--very high-dimensional categorical attributes such as particular product ids or people's names--rarely are incorporated in statistical modeling. However, they can play an important role in relational modeling: it may be informative to have communicated with a particular set of people or to have purchased a particular set of products. A key limitation of existing relational modeling techniques is how they aggregate bags (multisets) of values from related entities. The aggregations used by existing methods are simple summaries of the distributions of features of related entities: e.g., MEAN, MODE, SUM, or COUNT. This paper's main contribution is the introduction of aggregation operators that capture more information about the value distributions, by storing meta-data about value distributions and referencing this meta-data when aggregating--for example by computing class-conditional distributional distances. Such aggregations are particularly important for aggregating values from high-dimensional categorical attributes, for which the simple aggregates provide little information. In the first half of the paper we provide general guidelines for designing aggregation operators, introduce the new aggregators in the context of the relational learning system ACORA (Automated Construction of Relational Attributes), and provide theoretical justification. We also conjecture special properties of identifier attributes, e.g., they proxy for unobserved attributes and for information deeper in the relationship network. In the second half of the paper we provide extensive empirical evidence that the distribution-based aggregators indeed do facilitate modeling with high-dimensional categorical attributes, and in support of the aforementioned conjectures.", :doc-id "Distribution-based aggregation for relational learning with identifier attributes. 2006 Claudia Perlich, Foster J. Provost"}, 1250938 #search_api.search_api.Paper{:id 1250938, :key "conf/ismis/BibaFE09", :title "Efficient MAP Inference for Statistical Relational Models through Hybrid Metaheuristics.", :abstract "Statistical Relational Models are state-of-the-art representation formalisms at the intersection of logical and statistical machine learning. One of the most promising models is Markov Logic (ML) which combines Markov networks (MNs) and first-order logic by attaching weights to first-order formulas and using these as templates for features of MNs. MAP inference in ML is the task of finding the most likely state of a set of output variables given the state of the input variables and this problem is NP-hard. In this paper we present an algorithm for this inference task based on the Iterated Local Search (ILS) and Robust Tabu Search (RoTS) metaheuristics. The algorithm performs a biased sampling of the set of local optima by using RoTS as a local search procedure and repetitively jumping in the search space through a perturbation operator, focusing the search not on the full space of solutions but on a smaller subspace defined by the solutions that are locally optimal for the optimization engine. We show through extensive experiments in real-world domains that it improves over the state-of-the-art algorithm in terms of solution quality and inference time.", :author (#search_api.search_api.Author{:id 1482903, :first-name nil, :last-name nil, :full-name "Marenglen Biba"} #search_api.search_api.Author{:id 1126564, :first-name nil, :last-name nil, :full-name "Stefano Ferilli"} #search_api.search_api.Author{:id 769268, :first-name nil, :last-name nil, :full-name "Floriana Esposito"}), :year 2009, :venue "ISMIS", :ncit 0, :string "Efficient MAP Inference for Statistical Relational Models through Hybrid Metaheuristics.. Statistical Relational Models are state-of-the-art representation formalisms at the intersection of logical and statistical machine learning. One of the most promising models is Markov Logic (ML) which combines Markov networks (MNs) and first-order logic by attaching weights to first-order formulas and using these as templates for features of MNs. MAP inference in ML is the task of finding the most likely state of a set of output variables given the state of the input variables and this problem is NP-hard. In this paper we present an algorithm for this inference task based on the Iterated Local Search (ILS) and Robust Tabu Search (RoTS) metaheuristics. The algorithm performs a biased sampling of the set of local optima by using RoTS as a local search procedure and repetitively jumping in the search space through a perturbation operator, focusing the search not on the full space of solutions but on a smaller subspace defined by the solutions that are locally optimal for the optimization engine. We show through extensive experiments in real-world domains that it improves over the state-of-the-art algorithm in terms of solution quality and inference time.", :doc-id "Efficient MAP Inference for Statistical Relational Models through Hybrid Metaheuristics. 2009 Marenglen Biba, Stefano Ferilli, Floriana Esposito"}, 294586 #search_api.search_api.Paper{:id 294586, :key "conf/icdar/CeciBM05", :title "Relational Learning techniques for Document Image Understanding: Comparing Statistical and Logical approaches.", :abstract "In this paper, we evaluate and systematically compare two different (multi-)relational learning methods based on a statistical approach and a logical approach for the task of document image understanding. For a fair comparison, both methods are tested on the same real world dataset consisting of multipage articles published in an international journal. An analysis of pros and cons of both approaches is reported.", :author (#search_api.search_api.Author{:id 1294148, :first-name nil, :last-name nil, :full-name "Michelangelo Ceci"} #search_api.search_api.Author{:id 1385113, :first-name nil, :last-name nil, :full-name "Margherita Berardi"} #search_api.search_api.Author{:id 282116, :first-name nil, :last-name nil, :full-name "Donato Malerba"}), :year 2005, :venue "ICDAR", :ncit 3, :string "Relational Learning techniques for Document Image Understanding: Comparing Statistical and Logical approaches.. In this paper, we evaluate and systematically compare two different (multi-)relational learning methods based on a statistical approach and a logical approach for the task of document image understanding. For a fair comparison, both methods are tested on the same real world dataset consisting of multipage articles published in an international journal. An analysis of pros and cons of both approaches is reported.", :doc-id "Relational Learning techniques for Document Image Understanding: Comparing Statistical and Logical approaches. 2005 Michelangelo Ceci, Margherita Berardi, Donato Malerba"}, 1026778 #search_api.search_api.Paper{:id 1026778, :key "journals/nn/Linsker05", :title "Improved local learning rule for information maximization and related applications.", :abstract "For a neural network comprising feedforward and lateral connections, a local learning rule is proposed that causes the lateral connections to learn directly the inverse of a covariance matrix. In contrast to earlier work, the rule involves just one processing pass through the lateral connections for each input presentation, and consists of a simple anti-Hebbian term. This provides an effective and simple method for online network learning algorithms that implement optimization principles, drawn from statistics or from information or control theory, for which a running estimate of the covariance matrix inverse is useful. An application to infomax learning (mutual information maximization) in the presence of input and output noise is used to illustrate the method.", :author (#search_api.search_api.Author{:id 321062, :first-name nil, :last-name nil, :full-name "Ralph Linsker"}), :year 2005, :venue "Neural Networks", :ncit 19, :string "Improved local learning rule for information maximization and related applications.. For a neural network comprising feedforward and lateral connections, a local learning rule is proposed that causes the lateral connections to learn directly the inverse of a covariance matrix. In contrast to earlier work, the rule involves just one processing pass through the lateral connections for each input presentation, and consists of a simple anti-Hebbian term. This provides an effective and simple method for online network learning algorithms that implement optimization principles, drawn from statistics or from information or control theory, for which a running estimate of the covariance matrix inverse is useful. An application to infomax learning (mutual information maximization) in the presence of input and output noise is used to illustrate the method.", :doc-id "Improved local learning rule for information maximization and related applications. 2005 Ralph Linsker"}, 3341018 #search_api.search_api.Paper{:id 3341018, :key "phd/de/Kersting2006", :title "An inductive logic programming approach to statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"}), :year 2006, :venue nil, :ncit 15, :string "An inductive logic programming approach to statistical relational learning.. ", :doc-id "An inductive logic programming approach to statistical relational learning. 2006 Kristian Kersting"}, 3402586 #search_api.search_api.Paper{:id 3402586, :key "series/faia/2005-148", :title "An Inductive Logic Programming Approach to Statistical Relational Learning", :abstract nil, :author (#search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"}), :year 2005, :venue "Frontiers in Artificial Intelligence and Applications", :ncit 15, :string "An Inductive Logic Programming Approach to Statistical Relational Learning. ", :doc-id "An Inductive Logic Programming Approach to Statistical Relational Learning 2005 Kristian Kersting"}, 395451 #search_api.search_api.Paper{:id 395451, :key "conf/ilp/SaittaV08", :title "A Comparison between Two Statistical Relational Models.", :abstract "Statistical Relational Learning has received much attention this last decade. In the ILP community, several models have emerged for modelling and learning uncertain knowledge, expressed in subset of first order logics. Nevertheless, no deep comparisons have been made among them and, given an application, determining which model must be chosen is difficult. In this paper, we compare two of them, namely Markov Logic Networks and Bayesian Programs, especially with respect to their representation ability and inference methods. The comparison shows that the two models are substantially different, from the point of view of the user, so that choosing one really means choosing a different philosophy to look at the problem. In order to make the comparison more concrete, we have used a running example, which shows most of the interesting points of the approaches, yet remaining exactly tractable.", :author (#search_api.search_api.Author{:id 176408, :first-name nil, :last-name nil, :full-name "Lorenza Saitta"} #search_api.search_api.Author{:id 99042, :first-name nil, :last-name nil, :full-name "Christel Vrain"}), :year 2008, :venue "ILP", :ncit 1, :string "A Comparison between Two Statistical Relational Models.. Statistical Relational Learning has received much attention this last decade. In the ILP community, several models have emerged for modelling and learning uncertain knowledge, expressed in subset of first order logics. Nevertheless, no deep comparisons have been made among them and, given an application, determining which model must be chosen is difficult. In this paper, we compare two of them, namely Markov Logic Networks and Bayesian Programs, especially with respect to their representation ability and inference methods. The comparison shows that the two models are substantially different, from the point of view of the user, so that choosing one really means choosing a different philosophy to look at the problem. In order to make the comparison more concrete, we have used a running example, which shows most of the interesting points of the approaches, yet remaining exactly tractable.", :doc-id "A Comparison between Two Statistical Relational Models. 2008 Lorenza Saitta, Christel Vrain"}, 937563 #search_api.search_api.Paper{:id 937563, :key "journals/ir/Yang99", :title "An Evaluation of Statistical Approaches to Text Categorization.", :abstract "This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance&semi; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.", :author (#search_api.search_api.Author{:id 1066064, :first-name nil, :last-name nil, :full-name "Yiming Yang"}), :year 1999, :venue "Inf. Retr.", :ncit 1871, :string "An Evaluation of Statistical Approaches to Text Categorization.. This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance&semi; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.", :doc-id "An Evaluation of Statistical Approaches to Text Categorization. 1999 Yiming Yang"}, 391548 #search_api.search_api.Paper{:id 391548, :key "conf/ijcai/DavisOSBPC07", :title "Change of Representation for Statistical Relational Learning.", :abstract "Statistical relational learning (SRL) algorithms learn statistical models from relational data, such as that stored in a relational database. We previously introduced view learning for SRL, in which the view of a relational database can be automatically modified, yielding more accurate statistical models. The present paper presents SAYU-VISTA, an algorithm which advances beyond the initial view learning approach in three ways. First, it learns views that introduce new relational tables, rather than merely new fields for an existing table of the database. Second, new tables or new fields are not limited to being approximations to some target concept; instead, the new approach performs a type of predicate invention. The new approach avoids the classical problem with predicate invention, of learning many useless predicates, by keeping only new fields or tables (i.e., new predicates) that immediately improve the performance of the statistical model. Third, retained fields or tables can then be used in the definitions of further new fields or tables. We evaluate the new view learning approach on three relational classification tasks.", :author (#search_api.search_api.Author{:id 747351, :first-name nil, :last-name nil, :full-name "Jesse Davis"} #search_api.search_api.Author{:id 629180, :first-name nil, :last-name nil, :full-name "Irene M. Ong"} #search_api.search_api.Author{:id 1227127, :first-name nil, :last-name nil, :full-name "Jan Struyf"} #search_api.search_api.Author{:id 750446, :first-name nil, :last-name nil, :full-name "Elizabeth S. Burnside"} #search_api.search_api.Author{:id 138183, :first-name nil, :last-name nil, :full-name "David Page"} #search_api.search_api.Author{:id 1352436, :first-name nil, :last-name nil, :full-name "Vítor Santos Costa"}), :year 2007, :venue "IJCAI", :ncit 28, :string "Change of Representation for Statistical Relational Learning.. Statistical relational learning (SRL) algorithms learn statistical models from relational data, such as that stored in a relational database. We previously introduced view learning for SRL, in which the view of a relational database can be automatically modified, yielding more accurate statistical models. The present paper presents SAYU-VISTA, an algorithm which advances beyond the initial view learning approach in three ways. First, it learns views that introduce new relational tables, rather than merely new fields for an existing table of the database. Second, new tables or new fields are not limited to being approximations to some target concept; instead, the new approach performs a type of predicate invention. The new approach avoids the classical problem with predicate invention, of learning many useless predicates, by keeping only new fields or tables (i.e., new predicates) that immediately improve the performance of the statistical model. Third, retained fields or tables can then be used in the definitions of further new fields or tables. We evaluate the new view learning approach on three relational classification tasks.", :doc-id "Change of Representation for Statistical Relational Learning. 2007 Jesse Davis, Irene M. Ong, Jan Struyf, Elizabeth S. Burnside, David Page, Vítor Santos Costa"}, 271964 #search_api.search_api.Paper{:id 271964, :key "conf/icann/KopeczM97", :title "Relative Time Scales in the Self-Organization of Pattern Classification: From One-Shot to Statistical Learning.", :abstract nil, :author (#search_api.search_api.Author{:id 840288, :first-name nil, :last-name nil, :full-name "Klaus Kopecz"} #search_api.search_api.Author{:id 379366, :first-name nil, :last-name nil, :full-name "Karim Mohraz"}), :year 1997, :venue "ICANN", :ncit 0, :string "Relative Time Scales in the Self-Organization of Pattern Classification: From One-Shot to Statistical Learning.. ", :doc-id "Relative Time Scales in the Self-Organization of Pattern Classification: From One-Shot to Statistical Learning. 1997 Klaus Kopecz, Karim Mohraz"}, 1009341 #search_api.search_api.Paper{:id 1009341, :key "journals/ml/BlockeelJK06", :title "Introduction to the special issue on multi-relational data mining and statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 882341, :first-name nil, :last-name nil, :full-name "Hendrik Blockeel"} #search_api.search_api.Author{:id 408384, :first-name nil, :last-name nil, :full-name "David Jensen"} #search_api.search_api.Author{:id 954022, :first-name nil, :last-name nil, :full-name "Stefan Kramer"}), :year 2006, :venue "Machine Learning", :ncit 2, :string "Introduction to the special issue on multi-relational data mining and statistical relational learning.. ", :doc-id "Introduction to the special issue on multi-relational data mining and statistical relational learning. 2006 Hendrik Blockeel, David Jensen, Stefan Kramer"}, 622461 #search_api.search_api.Paper{:id 622461, :key "conf/stoc/Kearns93", :title "Efficient noise-tolerant learning from statistical queries.", :abstract "In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :author (#search_api.search_api.Author{:id 1249307, :first-name nil, :last-name nil, :full-name "Michael J. Kearns"}), :year 1993, :venue "STOC", :ncit 437, :string "Efficient noise-tolerant learning from statistical queries.. In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :doc-id "Efficient noise-tolerant learning from statistical queries. 1993 Michael J. Kearns"}, 1254526 #search_api.search_api.Paper{:id 1254526, :key "conf/pkdd/RettingerNT09", :title "Statistical Relational Learning with Formal Ontologies.", :abstract "We propose a learning approach for integrating formal knowledge into statistical inference by exploiting ontologies as a semantically rich and fully formal representation of prior knowledge. The logical constraints deduced from ontologies can be utilized to enhance and control the learning task by enforcing description logic satisfiability in a latent multi-relational graphical model. To demonstrate the feasibility of our approach we provide experiments using real world social network data in form of a $\\mathcal{SHOIN}(D)$ ontology. The results illustrate two main practical advancements: First, entities and entity relationships can be analyzed via the latent model structure. Second, enforcing the ontological constraints guarantees that the learned model does not predict inconsistent relations. In our experiments, this leads to an improved predictive performance.", :author (#search_api.search_api.Author{:id 444383, :first-name nil, :last-name nil, :full-name "Achim Rettinger"} #search_api.search_api.Author{:id 222492, :first-name nil, :last-name nil, :full-name "Matthias Nickles"} #search_api.search_api.Author{:id 680469, :first-name nil, :last-name nil, :full-name "Volker Tresp"}), :year 2009, :venue "ECML/PKDD (2)", :ncit 13, :string "Statistical Relational Learning with Formal Ontologies.. We propose a learning approach for integrating formal knowledge into statistical inference by exploiting ontologies as a semantically rich and fully formal representation of prior knowledge. The logical constraints deduced from ontologies can be utilized to enhance and control the learning task by enforcing description logic satisfiability in a latent multi-relational graphical model. To demonstrate the feasibility of our approach we provide experiments using real world social network data in form of a $\\mathcal{SHOIN}(D)$ ontology. The results illustrate two main practical advancements: First, entities and entity relationships can be analyzed via the latent model structure. Second, enforcing the ontological constraints guarantees that the learned model does not predict inconsistent relations. In our experiments, this leads to an improved predictive performance.", :doc-id "Statistical Relational Learning with Formal Ontologies. 2009 Achim Rettinger, Matthias Nickles, Volker Tresp"}, 3782751 #search_api.search_api.Paper{:id 3782751, :key "conf/ilp/SahaSR12", :title "What Kinds of Relational Features Are Useful for Statistical Learning?", :abstract nil, :author (#search_api.search_api.Author{:id 914315, :first-name nil, :last-name nil, :full-name "Amrita Saha"} #search_api.search_api.Author{:id 1118213, :first-name nil, :last-name nil, :full-name "Ashwin Srinivasan"} #search_api.search_api.Author{:id 376403, :first-name nil, :last-name nil, :full-name "Ganesh Ramakrishnan"}), :year 2012, :venue "ILP", :ncit 0, :string "What Kinds of Relational Features Are Useful for Statistical Learning?. ", :doc-id "What Kinds of Relational Features Are Useful for Statistical Learning? 2012 Amrita Saha, Ashwin Srinivasan, Ganesh Ramakrishnan"}}, :topic-index {1770656 {:id 1770656, :title "Numerical linear algebra", :type :wiki-api.core/category}, 11737376 {:id 11737376, :title "Statistical natural language processing", :type :wiki-api.core/category}, 3424576 {:id 3424576, :title "Kernel methods", :type :wiki-api.core/article}, 2157920 {:id 2157920, :title "Technology in society", :type :wiki-api.core/category}, 797088 {:id 797088, :title "Computer vision", :type :wiki-api.core/category}, 1045088 {:id 1045088, :title "Semiotics", :type :wiki-api.core/category}, 470752 {:id 470752, :title "Expectation–maximization algorithm", :type :wiki-api.core/article}, 4094720 {:id 4094720, :title "Latent class model", :type :wiki-api.core/article}, 331680 {:id 331680, :title "Edge detection", :type :wiki-api.core/article}, 336897 {:id 336897, :title "Function approximation", :type :wiki-api.core/article}, 15522913 {:id 15522913, :title "Predicate logic", :type :wiki-api.core/category}, 6539521 {:id 6539521, :title "Statistical theory", :type :wiki-api.core/category}, 871681 {:id 871681, :title "Mixture model", :type :wiki-api.core/article}, 30774561 {:id 30774561, :title "1956 in computer science", :type :wiki-api.core/category}, 32611713 {:id 32611713, :title "Emerging technologies", :type :wiki-api.core/category}, 772545 {:id 772545, :title "Probability distributions", :type :wiki-api.core/category}, 22532673 {:id 22532673, :title "Cluster analysis", :type :wiki-api.core/category}, 2883137 {:id 2883137, :title "Parametric model", :type :wiki-api.core/article}, 17306305 {:id 17306305, :title "Statistical classification", :type :wiki-api.core/category}, 693985 {:id 693985, :title "Probability theory", :type :wiki-api.core/category}, 1911810 {:id 1911810, :title "Language model", :type :wiki-api.core/article}, 1557538 {:id 1557538, :title "Research methods", :type :wiki-api.core/category}, 3697698 {:id 3697698, :title "Statistical tests", :type :wiki-api.core/category}, 697506 {:id 697506, :title "Numerical analysis", :type :wiki-api.core/category}, 17506498 {:id 17506498, :title "Logic and statistics", :type :wiki-api.core/category}, 9500290 {:id 9500290, :title "Experiments", :type :wiki-api.core/category}, 733122 {:id 733122, :title "Grammar", :type :wiki-api.core/category}, 11034627 {:id 11034627, :title "Industrial engineering", :type :wiki-api.core/category}, 8330403 {:id 8330403, :title "Dirichlet process", :type :wiki-api.core/article}, 160995 {:id 160995, :title "Statistical significance", :type :wiki-api.core/article}, 22712867 {:id 22712867, :title "Justification", :type :wiki-api.core/category}, 24960643 {:id 24960643, :title "Statistical outliers", :type :wiki-api.core/category}, 1028771 {:id 1028771, :title "Control theory", :type :wiki-api.core/category}, 23173987 {:id 23173987, :title "Heuristic algorithms", :type :wiki-api.core/category}, 1069987 {:id 1069987, :title "Organizations in cryptography", :type :wiki-api.core/category}, 700355 {:id 700355, :title "Artificial intelligence", :type :wiki-api.core/category}, 34044100 {:id 34044100, :title "Optimization algorithms and methods", :type :wiki-api.core/category}, 22691076 {:id 22691076, :title "Graphical models", :type :wiki-api.core/category}, 787876 {:id 787876, :title "Computational linguistics", :type :wiki-api.core/category}, 8050180 {:id 8050180, :title "Multi-agent systems", :type :wiki-api.core/category}, 1098276 {:id 1098276, :title "Mathematical terminology", :type :wiki-api.core/category}, 700292 {:id 700292, :title "Scientific method", :type :wiki-api.core/category}, 26308484 {:id 26308484, :title "Summary statistics for contingency tables", :type :wiki-api.core/category}, 1487877 {:id 1487877, :title "Sampling techniques", :type :wiki-api.core/category}, 17193061 {:id 17193061, :title "Statistical data types", :type :wiki-api.core/category}, 20924581 {:id 20924581, :title "Latent variable models", :type :wiki-api.core/category}, 22820037 {:id 22820037, :title "Scientific revolution", :type :wiki-api.core/category}, 692453 {:id 692453, :title "Matrix theory", :type :wiki-api.core/category}, 9387237 {:id 9387237, :title "Parametric statistics", :type :wiki-api.core/category}, 30982373 {:id 30982373, :title "Missing data", :type :wiki-api.core/category}, 31176997 {:id 31176997, :title "Support vector machines", :type :wiki-api.core/category}, 2675045 {:id 2675045, :title "Quality", :type :wiki-api.core/category}, 21764485 {:id 21764485, :title "Statistical ratios", :type :wiki-api.core/category}, 18462661 {:id 18462661, :title "Resampling (statistics)", :type :wiki-api.core/category}, 1008581 {:id 1008581, :title "Operations research", :type :wiki-api.core/category}, 8156101 {:id 8156101, :title "Statistical methods", :type :wiki-api.core/category}, 5486694 {:id 5486694, :title "1972 introductions", :type :wiki-api.core/category}, 22589574 {:id 22589574, :title "Instance-based learning", :type :wiki-api.core/article}, 24104134 {:id 24104134, :title "Conditional probability", :type :wiki-api.core/article}, 30876902 {:id 30876902, :title "Independent and identically distributed random variables", :type :wiki-api.core/article}, 693702 {:id 693702, :title "Functional programming", :type :wiki-api.core/category}, 140806 {:id 140806, :title "Maximum likelihood", :type :wiki-api.core/article}, 22476294 {:id 22476294, :title "Matrix decompositions", :type :wiki-api.core/category}, 17503782 {:id 17503782, :title "Formal sciences", :type :wiki-api.core/category}, 10678854 {:id 10678854, :title "Collective intelligence", :type :wiki-api.core/category}, 744038 {:id 744038, :title "Philosophy of language", :type :wiki-api.core/category}, 6539078 {:id 6539078, :title "Multivariate statistics", :type :wiki-api.core/category}, 709574 {:id 709574, :title "Human communication", :type :wiki-api.core/category}, 10978278 {:id 10978278, :title "Quantitative research", :type :wiki-api.core/category}, 11504647 {:id 11504647, :title "Systems of formal logic", :type :wiki-api.core/category}, 10974311 {:id 10974311, :title "Conditional constructs", :type :wiki-api.core/category}, 19667111 {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}, 966983 {:id 966983, :title "Applied mathematics", :type :wiki-api.core/category}, 226631 {:id 226631, :title "Logistic regression", :type :wiki-api.core/article}, 5065063 {:id 5065063, :title "Network architecture", :type :wiki-api.core/category}, 6759 {:id 6759, :title "Context-free grammar", :type :wiki-api.core/article}, 821959 {:id 821959, :title "Matrices", :type :wiki-api.core/category}, 804551 {:id 804551, :title "Psychometrics", :type :wiki-api.core/category}, 10983 {:id 10983, :title "First-order logic", :type :wiki-api.core/article}, 5175143 {:id 5175143, :title "Geostatistics", :type :wiki-api.core/category}, 318439 {:id 318439, :title "Text mining", :type :wiki-api.core/article}, 380008 {:id 380008, :title "Probably approximately correct learning", :type :wiki-api.core/article}, 1126536 {:id 1126536, :title "Optimization problem", :type :wiki-api.core/article}, 191752 {:id 191752, :title "Covariance matrix", :type :wiki-api.core/article}, 726312 {:id 726312, :title "Bioinformatics", :type :wiki-api.core/category}, 357672 {:id 357672, :title "Posterior probability", :type :wiki-api.core/article}, 693800 {:id 693800, :title "Geography", :type :wiki-api.core/category}, 878280 {:id 878280, :title "Standards organizations", :type :wiki-api.core/category}, 31024872 {:id 31024872, :title "Normal distribution", :type :wiki-api.core/category}, 744360 {:id 744360, :title "Networks", :type :wiki-api.core/category}, 21384136 {:id 21384136, :title "Computer-assisted translation", :type :wiki-api.core/category}, 3985352 {:id 3985352, :title "Ensemble learning", :type :wiki-api.core/category}, 27955209 {:id 27955209, :title "Model selection", :type :wiki-api.core/category}, 25825321 {:id 25825321, :title "Mathematical sciences", :type :wiki-api.core/category}, 1587689 {:id 1587689, :title "Estimation theory", :type :wiki-api.core/category}, 140841 {:id 140841, :title "Sufficient statistic", :type :wiki-api.core/article}, 14426697 {:id 14426697, :title "Market research", :type :wiki-api.core/category}, 5206601 {:id 5206601, :title "Data mining", :type :wiki-api.core/category}, 3864201 {:id 3864201, :title "Permutations", :type :wiki-api.core/category}, 694025 {:id 694025, :title "Information theory", :type :wiki-api.core/category}, 27593 {:id 27593, :title "Independence (probability theory)", :type :wiki-api.core/article}, 6538378 {:id 6538378, :title "Statistical data sets", :type :wiki-api.core/category}, 916810 {:id 916810, :title "Spam filtering", :type :wiki-api.core/category}, 1152426 {:id 1152426, :title "Philosophy of mathematics", :type :wiki-api.core/category}, 504458 {:id 504458, :title "Conditional probability distribution", :type :wiki-api.core/article}, 700074 {:id 700074, :title "Logical fallacies", :type :wiki-api.core/category}, 708266 {:id 708266, :title "Artificial intelligence researchers", :type :wiki-api.core/category}, 30787370 {:id 30787370, :title "Compiler construction", :type :wiki-api.core/category}, 17594154 {:id 17594154, :title "Decision trees", :type :wiki-api.core/category}, 16989227 {:id 16989227, :title "Data", :type :wiki-api.core/category}, 87339 {:id 87339, :title "Naive Bayes classifier", :type :wiki-api.core/article}, 4289067 {:id 4289067, :title "Computational neuroscience", :type :wiki-api.core/category}, 5673643 {:id 5673643, :title "Risk analysis", :type :wiki-api.core/category}, 8135339 {:id 8135339, :title "Articles with example Haskell code", :type :wiki-api.core/category}, 1228459 {:id 1228459, :title "Business intelligence", :type :wiki-api.core/category}, 23890667 {:id 23890667, :title "Knowledge representation languages", :type :wiki-api.core/category}, 1055691 {:id 1055691, :title "Learning", :type :wiki-api.core/category}, 45035 {:id 45035, :title "Likelihood-ratio test", :type :wiki-api.core/article}, 17990732 {:id 17990732, :title "Theory of probability distributions", :type :wiki-api.core/category}, 5038508 {:id 5038508, :title "Information, knowledge, and uncertainty", :type :wiki-api.core/category}, 11472332 {:id 11472332, :title "Articles containing proofs", :type :wiki-api.core/category}, 897612 {:id 897612, :title "Syntax", :type :wiki-api.core/category}, 1817228 {:id 1817228, :title "Training set", :type :wiki-api.core/article}, 1855180 {:id 1855180, :title "Problem solving", :type :wiki-api.core/category}, 946892 {:id 946892, :title "Econometrics", :type :wiki-api.core/category}, 28335948 {:id 28335948, :title "Articles with inconsistent citation formats", :type :wiki-api.core/category}, 7279789 {:id 7279789, :title "Information retrieval", :type :wiki-api.core/category}, 3234221 {:id 3234221, :title "Analysis of algorithms", :type :wiki-api.core/category}, 31195693 {:id 31195693, :title "Design of experiments", :type :wiki-api.core/category}, 22991469 {:id 22991469, :title "Inductive reasoning", :type :wiki-api.core/category}, 472877 {:id 472877, :title "Prior probability", :type :wiki-api.core/article}, 22187885 {:id 22187885, :title "List of machine learning algorithms", :type :wiki-api.core/article}, 962413 {:id 962413, :title "Image processing", :type :wiki-api.core/category}, 15325165 {:id 15325165, :title "Data clustering algorithms", :type :wiki-api.core/category}, 772270 {:id 772270, :title "Philosophy of science", :type :wiki-api.core/category}, 8398 {:id 8398, :title "Dimension (mathematics and physics)", :type :wiki-api.core/article}, 1019150 {:id 1019150, :title "Machine translation", :type :wiki-api.core/category}, 26263822 {:id 26263822, :title "Packaging machinery", :type :wiki-api.core/category}, 1179950 {:id 1179950, :title "Feature selection", :type :wiki-api.core/article}, 36286862 {:id 36286862, :title "Behavioral and social facets of systemic risk", :type :wiki-api.core/category}, 22958 {:id 22958, :title "Sample space", :type :wiki-api.core/article}, 1195726 {:id 1195726, :title "Error", :type :wiki-api.core/category}, 9588430 {:id 9588430, :title "Higher-order statistics", :type :wiki-api.core/article}, 35718126 {:id 35718126, :title "Exponential family distributions", :type :wiki-api.core/category}, 183503 {:id 183503, :title "Description logic", :type :wiki-api.core/article}, 2023695 {:id 2023695, :title "Pattern matching", :type :wiki-api.core/category}, 8495 {:id 8495, :title "Data set", :type :wiki-api.core/article}, 767343 {:id 767343, :title "Stochastic processes", :type :wiki-api.core/category}, 801135 {:id 801135, :title "Conditional independence", :type :wiki-api.core/article}, 17917391 {:id 17917391, :title "Missing completely at random", :type :wiki-api.core/article}, 26478223 {:id 26478223, :title "Reasoning", :type :wiki-api.core/category}, 14343887 {:id 14343887, :title "Precision and recall", :type :wiki-api.core/article}, 32549775 {:id 32549775, :title "Bayesian inference", :type :wiki-api.core/category}, 21053327 {:id 21053327, :title "United States Department of Commerce agencies", :type :wiki-api.core/category}, 706543 {:id 706543, :title "Machine learning", :type :wiki-api.core/category}, 772240 {:id 772240, :title "Epistemology", :type :wiki-api.core/category}, 22166736 {:id 22166736, :title "Non-classical logic", :type :wiki-api.core/category}, 914736 {:id 914736, :title "Programming paradigms", :type :wiki-api.core/category}, 26743152 {:id 26743152, :title "Conceptual models", :type :wiki-api.core/category}, 690672 {:id 690672, :title "Abstract algebra", :type :wiki-api.core/category}, 536080 {:id 536080, :title "Student's t-test", :type :wiki-api.core/article}, 1754736 {:id 1754736, :title "Cybernetics", :type :wiki-api.core/category}, 22121360 {:id 22121360, :title "Fellows of the Association for the Advancement of Artificial Intelligence", :type :wiki-api.core/category}, 22705265 {:id 22705265, :title "Probability assessment", :type :wiki-api.core/category}, 25174161 {:id 25174161, :title "Open problems", :type :wiki-api.core/category}, 958609 {:id 958609, :title "Statistical mechanics", :type :wiki-api.core/category}, 29549713 {:id 29549713, :title "Dimension reduction", :type :wiki-api.core/category}, 1331441 {:id 1331441, :title "Document classification", :type :wiki-api.core/article}, 31454449 {:id 31454449, :title "Human–machine interaction", :type :wiki-api.core/category}, 17193265 {:id 17193265, :title "Statistical inference", :type :wiki-api.core/category}, 25304497 {:id 25304497, :title "Government agencies established in 1901", :type :wiki-api.core/category}, 35997105 {:id 35997105, :title "Generalized linear models", :type :wiki-api.core/category}, 1792433 {:id 1792433, :title "Maximum a posteriori estimation", :type :wiki-api.core/article}, 763505 {:id 763505, :title "Signal processing", :type :wiki-api.core/category}, 33748657 {:id 33748657, :title "Geometric measurement", :type :wiki-api.core/category}, 434897 {:id 434897, :title "Hough transform", :type :wiki-api.core/article}, 2871217 {:id 2871217, :title "Educational psychology", :type :wiki-api.core/category}, 34310097 {:id 34310097, :title "Computational learning theory", :type :wiki-api.core/category}, 1323985 {:id 1323985, :title "Markov random field", :type :wiki-api.core/article}, 5490673 {:id 5490673, :title "Quality control", :type :wiki-api.core/category}, 427282 {:id 427282, :title "Mutual information", :type :wiki-api.core/article}, 4873906 {:id 4873906, :title "Systems engineering", :type :wiki-api.core/category}, 34379506 {:id 34379506, :title "Epistemology of science", :type :wiki-api.core/category}, 4861714 {:id 4861714, :title "Hypothesis testing", :type :wiki-api.core/category}, 29313810 {:id 29313810, :title "Gaithersburg, Maryland", :type :wiki-api.core/category}, 792595 {:id 792595, :title "Computational physics", :type :wiki-api.core/category}, 21523 {:id 21523, :title "Artificial neural network", :type :wiki-api.core/article}, 8707155 {:id 8707155, :title "Statistical shape analysis", :type :wiki-api.core/article}, 689427 {:id 689427, :title "Latent semantic analysis", :type :wiki-api.core/article}, 17179027 {:id 17179027, :title "Categorical data", :type :wiki-api.core/category}, 7699923 {:id 7699923, :title "Speech processing", :type :wiki-api.core/category}, 34698963 {:id 34698963, :title "Methods in sociology", :type :wiki-api.core/category}, 7484211 {:id 7484211, :title "Fellows of the American Academy of Arts and Sciences", :type :wiki-api.core/category}, 36425555 {:id 36425555, :title "Factor analysis", :type :wiki-api.core/category}, 954323 {:id 954323, :title "Tikhonov regularization", :type :wiki-api.core/article}, 29003796 {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category}, 36477012 {:id 36477012, :title "Concepts in physics", :type :wiki-api.core/category}, 6890644 {:id 6890644, :title "Singular value decomposition", :type :wiki-api.core/category}, 172244 {:id 172244, :title "Simulated annealing", :type :wiki-api.core/article}, 9387252 {:id 9387252, :title "Non-parametric statistics", :type :wiki-api.core/category}, 76340 {:id 76340, :title "Principal component analysis", :type :wiki-api.core/article}, 1009204 {:id 1009204, :title "Information science", :type :wiki-api.core/category}, 12469844 {:id 12469844, :title "Wikipedia articles with ASCII art", :type :wiki-api.core/category}, 10690420 {:id 10690420, :title "Cognitive neuroscience", :type :wiki-api.core/category}, 2210772 {:id 2210772, :title "Continuous distributions", :type :wiki-api.core/category}, 879637 {:id 879637, :title "Joint probability distribution", :type :wiki-api.core/article}, 8439989 {:id 8439989, :title "Jewish American scientists", :type :wiki-api.core/category}, 5657877 {:id 5657877, :title "Type I and type II errors", :type :wiki-api.core/article}, 3670357 {:id 3670357, :title "Markov logic network", :type :wiki-api.core/article}, 17735029 {:id 17735029, :title "Probability interpretations", :type :wiki-api.core/category}, 36163957 {:id 36163957, :title "Stochastic simulation", :type :wiki-api.core/category}, 31229429 {:id 31229429, :title "Fellow Members of the IEEE", :type :wiki-api.core/category}, 17612277 {:id 17612277, :title "Multi-robot systems", :type :wiki-api.core/category}, 960021 {:id 960021, :title "Natural language processing", :type :wiki-api.core/category}, 716309 {:id 716309, :title "Cartography", :type :wiki-api.core/category}, 20941685 {:id 20941685, :title "Tasks of Natural language processing", :type :wiki-api.core/category}, 22718453 {:id 22718453, :title "Markov networks", :type :wiki-api.core/category}, 5200150 {:id 5200150, :title "Monte Carlo methods", :type :wiki-api.core/category}, 313942 {:id 313942, :title "Local search (optimization)", :type :wiki-api.core/article}, 1991254 {:id 1991254, :title "Classification algorithms", :type :wiki-api.core/category}, 66294 {:id 66294, :title "Reinforcement learning", :type :wiki-api.core/article}, 1034006 {:id 1034006, :title "Particle physics", :type :wiki-api.core/category}, 8190902 {:id 8190902, :title "Anomaly detection", :type :wiki-api.core/article}, 15690807 {:id 15690807, :title "Count data", :type :wiki-api.core/article}, 5699671 {:id 5699671, :title "Programming language topics", :type :wiki-api.core/category}, 2276471 {:id 2276471, :title "Databases", :type :wiki-api.core/category}, 2538775 {:id 2538775, :title "Predictive modelling", :type :wiki-api.core/article}, 17684887 {:id 17684887, :title "Statistical approximations", :type :wiki-api.core/category}, 32190039 {:id 32190039, :title "Speech", :type :wiki-api.core/category}, 1053303 {:id 1053303, :title "Statistical learning theory", :type :wiki-api.core/article}, 1532663 {:id 1532663, :title "Artificial intelligence applications", :type :wiki-api.core/category}, 6533911 {:id 6533911, :title "Bayesian statistics", :type :wiki-api.core/category}, 5646263 {:id 5646263, :title "University of California, Los Angeles faculty", :type :wiki-api.core/category}, 30319607 {:id 30319607, :title "Human–computer interaction", :type :wiki-api.core/category}, 4842680 {:id 4842680, :title "Data security", :type :wiki-api.core/category}, 36312376 {:id 36312376, :title "Belief revision", :type :wiki-api.core/category}, 405944 {:id 405944, :title "Time complexity", :type :wiki-api.core/article}, 12535256 {:id 12535256, :title "Kernel methods for machine learning", :type :wiki-api.core/category}, 16920 {:id 16920, :title "Knowledge representation and reasoning", :type :wiki-api.core/article}, 6535800 {:id 6535800, :title "Covariance and correlation", :type :wiki-api.core/category}, 4628120 {:id 4628120, :title "Computational resources", :type :wiki-api.core/category}, 694008 {:id 694008, :title "Statistics", :type :wiki-api.core/category}, 598776 {:id 598776, :title "Score (statistics)", :type :wiki-api.core/article}, 233497 {:id 233497, :title "Unsupervised learning", :type :wiki-api.core/article}, 784409 {:id 784409, :title "Marketing", :type :wiki-api.core/category}, 1171513 {:id 1171513, :title "Neural networks", :type :wiki-api.core/category}, 990361 {:id 990361, :title "Theoretical computer science", :type :wiki-api.core/category}, 3224825 {:id 3224825, :title "Scientific modeling", :type :wiki-api.core/category}, 1406201 {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category}, 27248985 {:id 27248985, :title "Mathematical concepts", :type :wiki-api.core/category}, 9272793 {:id 9272793, :title "Computational statistics", :type :wiki-api.core/category}, 1009209 {:id 1009209, :title "Information", :type :wiki-api.core/category}, 690777 {:id 690777, :title "Linear algebra", :type :wiki-api.core/category}, 28927577 {:id 28927577, :title "Probabilistic models", :type :wiki-api.core/category}, 4822234 {:id 4822234, :title "Mathematical and quantitative methods (economics)", :type :wiki-api.core/category}, 160986 {:id 160986, :title "Order statistic", :type :wiki-api.core/article}, 1686106 {:id 1686106, :title "Israeli computer scientists", :type :wiki-api.core/category}, 691866 {:id 691866, :title "Functional analysis", :type :wiki-api.core/category}, 6537978 {:id 6537978, :title "Summary statistics", :type :wiki-api.core/category}, 21917434 {:id 21917434, :title "Information Age", :type :wiki-api.core/category}, 4890 {:id 4890, :title "Bayesian probability", :type :wiki-api.core/article}, 28979098 {:id 28979098, :title "Distributions with conjugate priors", :type :wiki-api.core/category}, 972730 {:id 972730, :title "1936 births", :type :wiki-api.core/category}, 17652827 {:id 17652827, :title "Statistical dependence", :type :wiki-api.core/category}, 951835 {:id 951835, :title "Computer data", :type :wiki-api.core/category}, 709243 {:id 709243, :title "Communication", :type :wiki-api.core/category}, 693979 {:id 693979, :title "Model theory", :type :wiki-api.core/category}, 598971 {:id 598971, :title "Fisher information", :type :wiki-api.core/article}, 796635 {:id 796635, :title "Knowledge representation", :type :wiki-api.core/category}, 4594748 {:id 4594748, :title "Computational problems", :type :wiki-api.core/category}, 958652 {:id 958652, :title "Speech recognition", :type :wiki-api.core/category}, 4183228 {:id 4183228, :title "Israeli philosophers", :type :wiki-api.core/category}, 29421852 {:id 29421852, :title "M-estimators", :type :wiki-api.core/category}, 14482748 {:id 14482748, :title "Data analysis", :type :wiki-api.core/category}, 20189596 {:id 20189596, :title "Probabilistic complexity theory", :type :wiki-api.core/category}, 17923612 {:id 17923612, :title "Types of probability distributions", :type :wiki-api.core/category}, 699964 {:id 699964, :title "Judea Pearl", :type :wiki-api.core/article}, 17228412 {:id 17228412, :title "Spatial data analysis", :type :wiki-api.core/category}, 1775388 {:id 1775388, :title "K-nearest neighbor algorithm", :type :wiki-api.core/article}, 6075324 {:id 6075324, :title "National Institute of Standards and Technology", :type :wiki-api.core/category}, 1188828 {:id 1188828, :title "Logic programming", :type :wiki-api.core/category}, 33547228 {:id 33547228, :title "Machine learning algorithms", :type :wiki-api.core/category}, 5232797 {:id 5232797, :title "Rutgers University alumni", :type :wiki-api.core/category}, 31813917 {:id 31813917, :title "Hidden Markov models", :type :wiki-api.core/category}, 37832157 {:id 37832157, :title "Deductive reasoning", :type :wiki-api.core/category}, 18956829 {:id 18956829, :title "Mathematical optimization", :type :wiki-api.core/category}, 6536797 {:id 6536797, :title "Statistical algorithms", :type :wiki-api.core/category}, 20926 {:id 20926, :title "Supervised learning", :type :wiki-api.core/article}, 26339806 {:id 26339806, :title "Auxiliary sciences of history", :type :wiki-api.core/category}, 24059390 {:id 24059390, :title "Markov models", :type :wiki-api.core/category}, 4003390 {:id 4003390, :title "Evaluation methods", :type :wiki-api.core/category}, 18070174 {:id 18070174, :title "Engineering statistics", :type :wiki-api.core/category}, 1966814 {:id 1966814, :title "C4.5 algorithm", :type :wiki-api.core/article}, 946910 {:id 946910, :title "Decision theory", :type :wiki-api.core/category}, 699134 {:id 699134, :title "Formal languages", :type :wiki-api.core/category}, 3782398 {:id 3782398, :title "Living people", :type :wiki-api.core/category}, 3175294 {:id 3175294, :title "Dimension", :type :wiki-api.core/category}, 1228702 {:id 1228702, :title "Product management", :type :wiki-api.core/category}, 22042655 {:id 22042655, :title "Statistical principles", :type :wiki-api.core/category}, 21981503 {:id 21981503, :title "Conditionals", :type :wiki-api.core/category}, 693727 {:id 693727, :title "Computational complexity theory", :type :wiki-api.core/category}, 17193471 {:id 17193471, :title "Statistical models", :type :wiki-api.core/category}, 21936671 {:id 21936671, :title "Non-parametric Bayesian methods", :type :wiki-api.core/category}, 3190431 {:id 3190431, :title "Spatial analysis", :type :wiki-api.core/article}, 1718975 {:id 1718975, :title "Bayesian networks", :type :wiki-api.core/category}, 28978911 {:id 28978911, :title "Conjugate prior distributions", :type :wiki-api.core/category}, 3515391 {:id 3515391, :title "Mathematical modeling", :type :wiki-api.core/category}}, :interface :search, :complete-labeling nil}}, :submap #utils.core.Success{:value #topic_maps.core.TopicMap{:topic-graph #graphs.core.Digraph{:nodes #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category} {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, :in-map {{:id 5206601, :title "Data mining", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} #{}, {:id 706543, :title "Machine learning", :type :wiki-api.core/category} #{}, {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article} #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}}, :out-map {{:id 5206601, :title "Data mining", :type :wiki-api.core/category} #{}, {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} #{}, {:id 706543, :title "Machine learning", :type :wiki-api.core/category} #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} #{}, {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article} #{}}}, :topic-docs #graphs.core.Digraph{:nodes #{1185824 574400 687201 1009537 3074945 1009218 1280035 473123 336067 {:id 5206601, :title "Data mining", :type :wiki-api.core/category} 675364 138052 388102 303622 3258022 473095 1033319 746727 1323591 3382280 893160 2927016 2881512 473321 91401 {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} 472937 3786794 2927018 950858 3258026 1288331 797516 3074924 1185901 1028237 20109 541902 1010126 1009230 473199 472783 952368 2775184 1319504 190192 1279856 498833 {:id 706543, :title "Machine learning", :type :wiki-api.core/category} 2784531 1009299 952340 985236 1017236 3623508 1009524 192565 1236661 302837 1032117 574389 57463 746711 388504 644121 {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} 1250938 294586 1026778 3341018 3402586 937563 {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article} 391548 271964 1009341 622461 3782751}, :in-map {1185824 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 574400 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category}}, 687201 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 1009537 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category}}, 3074945 #{{:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 1009218 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 1280035 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 473123 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category}}, 336067 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, {:id 5206601, :title "Data mining", :type :wiki-api.core/category} #{}, 675364 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 138052 #{{:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category}}, 388102 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 303622 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 3258022 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 473095 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 1033319 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 746727 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 1323591 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 3382280 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 893160 #{{:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 2927016 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 2881512 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 473321 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 91401 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category}}, {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} #{}, 472937 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 3786794 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category}}, 2927018 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 950858 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 3258026 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 1288331 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category}}, 797516 #{{:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 3074924 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 1185901 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 1028237 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 20109 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 541902 #{{:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category}}, 1010126 #{{:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category}}, 1009230 #{{:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 473199 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 472783 #{{:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 952368 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category}}, 2775184 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category}}, 1319504 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category}}, 190192 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 1279856 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 498833 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 706543, :title "Machine learning", :type :wiki-api.core/category} #{}, 2784531 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 1009299 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 952340 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 985236 #{{:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category}}, 1017236 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category}}, 3623508 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 1009524 #{{:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 192565 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 1236661 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 302837 #{{:id 1406201, :title "Search algorithms", :type :wiki-api.core/category}}, 1032117 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 574389 #{{:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 57463 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 746711 #{{:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 388504 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 644121 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} #{}, 1250938 #{{:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} {:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 294586 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 1026778 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 3341018 #{{:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 3402586 #{{:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category}}, 937563 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category}}, {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article} #{}, 391548 #{{:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 271964 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 1009341 #{{:id 5206601, :title "Data mining", :type :wiki-api.core/category} {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article}}, 622461 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}, 3782751 #{{:id 706543, :title "Machine learning", :type :wiki-api.core/category}}}, :out-map {1185824 #{}, 574400 #{}, 687201 #{}, 1009537 #{}, 3074945 #{}, 1009218 #{}, 1280035 #{}, 473123 #{}, 336067 #{}, {:id 5206601, :title "Data mining", :type :wiki-api.core/category} #{574400 1009537 473123 303622 91401 3786794 1288331 473199 952368 2775184 1319504 190192 1017236 937563 1009341}, 675364 #{}, 138052 #{}, 388102 #{}, 303622 #{}, 3258022 #{}, 473095 #{}, 1033319 #{}, 746727 #{}, 1323591 #{}, 3382280 #{}, 893160 #{}, 2927016 #{}, 2881512 #{}, 473321 #{}, 91401 #{}, {:id 1406201, :title "Search algorithms", :type :wiki-api.core/category} #{893160 797516 1009230 472783 1009524 302837 1250938}, 472937 #{}, 3786794 #{}, 2927018 #{}, 950858 #{}, 3258026 #{}, 1288331 #{}, 797516 #{}, 3074924 #{}, 1185901 #{}, 1028237 #{}, 20109 #{}, 541902 #{}, 1010126 #{}, 1009230 #{}, 473199 #{}, 472783 #{}, 952368 #{}, 2775184 #{}, 1319504 #{}, 190192 #{}, 1279856 #{}, 498833 #{}, {:id 706543, :title "Machine learning", :type :wiki-api.core/category} #{1185824 1009218 1280035 675364 473095 1033319 1323591 3382280 893160 2881512 473321 472937 950858 797516 1185901 1028237 20109 1009230 472783 1279856 498833 2784531 1009299 952340 1009524 192565 1236661 1032117 388504 644121 1250938 294586 1026778 271964 622461 3782751}, 2784531 #{}, 1009299 #{}, 952340 #{}, 985236 #{}, 1017236 #{}, 3623508 #{}, 1009524 #{}, 192565 #{}, 1236661 #{}, 302837 #{}, 1032117 #{}, 574389 #{}, 57463 #{}, 746711 #{}, 388504 #{}, 644121 #{}, {:id 29003796, :title "Inductive logic programming", :type :wiki-api.core/category} #{574400 3074945 138052 541902 1010126 985236 574389 746711 3341018 3402586}, 1250938 #{}, 294586 #{}, 1026778 #{}, 3341018 #{}, 3402586 #{}, 937563 #{}, {:id 19667111, :title "Statistical relational learning", :type :wiki-api.core/article} #{687201 3074945 336067 388102 303622 3258022 746727 2927016 2927018 3258026 3074924 473199 190192 3623508 574389 57463 746711 3341018 391548 1009341}, 391548 #{}, 271964 #{}, 1009341 #{}, 622461 #{}, 3782751 #{}}}, :doc-map {1185824 #search_api.search_api.Paper{:id 1185824, :key "journals/jocn/AblaKO08", :title "On-line Assessment of Statistical Learning by Event-related Potentials.", :abstract "We investigated the neural processes involved in on-line statistical learning and word segmentation. Auditory event-related potentials (ERPs) were recorded while participants were exposed to continuous, nonlinguistic auditory sequences, the elements of which were organized into “tritone words” that were sequenced in random order, with no silent spaces between them. After listening to three 6.6-min sessions of sequences, the participants performed a behavioral choice test, in which they were instructed to indicate the most familiar tone sequence in each test trial by pressing buttons. The participants were divided into three groups (high, middle, and low learners) based on their behavioral performance. The overall mean performance was 74.4%, indicating that the tone sequence was segmented and that the participants learned the tone words statistically. Grand-averaged ERPs showed that word onset (initial tone) elicited the largest N100 and N400 in the early learning session of high learners, but in middle learners, the word-onset effect was elicited in a later session, and there was no effect in low learners. The N400 amplitudes significantly differed between the three learning sessions in the high-and middle-learner groups. The results suggest that the N400 effect indicates not only on-line word segmentation but also the degree of statistical learning. This study provides insight into the neural mechanisms underlying on-line statistical learning processes.", :author (#search_api.search_api.Author{:id 85341, :first-name nil, :last-name nil, :full-name "Dilshat Abla"} #search_api.search_api.Author{:id 167451, :first-name nil, :last-name nil, :full-name "Kentaro Katahira"} #search_api.search_api.Author{:id 883499, :first-name nil, :last-name nil, :full-name "Kazuo Okanoya"}), :year 2008, :venue "J. Cognitive Neuroscience", :ncit 0, :string "On-line Assessment of Statistical Learning by Event-related Potentials.. We investigated the neural processes involved in on-line statistical learning and word segmentation. Auditory event-related potentials (ERPs) were recorded while participants were exposed to continuous, nonlinguistic auditory sequences, the elements of which were organized into “tritone words” that were sequenced in random order, with no silent spaces between them. After listening to three 6.6-min sessions of sequences, the participants performed a behavioral choice test, in which they were instructed to indicate the most familiar tone sequence in each test trial by pressing buttons. The participants were divided into three groups (high, middle, and low learners) based on their behavioral performance. The overall mean performance was 74.4%, indicating that the tone sequence was segmented and that the participants learned the tone words statistically. Grand-averaged ERPs showed that word onset (initial tone) elicited the largest N100 and N400 in the early learning session of high learners, but in middle learners, the word-onset effect was elicited in a later session, and there was no effect in low learners. The N400 amplitudes significantly differed between the three learning sessions in the high-and middle-learner groups. The results suggest that the N400 effect indicates not only on-line word segmentation but also the degree of statistical learning. This study provides insight into the neural mechanisms underlying on-line statistical learning processes.", :doc-id "On-line Assessment of Statistical Learning by Event-related Potentials. 2008 Dilshat Abla, Kentaro Katahira, Kazuo Okanoya"}, 574400 #search_api.search_api.Paper{:id 574400, :key "conf/sbia/Raedt08", :title "Logical and Relational Learning.", :abstract "I use the term logical and relational learning (LRL) to refer to the subfield of machine learning and data mining that is concerned with learning in expressive logical or relational representations. It is the union of inductive logic programming, (statistical) relational learning and multi-relational data mining and constitutes a general class of techniques and methodology for learning from structured data (such as graphs, networks, relational databases) and background knowledge. During the course of its existence, logical and relational learning has changed dramatically. Whereas early work was mainly concerned with logical issues (and even program synthesis from examples), in the 90s its focus was on the discovery of new and interpretable knowledge from structured data, often in the form of rules or patterns. Since then the range of tasks to which logical and relational learning has been applied has significantly broadened and now covers almost all machine learning problems and settings. Today, there exist logical and relational learning methods for reinforcement learning, statistical learning, distance- and kernel-based learning in addition to traditional symbolic machine learning approaches. At the same time, logical and relational learning problems are appearing everywhere. Advances in intelligent systems are enabling the generation of high-level symbolic and structured data in a wide variety of domains, including the semantic web, robotics, vision, social networks, and the life sciences, which in turn raises new challenges and opportunities for logical and relational learning. These developments have led to a new view on logical and relational learning and its role in machine learning and artificial intelligence. In this talk, I shall reflect on this view by identifying some of the lessons learned in logical and relational learning and formulating some challenges for future developments.", :author (#search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2008, :venue "SBIA", :ncit 173, :string "Logical and Relational Learning.. I use the term logical and relational learning (LRL) to refer to the subfield of machine learning and data mining that is concerned with learning in expressive logical or relational representations. It is the union of inductive logic programming, (statistical) relational learning and multi-relational data mining and constitutes a general class of techniques and methodology for learning from structured data (such as graphs, networks, relational databases) and background knowledge. During the course of its existence, logical and relational learning has changed dramatically. Whereas early work was mainly concerned with logical issues (and even program synthesis from examples), in the 90s its focus was on the discovery of new and interpretable knowledge from structured data, often in the form of rules or patterns. Since then the range of tasks to which logical and relational learning has been applied has significantly broadened and now covers almost all machine learning problems and settings. Today, there exist logical and relational learning methods for reinforcement learning, statistical learning, distance- and kernel-based learning in addition to traditional symbolic machine learning approaches. At the same time, logical and relational learning problems are appearing everywhere. Advances in intelligent systems are enabling the generation of high-level symbolic and structured data in a wide variety of domains, including the semantic web, robotics, vision, social networks, and the life sciences, which in turn raises new challenges and opportunities for logical and relational learning. These developments have led to a new view on logical and relational learning and its role in machine learning and artificial intelligence. In this talk, I shall reflect on this view by identifying some of the lessons learned in logical and relational learning and formulating some challenges for future developments.", :doc-id "Logical and Relational Learning. 2008 Luc De Raedt"}, 687201 #search_api.search_api.Paper{:id 687201, :key "conf/grc/Chen07", :title "Research on Statistical Relational Learning and Rough Set in SRL.", :abstract "Statistical relational learning constructs statistical mod- els from relational databases, combining the powers of re- lational learning and statistical learning. Its strong abil- ity and special property make statistical relational learn- ing become one of the important areas in machine learn- ing. In this paper, the general concepts and characteris- tics of statistical relational learning are presented firstly. Then some major branches of this newly emerging field are discussed, including logic and rule-based approaches, frame and object-oriented approaches, and several other important approaches. After that some methods of apply- ing rough set in statistical relational learning are described, such as gRS-ILP and VPRSILP. Finally applications of sta- tistical relational learning are briefly introduced and some future directions of statistical relational learning and the prospects of rough set in this area are pointed out.", :author (#search_api.search_api.Author{:id 74552, :first-name nil, :last-name nil, :full-name "Fei Chen"}), :year 2007, :venue "GrC", :ncit 0, :string "Research on Statistical Relational Learning and Rough Set in SRL.. Statistical relational learning constructs statistical mod- els from relational databases, combining the powers of re- lational learning and statistical learning. Its strong abil- ity and special property make statistical relational learn- ing become one of the important areas in machine learn- ing. In this paper, the general concepts and characteris- tics of statistical relational learning are presented firstly. Then some major branches of this newly emerging field are discussed, including logic and rule-based approaches, frame and object-oriented approaches, and several other important approaches. After that some methods of apply- ing rough set in statistical relational learning are described, such as gRS-ILP and VPRSILP. Finally applications of sta- tistical relational learning are briefly introduced and some future directions of statistical relational learning and the prospects of rough set in this area are pointed out.", :doc-id "Research on Statistical Relational Learning and Rough Set in SRL. 2007 Fei Chen"}, 1009537 #search_api.search_api.Paper{:id 1009537, :key "journals/ml/Hofmann01", :title "Unsupervised Learning by Probabilistic Latent Semantic Analysis.", :abstract "This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.", :author (#search_api.search_api.Author{:id 8394, :first-name nil, :last-name nil, :full-name "Thomas Hofmann"}), :year 2001, :venue "Machine Learning", :ncit 2463, :string "Unsupervised Learning by Probabilistic Latent Semantic Analysis.. This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.", :doc-id "Unsupervised Learning by Probabilistic Latent Semantic Analysis. 2001 Thomas Hofmann"}, 3074945 #search_api.search_api.Paper{:id 3074945, :key "journals/ml/BlockeelBRDKY11", :title "Guest editorial to the special issue on inductive logic programming, mining and learning in graphs and statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 882341, :first-name nil, :last-name nil, :full-name "Hendrik Blockeel"} #search_api.search_api.Author{:id 272602, :first-name nil, :last-name nil, :full-name "Karsten M. Borgwardt"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"} #search_api.search_api.Author{:id 668085, :first-name nil, :last-name nil, :full-name "Pedro Domingos"} #search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"} #search_api.search_api.Author{:id 324907, :first-name nil, :last-name nil, :full-name "Xifeng Yan"}), :year 2011, :venue "Machine Learning", :ncit 0, :string "Guest editorial to the special issue on inductive logic programming, mining and learning in graphs and statistical relational learning.. ", :doc-id "Guest editorial to the special issue on inductive logic programming, mining and learning in graphs and statistical relational learning. 2011 Hendrik Blockeel, Karsten M. Borgwardt, Luc De Raedt, Pedro Domingos, Kristian Kersting, Xifeng Yan"}, 1009218 #search_api.search_api.Paper{:id 1009218, :key "journals/ml/Boulle04", :title "Khiops: A Statistical Discretization Method of Continuous Attributes.", :abstract "In supervised machine learning, some algorithms are restricted to discrete data and have to discretize continuous attributes. Many discretization methods, based on statistical criteria, information content, or other specialized criteria, have been studied in the past. In this paper, we propose the discretization method Khiops,1 based on the chi-square statistic. In contrast with related methods ChiMerge and ChiSplit, this method optimizes the chi-square criterion in a global manner on the whole discretization domain and does not require any stopping criterion. A theoretical study followed by experiments demonstrates the robustness and the good predictive performance of the method.", :author (#search_api.search_api.Author{:id 1526635, :first-name nil, :last-name nil, :full-name "Marc Boullé"}), :year 2004, :venue "Machine Learning", :ncit 0, :string "Khiops: A Statistical Discretization Method of Continuous Attributes.. In supervised machine learning, some algorithms are restricted to discrete data and have to discretize continuous attributes. Many discretization methods, based on statistical criteria, information content, or other specialized criteria, have been studied in the past. In this paper, we propose the discretization method Khiops,1 based on the chi-square statistic. In contrast with related methods ChiMerge and ChiSplit, this method optimizes the chi-square criterion in a global manner on the whole discretization domain and does not require any stopping criterion. A theoretical study followed by experiments demonstrates the robustness and the good predictive performance of the method.", :doc-id "Khiops: A Statistical Discretization Method of Continuous Attributes. 2004 Marc Boullé"}, 1280035 #search_api.search_api.Paper{:id 1280035, :key "journals/nn/Watanabe10", :title "Equations of states in singular statistical estimation.", :abstract "Learning machines that have hierarchical structures or hidden variables are singular statistical models because they are nonidentifiable and their Fisher information matrices are singular. In singular statistical models, neither does the Bayes a posteriori distribution converge to the normal distribution nor does the maximum likelihood estimator satisfy asymptotic normality. This is the main reason that it has been difficult to predict their generalization performance from trained states. In this paper, we study four errors, (1) the Bayes generalization error, (2) the Bayes training error, (3) the Gibbs generalization error, and (4) the Gibbs training error, and prove that there are universal mathematical relations among these errors. The formulas proved in this paper are equations of states in statistical estimation because they hold for any true distribution, any parametric model, and any a priori distribution. Also we show that the Bayes and Gibbs generalization errors can be estimated by Bayes and Gibbs training errors, and we propose widely applicable information criteria that can be applied to both regular and singular statistical models.", :author (#search_api.search_api.Author{:id 976510, :first-name nil, :last-name nil, :full-name "Sumio Watanabe"}), :year 2010, :venue "Neural Networks", :ncit 13, :string "Equations of states in singular statistical estimation.. Learning machines that have hierarchical structures or hidden variables are singular statistical models because they are nonidentifiable and their Fisher information matrices are singular. In singular statistical models, neither does the Bayes a posteriori distribution converge to the normal distribution nor does the maximum likelihood estimator satisfy asymptotic normality. This is the main reason that it has been difficult to predict their generalization performance from trained states. In this paper, we study four errors, (1) the Bayes generalization error, (2) the Bayes training error, (3) the Gibbs generalization error, and (4) the Gibbs training error, and prove that there are universal mathematical relations among these errors. The formulas proved in this paper are equations of states in statistical estimation because they hold for any true distribution, any parametric model, and any a priori distribution. Also we show that the Bayes and Gibbs generalization errors can be estimated by Bayes and Gibbs training errors, and we propose widely applicable information criteria that can be applied to both regular and singular statistical models.", :doc-id "Equations of states in singular statistical estimation. 2010 Sumio Watanabe"}, 473123 #search_api.search_api.Paper{:id 473123, :key "conf/kdd/NevilleJFH03", :title "Learning relational probability trees.", :abstract "Classification trees are widely used in the machine learning and data mining communities for modeling propositional data. Recent work has extended this basic paradigm to probability estimation trees. Traditional tree learning algorithms assume that instances in the training data are homogenous and independently distributed. Relational probability trees (RPTs) extend standard probability estimation trees to a relational setting in which data instances are heterogeneous and interdependent. Our algorithm for learning the structure and parameters of an RPT searches over a space of relational features that use aggregation functions (e.g. AVERAGE, MODE, COUNT) to dynamically propositionalize relational data and create binary splits within the RPT. Previous work has identified a number of statistical biases due to characteristics of relational data such as autocorrelation and degree disparity. The RPT algorithm uses a novel form of randomization test to adjust for these biases. On a variety of relational learning tasks, RPTs built using randomization tests are significantly smaller than other models and achieve equivalent, or better, performance.", :author (#search_api.search_api.Author{:id 150768, :first-name nil, :last-name nil, :full-name "Jennifer Neville"} #search_api.search_api.Author{:id 408381, :first-name nil, :last-name nil, :full-name "David Jensen"} #search_api.search_api.Author{:id 1434755, :first-name nil, :last-name nil, :full-name "Lisa Friedland"} #search_api.search_api.Author{:id 575750, :first-name nil, :last-name nil, :full-name "Michael Hay"}), :year 2003, :venue "KDD", :ncit 191, :string "Learning relational probability trees.. Classification trees are widely used in the machine learning and data mining communities for modeling propositional data. Recent work has extended this basic paradigm to probability estimation trees. Traditional tree learning algorithms assume that instances in the training data are homogenous and independently distributed. Relational probability trees (RPTs) extend standard probability estimation trees to a relational setting in which data instances are heterogeneous and interdependent. Our algorithm for learning the structure and parameters of an RPT searches over a space of relational features that use aggregation functions (e.g. AVERAGE, MODE, COUNT) to dynamically propositionalize relational data and create binary splits within the RPT. Previous work has identified a number of statistical biases due to characteristics of relational data such as autocorrelation and degree disparity. The RPT algorithm uses a novel form of randomization test to adjust for these biases. On a variety of relational learning tasks, RPTs built using randomization tests are significantly smaller than other models and achieve equivalent, or better, performance.", :doc-id "Learning relational probability trees. 2003 Jennifer Neville, David Jensen, Lisa Friedland, Michael Hay"}, 336067 #search_api.search_api.Paper{:id 336067, :key "conf/icml/KokD07", :title "Statistical predicate invention.", :abstract "We propose statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models and predicate invention in ILP. We propose an initial model for SPI based on second-order Markov logic, in which predicates as well as arguments can be variables, and the domain of discourse is not fully known in advance. Our approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with (e.g., it clusters relations by the clusters of the objects they relate). Since different clusterings are better for predicting different subsets of the atoms, we allow multiple cross-cutting clusterings. We show that this approach outperforms Markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets.", :author (#search_api.search_api.Author{:id 801078, :first-name nil, :last-name nil, :full-name "Stanley Kok"} #search_api.search_api.Author{:id 668085, :first-name nil, :last-name nil, :full-name "Pedro Domingos"}), :year 2007, :venue "ICML", :ncit 59, :string "Statistical predicate invention.. We propose statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models and predicate invention in ILP. We propose an initial model for SPI based on second-order Markov logic, in which predicates as well as arguments can be variables, and the domain of discourse is not fully known in advance. Our approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with (e.g., it clusters relations by the clusters of the objects they relate). Since different clusterings are better for predicting different subsets of the atoms, we allow multiple cross-cutting clusterings. We show that this approach outperforms Markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets.", :doc-id "Statistical predicate invention. 2007 Stanley Kok, Pedro Domingos"}, 675364 #search_api.search_api.Paper{:id 675364, :key "conf/www/QinLZWXL08", :title "Learning to rank relational objects and its application to web search.", :abstract "Learning to rank is a new statistical learning technology on creating a ranking model for sorting objects. The technology has been successfully applied to web search, and is becoming one of the key machineries for building search engines. Existing approaches to learning to rank, however, did not consider the cases in which there exists relationship between the objects to be ranked, despite of the fact that such situations are very common in practice. For example, in web search, given a query certain relationships usually exist among the the retrieved documents, e.g., URL hierarchy, similarity, etc., and sometimes it is necessary to utilize the information in ranking of the documents. This paper addresses the issue and formulates it as a novel learning problem, referred to as, 'learning to rank relational objects'. In the new learning task, the ranking model is defined as a function of not only the contents (features) of objects but also the relations between objects. The paper further focuses on one setting of the learning problem in which the way of using relation information is predetermined. It formalizes the learning task as an optimization problem in the setting. The paper then proposes a new method to perform the optimization task, particularly an implementation based on SVM. Experimental results show that the proposed method outperforms the baseline methods for two ranking tasks (Pseudo Relevance Feedback and Topic Distillation) in web search, indicating that the proposed method can indeed make effective use of relation information and content information in ranking.", :author (#search_api.search_api.Author{:id 17439, :first-name nil, :last-name nil, :full-name "Tao Qin"} #search_api.search_api.Author{:id 1411681, :first-name nil, :last-name nil, :full-name "Tie-Yan Liu"} #search_api.search_api.Author{:id 981048, :first-name nil, :last-name nil, :full-name "Xu-Dong Zhang"} #search_api.search_api.Author{:id 566222, :first-name nil, :last-name nil, :full-name "De-Sheng Wang"} #search_api.search_api.Author{:id 1482892, :first-name nil, :last-name nil, :full-name "Wen-Ying Xiong"} #search_api.search_api.Author{:id 742456, :first-name nil, :last-name nil, :full-name "Hang Li"}), :year 2008, :venue "WWW", :ncit 62, :string "Learning to rank relational objects and its application to web search.. Learning to rank is a new statistical learning technology on creating a ranking model for sorting objects. The technology has been successfully applied to web search, and is becoming one of the key machineries for building search engines. Existing approaches to learning to rank, however, did not consider the cases in which there exists relationship between the objects to be ranked, despite of the fact that such situations are very common in practice. For example, in web search, given a query certain relationships usually exist among the the retrieved documents, e.g., URL hierarchy, similarity, etc., and sometimes it is necessary to utilize the information in ranking of the documents. This paper addresses the issue and formulates it as a novel learning problem, referred to as, 'learning to rank relational objects'. In the new learning task, the ranking model is defined as a function of not only the contents (features) of objects but also the relations between objects. The paper further focuses on one setting of the learning problem in which the way of using relation information is predetermined. It formalizes the learning task as an optimization problem in the setting. The paper then proposes a new method to perform the optimization task, particularly an implementation based on SVM. Experimental results show that the proposed method outperforms the baseline methods for two ranking tasks (Pseudo Relevance Feedback and Topic Distillation) in web search, indicating that the proposed method can indeed make effective use of relation information and content information in ranking.", :doc-id "Learning to rank relational objects and its application to web search. 2008 Tao Qin, Tie-Yan Liu, Xu-Dong Zhang, De-Sheng Wang, Wen-Ying Xiong, Hang Li"}, 138052 #search_api.search_api.Paper{:id 138052, :key "conf/dagstuhl/PasseriniFR05", :title "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.", :abstract "We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :author (#search_api.search_api.Author{:id 1520074, :first-name nil, :last-name nil, :full-name "Andrea Passerini"} #search_api.search_api.Author{:id 34394, :first-name nil, :last-name nil, :full-name "Paolo Frasconi"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2005, :venue "Probabilistic, Logical and Relational Learning", :ncit 31, :string "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.. We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :doc-id "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting. 2005 Andrea Passerini, Paolo Frasconi, Luc De Raedt"}, 388102 #search_api.search_api.Paper{:id 388102, :key "conf/ijcai/DavisBDPRCS05", :title "View Learning for Statistical Relational Learning: With an Application to Mammography.", :abstract "Statistical relational learning (SRL) constructs probabilistic models from relational databases. A key capability of SRL is the learning of arcs (in the Bayes net sense) connecting entries in different rows of a relational table, or in different tables. Nevertheless, SRL approaches currently are constrained to use the existing database schema. For many database applications, users find it profitable to define alternative \"views\" of the database, in effect defining new fields or tables. Such new fields or tables can also be highly useful in learning. We provide SRL with the capability of learning new views.", :author (#search_api.search_api.Author{:id 747351, :first-name nil, :last-name nil, :full-name "Jesse Davis"} #search_api.search_api.Author{:id 750446, :first-name nil, :last-name nil, :full-name "Elizabeth S. Burnside"} #search_api.search_api.Author{:id 1157247, :first-name nil, :last-name nil, :full-name "Inês de Castro Dutra"} #search_api.search_api.Author{:id 138183, :first-name nil, :last-name nil, :full-name "David Page"} #search_api.search_api.Author{:id 464920, :first-name nil, :last-name nil, :full-name "Raghu Ramakrishnan"} #search_api.search_api.Author{:id 1352436, :first-name nil, :last-name nil, :full-name "Vítor Santos Costa"} #search_api.search_api.Author{:id 7, :first-name nil, :last-name nil, :full-name "Jude W. Shavlik"}), :year 2005, :venue "IJCAI", :ncit 33, :string "View Learning for Statistical Relational Learning: With an Application to Mammography.. Statistical relational learning (SRL) constructs probabilistic models from relational databases. A key capability of SRL is the learning of arcs (in the Bayes net sense) connecting entries in different rows of a relational table, or in different tables. Nevertheless, SRL approaches currently are constrained to use the existing database schema. For many database applications, users find it profitable to define alternative \"views\" of the database, in effect defining new fields or tables. Such new fields or tables can also be highly useful in learning. We provide SRL with the capability of learning new views.", :doc-id "View Learning for Statistical Relational Learning: With an Application to Mammography. 2005 Jesse Davis, Elizabeth S. Burnside, Inês de Castro Dutra, David Page, Raghu Ramakrishnan, Vítor Santos Costa, Jude W. Shavlik"}, 303622 #search_api.search_api.Paper{:id 303622, :key "conf/icdm/XiangN08", :title "Pseudolikelihood EM for Within-network Relational Learning.", :abstract "In this work, we study the problem of \\emph{within-network} relational learning and inference, where models are learned on a partially labeled relational dataset and then are applied to predict the classes of unlabeled instances in the same graph. We categorize recent work in statistical relational learning into three alternative approaches for this setting: disjoint learning with disjoint inference, disjoint learning with collective inference, and collective learning with collective inference. Models from each of these categories has been employed previously in different settings, but to our knowledge there has been no systematic comparison of models from all three categories. In this paper, we develop a novel pseudolikelihood EM method that facilitates more general \\emph{collective learning} and \\emph{collective inference} on partially labeled relational networks. We then compare this method to competing methods from the other categories on both synthetic and real-world data. We show that collective learning and inference with the pseudolikelihood EM approach achieves significantly higher accuracy than the other types of models when there are a moderate number of labeled examples in the data graph.", :author (#search_api.search_api.Author{:id 819343, :first-name nil, :last-name nil, :full-name "Rongjing Xiang"} #search_api.search_api.Author{:id 150768, :first-name nil, :last-name nil, :full-name "Jennifer Neville"}), :year 2008, :venue "ICDM", :ncit 9, :string "Pseudolikelihood EM for Within-network Relational Learning.. In this work, we study the problem of \\emph{within-network} relational learning and inference, where models are learned on a partially labeled relational dataset and then are applied to predict the classes of unlabeled instances in the same graph. We categorize recent work in statistical relational learning into three alternative approaches for this setting: disjoint learning with disjoint inference, disjoint learning with collective inference, and collective learning with collective inference. Models from each of these categories has been employed previously in different settings, but to our knowledge there has been no systematic comparison of models from all three categories. In this paper, we develop a novel pseudolikelihood EM method that facilitates more general \\emph{collective learning} and \\emph{collective inference} on partially labeled relational networks. We then compare this method to competing methods from the other categories on both synthetic and real-world data. We show that collective learning and inference with the pseudolikelihood EM approach achieves significantly higher accuracy than the other types of models when there are a moderate number of labeled examples in the data graph.", :doc-id "Pseudolikelihood EM for Within-network Relational Learning. 2008 Rongjing Xiang, Jennifer Neville"}, 3258022 #search_api.search_api.Paper{:id 3258022, :key "journals/ml/NatarajanKKGS12", :title "Gradient-based boosting for statistical relational learning: The relational dependency network case.", :abstract nil, :author (#search_api.search_api.Author{:id 883462, :first-name nil, :last-name nil, :full-name "Sriraam Natarajan"} #search_api.search_api.Author{:id 905516, :first-name nil, :last-name nil, :full-name "Tushar Khot"} #search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"} #search_api.search_api.Author{:id 1351938, :first-name nil, :last-name nil, :full-name "Bernd Gutmann"} #search_api.search_api.Author{:id 7, :first-name nil, :last-name nil, :full-name "Jude W. Shavlik"}), :year 2012, :venue "Machine Learning", :ncit 20, :string "Gradient-based boosting for statistical relational learning: The relational dependency network case.. ", :doc-id "Gradient-based boosting for statistical relational learning: The relational dependency network case. 2012 Sriraam Natarajan, Tushar Khot, Kristian Kersting, Bernd Gutmann, Jude W. Shavlik"}, 473095 #search_api.search_api.Paper{:id 473095, :key "conf/kdd/Moore06", :title "New cached-sufficient statistics algorithms for quickly answering statistical questions.", :abstract "This talk is about recent work on new ways to exploit preprocessed views of data tables for tractably solving big statistical queries. We'll describe deployments of these new algorithms in the realms of detecting killer asteroids and unnatural disease outbreaks.In recent years, several groups have looked at methods for pre-storing general sufficient statistics of the data in spatial data structures such as kd-trees and ball-trees so that both frequentist and Bayesian statistical operations become fast for large datasets. In this talk we will look at two other classes of optimization required in important statistical queries.The first involves iterating over all spatial regions (big and small). The second involves detection of tracks from noisy intermittent observations separated far apart in time and space. We will also discuss the implications that have arisen from making these operations tractable. We will focus particularly onDetecting all asteroids in the solar system larger than Pittsburgh's Cathedral of Learning (data to be collected over 2006-2010). Early detection of emerging diseases based on national monitoring of health-related transactions..", :author (#search_api.search_api.Author{:id 685289, :first-name nil, :last-name nil, :full-name "Andrew Moore"}), :year 2006, :venue "KDD", :ncit 0, :string "New cached-sufficient statistics algorithms for quickly answering statistical questions.. This talk is about recent work on new ways to exploit preprocessed views of data tables for tractably solving big statistical queries. We'll describe deployments of these new algorithms in the realms of detecting killer asteroids and unnatural disease outbreaks.In recent years, several groups have looked at methods for pre-storing general sufficient statistics of the data in spatial data structures such as kd-trees and ball-trees so that both frequentist and Bayesian statistical operations become fast for large datasets. In this talk we will look at two other classes of optimization required in important statistical queries.The first involves iterating over all spatial regions (big and small). The second involves detection of tracks from noisy intermittent observations separated far apart in time and space. We will also discuss the implications that have arisen from making these operations tractable. We will focus particularly onDetecting all asteroids in the solar system larger than Pittsburgh's Cathedral of Learning (data to be collected over 2006-2010). Early detection of emerging diseases based on national monitoring of health-related transactions..", :doc-id "New cached-sufficient statistics algorithms for quickly answering statistical questions. 2006 Andrew Moore"}, 1033319 #search_api.search_api.Paper{:id 1033319, :key "journals/pami/WechslerDLC04", :title "Motion Estimation Using Statistical Learning Theory.", :abstract "Abstract--This paper describes a novel application of Statistical Learning Theory (SLT) to single motion estimation and tracking. The problem of motion estimation can be related to statistical model selection, where the goal is to select one (correct) motion model from several possible motion models, given finite noisy samples. SLT, also known as Vapnik-Chervonenkis (VC), theory provides analytic generalization bounds for model selection, which have been used successfully for practical model selection. This paper describes a successful application of an SLT-based model selection approach to the challenging problem of estimating optimal motion models from small data sets of image measurements (flow). We present results of experiments on both synthetic and real image sequences for motion interpolation and extrapolation; these results demonstrate the feasibility and strength of our approach. Our experimental results show that for motion estimation applications, SLT-based model selection compares favorably against alternative model selection methods, such as the Akaike's fpe, Schwartz' criterion (sc), Generalized Cross-Validation (gcv), and Shibata's Model Selector (sms). The paper also shows how to address the aperture problem using SLT-based model selection for penalized linear (ridge regression) formulation.", :author (#search_api.search_api.Author{:id 1499994, :first-name nil, :last-name nil, :full-name "Harry Wechsler"} #search_api.search_api.Author{:id 398169, :first-name nil, :last-name nil, :full-name "Zoran Duric"} #search_api.search_api.Author{:id 1493163, :first-name nil, :last-name nil, :full-name "Fayin Li"} #search_api.search_api.Author{:id 1104310, :first-name nil, :last-name nil, :full-name "Vladimir Cherkassky"}), :year 2004, :venue "IEEE Trans. Pattern Anal. Mach. Intell.", :ncit 50, :string "Motion Estimation Using Statistical Learning Theory.. Abstract--This paper describes a novel application of Statistical Learning Theory (SLT) to single motion estimation and tracking. The problem of motion estimation can be related to statistical model selection, where the goal is to select one (correct) motion model from several possible motion models, given finite noisy samples. SLT, also known as Vapnik-Chervonenkis (VC), theory provides analytic generalization bounds for model selection, which have been used successfully for practical model selection. This paper describes a successful application of an SLT-based model selection approach to the challenging problem of estimating optimal motion models from small data sets of image measurements (flow). We present results of experiments on both synthetic and real image sequences for motion interpolation and extrapolation; these results demonstrate the feasibility and strength of our approach. Our experimental results show that for motion estimation applications, SLT-based model selection compares favorably against alternative model selection methods, such as the Akaike's fpe, Schwartz' criterion (sc), Generalized Cross-Validation (gcv), and Shibata's Model Selector (sms). The paper also shows how to address the aperture problem using SLT-based model selection for penalized linear (ridge regression) formulation.", :doc-id "Motion Estimation Using Statistical Learning Theory. 2004 Harry Wechsler, Zoran Duric, Fayin Li, Vladimir Cherkassky"}, 746727 #search_api.search_api.Paper{:id 746727, :key "journals/aicom/Tian06", :title "Context-based statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 260142, :first-name nil, :last-name nil, :full-name "YongHong Tian"}), :year 2006, :venue "AI Commun.", :ncit 0, :string "Context-based statistical relational learning.. ", :doc-id "Context-based statistical relational learning. 2006 YongHong Tian"}, 1323591 #search_api.search_api.Paper{:id 1323591, :key "conf/icc/TiwanaSA09", :title "Statistical Learning for Automated RRM: Application to eUTRAN Mobility.", :abstract "Self Organizing Network (SON) functionalities are currently developed to improve network performance and management tasks. SON functionalities require efficient utilization of data extracted from the network. In this context, the paper has two objectives. First it is shown that one can use simple statistical learning techniques such as regression to extract a model from data. The model comprises closed form expressions that approximate the functional relations between measured Key Performance Indicators (KPIs) and Radio Resource Management (RRM) parameters. Second, it is shown how the model can be integrated in a monitoring process and be used to devise an efficient auto-tuning algorithm. To this end, two case studies of handover monitoring and handover auto-tuning in a LTE network are described and illustrate the application of the proposed approach.", :author (#search_api.search_api.Author{:id -1, :first-name nil, :last-name nil, :full-name "Moazzam Islam Tiwana"} #search_api.search_api.Author{:id 1094720, :first-name nil, :last-name nil, :full-name "Berna Sayraç"} #search_api.search_api.Author{:id 223882, :first-name nil, :last-name nil, :full-name "Zwi Altman"}), :year 2009, :venue "ICC", :ncit 6, :string "Statistical Learning for Automated RRM: Application to eUTRAN Mobility.. Self Organizing Network (SON) functionalities are currently developed to improve network performance and management tasks. SON functionalities require efficient utilization of data extracted from the network. In this context, the paper has two objectives. First it is shown that one can use simple statistical learning techniques such as regression to extract a model from data. The model comprises closed form expressions that approximate the functional relations between measured Key Performance Indicators (KPIs) and Radio Resource Management (RRM) parameters. Second, it is shown how the model can be integrated in a monitoring process and be used to devise an efficient auto-tuning algorithm. To this end, two case studies of handover monitoring and handover auto-tuning in a LTE network are described and illustrate the application of the proposed approach.", :doc-id "Statistical Learning for Automated RRM: Application to eUTRAN Mobility. 2009 Moazzam Islam Tiwana, Berna Sayraç, Zwi Altman"}, 3382280 #search_api.search_api.Paper{:id 3382280, :key "journals/cogsci/ArciuliS12", :title "Statistical Learning Is Related to Reading Ability in Children and Adults.", :abstract nil, :author (#search_api.search_api.Author{:id 14233998, :first-name nil, :last-name nil, :full-name "Joanne Arciuli"} #search_api.search_api.Author{:id 14270159, :first-name nil, :last-name nil, :full-name "Ian C. Simpson"}), :year 2012, :venue "Cognitive Science", :ncit 0, :string "Statistical Learning Is Related to Reading Ability in Children and Adults.. ", :doc-id "Statistical Learning Is Related to Reading Ability in Children and Adults. 2012 Joanne Arciuli, Ian C. Simpson"}, 893160 #search_api.search_api.Paper{:id 893160, :key "journals/ida/CucchiaraMPR01", :title "An application of machine learning and statistics to defect detection.", :abstract "We present an application of machine learning and statistics to the problem of distinguishing between defective and non-defective industrial workpieces, where the defect takes the form of a long and thin crack on the surface of the piece. From the images of pieces a number of features are extracted by using the Hough transform and the Correlated Hough transform. Two datasets are considered, one containing only features related to the Hough transform and the other containing also features related to the Correlated Hough transform. On these datasets we have compared six different learning algorithms: an attribute-value learner, C4.5, a backpropagation neural network, NeuralWorks Predict, a k-nearest neighbour algorithm, and three statistical techniques, linear, logistic and quadratic discriminant. The experiments show that C4.5 performs best for both feature sets and gives an average accuracy of 93.3% for the first dataset and 95.9% for the second dataset.", :author (#search_api.search_api.Author{:id 394534, :first-name nil, :last-name nil, :full-name "Rita Cucchiara"} #search_api.search_api.Author{:id 1065974, :first-name nil, :last-name nil, :full-name "Paola Mello"} #search_api.search_api.Author{:id 893047, :first-name nil, :last-name nil, :full-name "Massimo Piccardi"} #search_api.search_api.Author{:id 1316705, :first-name nil, :last-name nil, :full-name "Fabrizio Riguzzi"}), :year 2001, :venue "Intell. Data Anal.", :ncit 3, :string "An application of machine learning and statistics to defect detection.. We present an application of machine learning and statistics to the problem of distinguishing between defective and non-defective industrial workpieces, where the defect takes the form of a long and thin crack on the surface of the piece. From the images of pieces a number of features are extracted by using the Hough transform and the Correlated Hough transform. Two datasets are considered, one containing only features related to the Hough transform and the other containing also features related to the Correlated Hough transform. On these datasets we have compared six different learning algorithms: an attribute-value learner, C4.5, a backpropagation neural network, NeuralWorks Predict, a k-nearest neighbour algorithm, and three statistical techniques, linear, logistic and quadratic discriminant. The experiments show that C4.5 performs best for both feature sets and gives an average accuracy of 93.3% for the first dataset and 95.9% for the second dataset.", :doc-id "An application of machine learning and statistics to defect detection. 2001 Rita Cucchiara, Paola Mello, Massimo Piccardi, Fabrizio Riguzzi"}, 2927016 #search_api.search_api.Paper{:id 2927016, :key "journals/sigkdd/Landwehr09", :title "Trading expressivity for efficiency in statistical relational learning: Ph.D. thesis abstract.", :abstract nil, :author (#search_api.search_api.Author{:id 127590, :first-name nil, :last-name nil, :full-name "Niels Landwehr"}), :year 2009, :venue "SIGKDD Explorations", :ncit 0, :string "Trading expressivity for efficiency in statistical relational learning: Ph.D. thesis abstract.. ", :doc-id "Trading expressivity for efficiency in statistical relational learning: Ph.D. thesis abstract. 2009 Niels Landwehr"}, 2881512 #search_api.search_api.Paper{:id 2881512, :key "journals/jocn/Turk-BrowneSCJ09", :title "Neural Evidence of Statistical Learning: Efficient Detection of Visual Regularities Without Awareness.", :abstract "Our environment contains regularities distributed in space and time that can be detected by way of statistical learning. This unsupervised learning occurs without intent or awareness, but little is known about how it relates to other types of learning, how it affects perceptual processing, and how quickly it can occur. Here we use fMRI during statistical learning to explore these questions. Participants viewed statistically structured versus unstructured sequences of shapes while performing a task unrelated to the structure. Robust neural responses to statistical structure were observed, and these responses were notable in four ways: First, responses to structure were observed in the striatum and medial temporal lobe, suggesting that statistical learning may be related to other forms of associative learning and relational memory. Second, statistical regularities yielded greater activation in category-specific visual regions (object-selective lateral occipital cortex and word-selective ventral occipito-temporal cortex), demonstrating that these regions are sensitive to information distributed in time. Third, evidence of learning emerged early during familiarization, showing that statistical learning can operate very quickly and with little exposure. Finally, neural signatures of learning were dissociable from subsequent explicit familiarity, suggesting that learning can occur in the absence of awareness. Overall, our findings help elucidate the underlying nature of statistical learning.", :author (#search_api.search_api.Author{:id 795446, :first-name nil, :last-name nil, :full-name "Nicholas B. Turk-Browne"} #search_api.search_api.Author{:id 1336480, :first-name nil, :last-name nil, :full-name "Brian J. Scholl"} #search_api.search_api.Author{:id 1113401, :first-name nil, :last-name nil, :full-name "Marvin M. Chun"} #search_api.search_api.Author{:id 778152, :first-name nil, :last-name nil, :full-name "Marcia K. Johnson"}), :year 2009, :venue "J. Cognitive Neuroscience", :ncit 25, :string "Neural Evidence of Statistical Learning: Efficient Detection of Visual Regularities Without Awareness.. Our environment contains regularities distributed in space and time that can be detected by way of statistical learning. This unsupervised learning occurs without intent or awareness, but little is known about how it relates to other types of learning, how it affects perceptual processing, and how quickly it can occur. Here we use fMRI during statistical learning to explore these questions. Participants viewed statistically structured versus unstructured sequences of shapes while performing a task unrelated to the structure. Robust neural responses to statistical structure were observed, and these responses were notable in four ways: First, responses to structure were observed in the striatum and medial temporal lobe, suggesting that statistical learning may be related to other forms of associative learning and relational memory. Second, statistical regularities yielded greater activation in category-specific visual regions (object-selective lateral occipital cortex and word-selective ventral occipito-temporal cortex), demonstrating that these regions are sensitive to information distributed in time. Third, evidence of learning emerged early during familiarization, showing that statistical learning can operate very quickly and with little exposure. Finally, neural signatures of learning were dissociable from subsequent explicit familiarity, suggesting that learning can occur in the absence of awareness. Overall, our findings help elucidate the underlying nature of statistical learning.", :doc-id "Neural Evidence of Statistical Learning: Efficient Detection of Visual Regularities Without Awareness. 2009 Nicholas B. Turk-Browne, Brian J. Scholl, Marvin M. Chun, Marcia K. Johnson"}, 473321 #search_api.search_api.Paper{:id 473321, :key "conf/kdd/SuchanekIW06", :title "Combining linguistic and statistical analysis to extract relations from web documents.", :abstract "The World Wide Web provides a nearly endless source of knowledge, which is mostly given in natural language. A first step towards exploiting this data automatically could be to extract pairs of a given semantic relation from text documents - for example all pairs of a person and her birthdate. One strategy for this task is to find text patterns that express the semantic relation, to generalize these patterns, and to apply them to a corpus to find new pairs. In this paper, we show that this approach profits significantly when deep linguistic structures are used instead of surface text patterns. We demonstrate how linguistic structures can be represented for machine learning, and we provide a theoretical analysis of the pattern matching approach. We show the benefits of our approach by extensive experiments with our prototype system LEILA.", :author (#search_api.search_api.Author{:id 793922, :first-name nil, :last-name nil, :full-name "Fabian M. Suchanek"} #search_api.search_api.Author{:id 528047, :first-name nil, :last-name nil, :full-name "Georgiana Ifrim"} #search_api.search_api.Author{:id 1015097, :first-name nil, :last-name nil, :full-name "Gerhard Weikum"}), :year 2006, :venue "KDD", :ncit 97, :string "Combining linguistic and statistical analysis to extract relations from web documents.. The World Wide Web provides a nearly endless source of knowledge, which is mostly given in natural language. A first step towards exploiting this data automatically could be to extract pairs of a given semantic relation from text documents - for example all pairs of a person and her birthdate. One strategy for this task is to find text patterns that express the semantic relation, to generalize these patterns, and to apply them to a corpus to find new pairs. In this paper, we show that this approach profits significantly when deep linguistic structures are used instead of surface text patterns. We demonstrate how linguistic structures can be represented for machine learning, and we provide a theoretical analysis of the pattern matching approach. We show the benefits of our approach by extensive experiments with our prototype system LEILA.", :doc-id "Combining linguistic and statistical analysis to extract relations from web documents. 2006 Fabian M. Suchanek, Georgiana Ifrim, Gerhard Weikum"}, 91401 #search_api.search_api.Paper{:id 91401, :key "conf/chi/PatelFLH08", :title "Investigating statistical machine learning as a tool for software development.", :abstract "As statistical machine learning algorithms and techniques continue to mature, many researchers and developers see statistical machine learning not only as a topic of expert study, but also as a tool for software development. Extensive prior work has studied software development, but little prior work has studied software developers applying statistical machine learning. This paper presents interviews of eleven researchers experienced in applying statistical machine learning algorithms and techniques to human-computer interaction problems, as well as a study of ten participants working during a five-hour study to apply statistical machine learning algorithms and techniques to a realistic problem. We distill three related categories of difficulties that arise in applying statistical machine learning as a tool for software development: (1) difficulty pursuing statistical machine learning as an iterative and exploratory process, (2) difficulty understanding relationships between data and the behavior of statistical machine learning algorithms, and (3) difficulty evaluating the performance of statistical machine learning algorithms and techniques in the context of applications. This paper provides important new insight into these difficulties and the need for development tools that better support the application of statistical machine learning.", :author (#search_api.search_api.Author{:id 939325, :first-name nil, :last-name nil, :full-name "Kayur Patel"} #search_api.search_api.Author{:id 1505625, :first-name nil, :last-name nil, :full-name "James Fogarty"} #search_api.search_api.Author{:id 82771, :first-name nil, :last-name nil, :full-name "James A. Landay"} #search_api.search_api.Author{:id 907542, :first-name nil, :last-name nil, :full-name "Beverly L. Harrison"}), :year 2008, :venue "CHI", :ncit 22, :string "Investigating statistical machine learning as a tool for software development.. As statistical machine learning algorithms and techniques continue to mature, many researchers and developers see statistical machine learning not only as a topic of expert study, but also as a tool for software development. Extensive prior work has studied software development, but little prior work has studied software developers applying statistical machine learning. This paper presents interviews of eleven researchers experienced in applying statistical machine learning algorithms and techniques to human-computer interaction problems, as well as a study of ten participants working during a five-hour study to apply statistical machine learning algorithms and techniques to a realistic problem. We distill three related categories of difficulties that arise in applying statistical machine learning as a tool for software development: (1) difficulty pursuing statistical machine learning as an iterative and exploratory process, (2) difficulty understanding relationships between data and the behavior of statistical machine learning algorithms, and (3) difficulty evaluating the performance of statistical machine learning algorithms and techniques in the context of applications. This paper provides important new insight into these difficulties and the need for development tools that better support the application of statistical machine learning.", :doc-id "Investigating statistical machine learning as a tool for software development. 2008 Kayur Patel, James Fogarty, James A. Landay, Beverly L. Harrison"}, 472937 #search_api.search_api.Paper{:id 472937, :key "conf/kdd/Koller03", :title "Statistical learning from relational data.", :abstract nil, :author (#search_api.search_api.Author{:id 560995, :first-name nil, :last-name nil, :full-name "Daphne Koller"}), :year 2003, :venue "KDD", :ncit 0, :string "Statistical learning from relational data.. ", :doc-id "Statistical learning from relational data. 2003 Daphne Koller"}, 3786794 #search_api.search_api.Paper{:id 3786794, :key "conf/fusion/JandelSW12", :title "Online learnability of Statistical Relational Learning in anomaly detection.", :abstract nil, :author (#search_api.search_api.Author{:id 1344508, :first-name nil, :last-name nil, :full-name "Magnus Jändel"} #search_api.search_api.Author{:id 1319563, :first-name nil, :last-name nil, :full-name "Pontus Svenson"} #search_api.search_api.Author{:id 14343624, :first-name nil, :last-name nil, :full-name "Niclas Wadströmer"}), :year 2012, :venue "FUSION", :ncit 0, :string "Online learnability of Statistical Relational Learning in anomaly detection.. ", :doc-id "Online learnability of Statistical Relational Learning in anomaly detection. 2012 Magnus Jändel, Pontus Svenson, Niclas Wadströmer"}, 2927018 #search_api.search_api.Paper{:id 2927018, :key "journals/sigkdd/RamonCFK09", :title "StReBio'09: statistical relational learning and mining in bioinformatics.", :abstract nil, :author (#search_api.search_api.Author{:id 1015109, :first-name nil, :last-name nil, :full-name "Jan Ramon"} #search_api.search_api.Author{:id 892400, :first-name nil, :last-name nil, :full-name "Fabrizio Costa"} #search_api.search_api.Author{:id 981793, :first-name nil, :last-name nil, :full-name "Christophe Costa Florêncio"} #search_api.search_api.Author{:id 19675, :first-name nil, :last-name nil, :full-name "Joost N. Kok"}), :year 2009, :venue "SIGKDD Explorations", :ncit 0, :string "StReBio'09: statistical relational learning and mining in bioinformatics.. ", :doc-id "StReBio'09: statistical relational learning and mining in bioinformatics. 2009 Jan Ramon, Fabrizio Costa, Christophe Costa Florêncio, Joost N. Kok"}, 950858 #search_api.search_api.Paper{:id 950858, :key "journals/jacm/Kearns98", :title "Efficient Noise-Tolerant Learning from Statistical Queries.", :abstract "In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :author (#search_api.search_api.Author{:id 1249307, :first-name nil, :last-name nil, :full-name "Michael J. Kearns"}), :year 1998, :venue "J. ACM", :ncit 456, :string "Efficient Noise-Tolerant Learning from Statistical Queries.. In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :doc-id "Efficient Noise-Tolerant Learning from Statistical Queries. 1998 Michael J. Kearns"}, 3258026 #search_api.search_api.Paper{:id 3258026, :key "journals/ml/RiguzziM12", :title "Applying the information bottleneck to statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 1316705, :first-name nil, :last-name nil, :full-name "Fabrizio Riguzzi"} #search_api.search_api.Author{:id 1294122, :first-name nil, :last-name nil, :full-name "Nicola Di Mauro"}), :year 2012, :venue "Machine Learning", :ncit 14, :string "Applying the information bottleneck to statistical relational learning.. ", :doc-id "Applying the information bottleneck to statistical relational learning. 2012 Fabrizio Riguzzi, Nicola Di Mauro"}, 1288331 #search_api.search_api.Paper{:id 1288331, :key "journals/sadm/SundararaghavanZ09", :title "A statistical learning approach for the design of polycrystalline materials.", :abstract "Important physical properties such as yield strength, elastic modulus, and thermal conductivity depend on the material microstructure. Realization of optimal microstructures is important for hardware components in aerospace applications where there is a need to optimize material properties for improved performance. Microstructures can be tailored through controlled deformation or heat treatment. However, identification of the optimal processing path is a non-trivial (and non-unique) problem. Data-mining techniques are eminently suitable for process design since optimal processing paths can be selected based on available information from a large database-relating processes, properties, and microstructures. In this paper, the problem of designing processing stages that lead to a desired microstructure or material property is addressed by mining over a database of microstructural signatures. A hierarchical X-means classifier is designed to match crystallographic orientation features to a class of microstructural signatures within a database. Instead of the conventional distortion minimization algorithm of k-means, X-means maximizes a Bayesian information measure for calculating cluster centers which allows automatic detection of number of classes. Using the microstructural database, an adaptive data-compression technique based on proper orthogonal decomposition (POD) has been designed to accelerate materials design. In this technique, reduced modes selected adaptively from the database are used to speed up auxiliary microstructure optimization algorithms built over the database. Copyright &copy; 2009 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 1: 000-000, 2009", :author (#search_api.search_api.Author{:id 1415860, :first-name nil, :last-name nil, :full-name "Veera Sundararaghavan"} #search_api.search_api.Author{:id 585807, :first-name nil, :last-name nil, :full-name "Nicholas Zabaras"}), :year 2009, :venue "Statistical Analysis and Data Mining", :ncit 3, :string "A statistical learning approach for the design of polycrystalline materials.. Important physical properties such as yield strength, elastic modulus, and thermal conductivity depend on the material microstructure. Realization of optimal microstructures is important for hardware components in aerospace applications where there is a need to optimize material properties for improved performance. Microstructures can be tailored through controlled deformation or heat treatment. However, identification of the optimal processing path is a non-trivial (and non-unique) problem. Data-mining techniques are eminently suitable for process design since optimal processing paths can be selected based on available information from a large database-relating processes, properties, and microstructures. In this paper, the problem of designing processing stages that lead to a desired microstructure or material property is addressed by mining over a database of microstructural signatures. A hierarchical X-means classifier is designed to match crystallographic orientation features to a class of microstructural signatures within a database. Instead of the conventional distortion minimization algorithm of k-means, X-means maximizes a Bayesian information measure for calculating cluster centers which allows automatic detection of number of classes. Using the microstructural database, an adaptive data-compression technique based on proper orthogonal decomposition (POD) has been designed to accelerate materials design. In this technique, reduced modes selected adaptively from the database are used to speed up auxiliary microstructure optimization algorithms built over the database. Copyright &copy; 2009 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 1: 000-000, 2009", :doc-id "A statistical learning approach for the design of polycrystalline materials. 2009 Veera Sundararaghavan, Nicholas Zabaras"}, 797516 #search_api.search_api.Paper{:id 797516, :key "journals/coling/OchN04", :title "The Alignment Template Approach to Statistical Machine Translation.", :abstract "A phrase-based statistical machine translation approach &mdash; the alignment template approach &mdash; is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source&ndash;channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German&ndash;English speech VERBMOBIL task, we analyze the effect of various system components. On the French&ndash;English Canadian HANSARDS task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese&ndash;English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.", :author (#search_api.search_api.Author{:id 1493982, :first-name nil, :last-name nil, :full-name "Franz Josef Och"} #search_api.search_api.Author{:id 526052, :first-name nil, :last-name nil, :full-name "Hermann Ney"}), :year 2004, :venue "Computational Linguistics", :ncit 644, :string "The Alignment Template Approach to Statistical Machine Translation.. A phrase-based statistical machine translation approach &mdash; the alignment template approach &mdash; is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source&ndash;channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German&ndash;English speech VERBMOBIL task, we analyze the effect of various system components. On the French&ndash;English Canadian HANSARDS task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese&ndash;English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.", :doc-id "The Alignment Template Approach to Statistical Machine Translation. 2004 Franz Josef Och, Hermann Ney"}, 3074924 #search_api.search_api.Paper{:id 3074924, :key "journals/ml/RettingerNT11", :title "Statistical relational learning of trust.", :abstract nil, :author (#search_api.search_api.Author{:id 444383, :first-name nil, :last-name nil, :full-name "Achim Rettinger"} #search_api.search_api.Author{:id 222492, :first-name nil, :last-name nil, :full-name "Matthias Nickles"} #search_api.search_api.Author{:id 680469, :first-name nil, :last-name nil, :full-name "Volker Tresp"}), :year 2011, :venue "Machine Learning", :ncit 6, :string "Statistical relational learning of trust.. ", :doc-id "Statistical relational learning of trust. 2011 Achim Rettinger, Matthias Nickles, Volker Tresp"}, 1185901 #search_api.search_api.Paper{:id 1185901, :key "journals/jocn/MuellerBF08", :title "The Role of Pause Cues in Language Learning: The Emergence of Event-related Potentials Related to Sequence Processing.", :abstract "Humans can derive sequential dependencies from unfamiliar artificial speech within several of minutes of exposure. However, there is an ongoing debate about the nature of the underlying learning processes. In a widely discussed study Pea et al. [Pea, M., Bonatti, L. L., Nespor, M., & Mehler, J. Signal-driven computations in speech processing. Science, 298, 604607, 2002] argued for the importance of subtle acoustic cues in the signal, such as pauses, in order to switch between two computational mechanisms, which are conceptualized as rule-based versus statistical. The present study was aimed to approach this problem by recording event-related potentials in response to correct and incorrect phrases consisting of bisyllabics after short exposure to either rule-based or random artificial speech streams. Rule-based streams contained dependencies of the form AXC, whereby A elements reliably predicted the C elements and X elements were variable. Participants were exposed to four input and test phases. Two of the input streams were rule-based and contained either only probabilistic information related to the distribution of the AXC stimuli or an additional acoustic cue indicating the boundaries of relevant units. The other two streams were random variations of the rule-based streams. During the test phase in the condition with pause cues, an early negativity and a later positivity emerged for correct and incorrect items in comparison to their acoustically identical counterparts, which were presented after the random control condition. In the noncued condition, only negativities were seen. The timing and the scalp distribution of the negativities were different for correct and incorrect sequences in both the cued and the noncued conditions. The results are interpreted in support of a view of grammatical learning in which both distributional and acoustic cues may contribute to different aspects of syntactic learning.", :author (#search_api.search_api.Author{:id 983626, :first-name nil, :last-name nil, :full-name "Jutta L. Mueller"} #search_api.search_api.Author{:id -1, :first-name nil, :last-name nil, :full-name "Jörg Bahlmann"} #search_api.search_api.Author{:id 258614, :first-name nil, :last-name nil, :full-name "Angela D. Friederici"}), :year 2008, :venue "J. Cognitive Neuroscience", :ncit 16, :string "The Role of Pause Cues in Language Learning: The Emergence of Event-related Potentials Related to Sequence Processing.. Humans can derive sequential dependencies from unfamiliar artificial speech within several of minutes of exposure. However, there is an ongoing debate about the nature of the underlying learning processes. In a widely discussed study Pea et al. [Pea, M., Bonatti, L. L., Nespor, M., & Mehler, J. Signal-driven computations in speech processing. Science, 298, 604607, 2002] argued for the importance of subtle acoustic cues in the signal, such as pauses, in order to switch between two computational mechanisms, which are conceptualized as rule-based versus statistical. The present study was aimed to approach this problem by recording event-related potentials in response to correct and incorrect phrases consisting of bisyllabics after short exposure to either rule-based or random artificial speech streams. Rule-based streams contained dependencies of the form AXC, whereby A elements reliably predicted the C elements and X elements were variable. Participants were exposed to four input and test phases. Two of the input streams were rule-based and contained either only probabilistic information related to the distribution of the AXC stimuli or an additional acoustic cue indicating the boundaries of relevant units. The other two streams were random variations of the rule-based streams. During the test phase in the condition with pause cues, an early negativity and a later positivity emerged for correct and incorrect items in comparison to their acoustically identical counterparts, which were presented after the random control condition. In the noncued condition, only negativities were seen. The timing and the scalp distribution of the negativities were different for correct and incorrect sequences in both the cued and the noncued conditions. The results are interpreted in support of a view of grammatical learning in which both distributional and acoustic cues may contribute to different aspects of syntactic learning.", :doc-id "The Role of Pause Cues in Language Learning: The Emergence of Event-related Potentials Related to Sequence Processing. 2008 Jutta L. Mueller, Jörg Bahlmann, Angela D. Friederici"}, 1028237 #search_api.search_api.Paper{:id 1028237, :key "journals/npl/LuoUN99", :title "Unsupervised Learning of Higher-Order Statistics.", :abstract "This paper deals with the adaptive extraction of higher-order statistics of related signals. We will show how to use higher-order neural networks to adaptively extract the higher-order cumulant matrices and tensors with an invariant weight norm. This proposed scheme can serve as an alternative tool in many application fields with higher-order statistics.", :author (#search_api.search_api.Author{:id 658006, :first-name nil, :last-name nil, :full-name "Fa-Long Luo"} #search_api.search_api.Author{:id 416876, :first-name nil, :last-name nil, :full-name "Rolf Unbehauen"} #search_api.search_api.Author{:id 1370072, :first-name nil, :last-name nil, :full-name "Tertulien Ndjountche"}), :year 1999, :venue "Neural Processing Letters", :ncit 0, :string "Unsupervised Learning of Higher-Order Statistics.. This paper deals with the adaptive extraction of higher-order statistics of related signals. We will show how to use higher-order neural networks to adaptively extract the higher-order cumulant matrices and tensors with an invariant weight norm. This proposed scheme can serve as an alternative tool in many application fields with higher-order statistics.", :doc-id "Unsupervised Learning of Higher-Order Statistics. 1999 Fa-Long Luo, Rolf Unbehauen, Tertulien Ndjountche"}, 20109 #search_api.search_api.Paper{:id 20109, :key "conf/acl/Chiang05", :title "A Hierarchical Phrase-Based Model for Statistical Machine Translation.", :abstract "We present a statistical phrase-based translation model that uses hierarchical phrases---phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.", :author (#search_api.search_api.Author{:id 415922, :first-name nil, :last-name nil, :full-name "David Chiang"}), :year 2005, :venue "ACL", :ncit 739, :string "A Hierarchical Phrase-Based Model for Statistical Machine Translation.. We present a statistical phrase-based translation model that uses hierarchical phrases---phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.", :doc-id "A Hierarchical Phrase-Based Model for Statistical Machine Translation. 2005 David Chiang"}, 541902 #search_api.search_api.Paper{:id 541902, :key "conf/pkdd/Raedt05", :title "Statistical Relational Learning: An Inductive Logic Programming Perspective.", :abstract nil, :author (#search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2005, :venue "PKDD", :ncit 3, :string "Statistical Relational Learning: An Inductive Logic Programming Perspective.. ", :doc-id "Statistical Relational Learning: An Inductive Logic Programming Perspective. 2005 Luc De Raedt"}, 1010126 #search_api.search_api.Paper{:id 1010126, :key "journals/ml/DietterichDGMT08", :title "Structured machine learning: the next ten years.", :abstract "The field of inductive logic programming (ILP) has made steady progress, since the first ILP workshop in 1991, based on a balance of developments in theory, implementations and applications. More recently there has been an increased emphasis on Probabilistic ILP and the related fields of Statistical Relational Learning (SRL) and Structured Prediction. The goal of the current paper is to consider these emerging trends and chart out the strategic directions and open problems for the broader area of structured machine learning for the next 10 years.", :author (#search_api.search_api.Author{:id 737377, :first-name nil, :last-name nil, :full-name "Thomas G. Dietterich"} #search_api.search_api.Author{:id 668085, :first-name nil, :last-name nil, :full-name "Pedro Domingos"} #search_api.search_api.Author{:id 1449720, :first-name nil, :last-name nil, :full-name "Lise Getoor"} #search_api.search_api.Author{:id 120131, :first-name nil, :last-name nil, :full-name "Stephen Muggleton"} #search_api.search_api.Author{:id 435675, :first-name nil, :last-name nil, :full-name "Prasad Tadepalli"}), :year 2008, :venue "Machine Learning", :ncit 42, :string "Structured machine learning: the next ten years.. The field of inductive logic programming (ILP) has made steady progress, since the first ILP workshop in 1991, based on a balance of developments in theory, implementations and applications. More recently there has been an increased emphasis on Probabilistic ILP and the related fields of Statistical Relational Learning (SRL) and Structured Prediction. The goal of the current paper is to consider these emerging trends and chart out the strategic directions and open problems for the broader area of structured machine learning for the next 10 years.", :doc-id "Structured machine learning: the next ten years. 2008 Thomas G. Dietterich, Pedro Domingos, Lise Getoor, Stephen Muggleton, Prasad Tadepalli"}, 1009230 #search_api.search_api.Paper{:id 1009230, :key "journals/ml/BrazdilSC03", :title "Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results.", :abstract "We present a meta-learning method to support selection of candidate learning algorithms. It uses a k-Nearest Neighbor algorithm to identify the datasets that are most similar to the one at hand. The distance between datasets is assessed using a relatively small set of data characteristics, which was selected to represent properties that affect algorithm performance. The performance of the candidate algorithms on those datasets is used to generate a recommendation to the user in the form of a ranking. The performance is assessed using a multicriteria evaluation measure that takes not only accuracy, but also time into account. As it is not common in Machine Learning to work with rankings, we had to identify and adapt existing statistical techniques to devise an appropriate evaluation methodology. Using that methodology, we show that the meta-learning method presented leads to significantly better rankings than the baseline ranking method. The evaluation methodology is general and can be adapted to other ranking problems. Although here we have concentrated on ranking classification algorithms, the meta-learning framework presented can provide assistance in the selection of combinations of methods or more complex problem solving strategies.", :author (#search_api.search_api.Author{:id 1157783, :first-name nil, :last-name nil, :full-name "Pavel Brazdil"} #search_api.search_api.Author{:id 791367, :first-name nil, :last-name nil, :full-name "Carlos Soares"} #search_api.search_api.Author{:id 212848, :first-name nil, :last-name nil, :full-name "Joaquim Pinto da Costa"}), :year 2003, :venue "Machine Learning", :ncit 200, :string "Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results.. We present a meta-learning method to support selection of candidate learning algorithms. It uses a k-Nearest Neighbor algorithm to identify the datasets that are most similar to the one at hand. The distance between datasets is assessed using a relatively small set of data characteristics, which was selected to represent properties that affect algorithm performance. The performance of the candidate algorithms on those datasets is used to generate a recommendation to the user in the form of a ranking. The performance is assessed using a multicriteria evaluation measure that takes not only accuracy, but also time into account. As it is not common in Machine Learning to work with rankings, we had to identify and adapt existing statistical techniques to devise an appropriate evaluation methodology. Using that methodology, we show that the meta-learning method presented leads to significantly better rankings than the baseline ranking method. The evaluation methodology is general and can be adapted to other ranking problems. Although here we have concentrated on ranking classification algorithms, the meta-learning framework presented can provide assistance in the selection of combinations of methods or more complex problem solving strategies.", :doc-id "Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results. 2003 Pavel Brazdil, Carlos Soares, Joaquim Pinto da Costa"}, 473199 #search_api.search_api.Paper{:id 473199, :key "conf/kdd/PopesculU04", :title "Cluster-based concept invention for statistical relational learning.", :abstract "We use clustering to derive new relations which augment database schema used in automatic generation of predictive features in statistical relational learning. Entities derived from clusters increase the expressivity of feature spaces by creating new first-class concepts which contribute to the creation of new features. For example, in CiteSeer, papers can be clustered based on words or citations giving \"topics\", and authors can be clustered based on documents they co-author giving \"communities\". Such cluster-derived concepts become part of more complex feature expressions. Out of the large number of generated features, those which improve predictive accuracy are kept in the model, as decided by statistical feature selection criteria. We present results demonstrating improved accuracy on two tasks, venue prediction and link prediction, using CiteSeer data.", :author (#search_api.search_api.Author{:id 597076, :first-name nil, :last-name nil, :full-name "Alexandrin Popescul"} #search_api.search_api.Author{:id 352656, :first-name nil, :last-name nil, :full-name "Lyle H. Ungar"}), :year 2004, :venue "KDD", :ncit 31, :string "Cluster-based concept invention for statistical relational learning.. We use clustering to derive new relations which augment database schema used in automatic generation of predictive features in statistical relational learning. Entities derived from clusters increase the expressivity of feature spaces by creating new first-class concepts which contribute to the creation of new features. For example, in CiteSeer, papers can be clustered based on words or citations giving \"topics\", and authors can be clustered based on documents they co-author giving \"communities\". Such cluster-derived concepts become part of more complex feature expressions. Out of the large number of generated features, those which improve predictive accuracy are kept in the model, as decided by statistical feature selection criteria. We present results demonstrating improved accuracy on two tasks, venue prediction and link prediction, using CiteSeer data.", :doc-id "Cluster-based concept invention for statistical relational learning. 2004 Alexandrin Popescul, Lyle H. Ungar"}, 472783 #search_api.search_api.Paper{:id 472783, :key "conf/kdd/HeckermanGC94", :title "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.", :abstract "We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :author (#search_api.search_api.Author{:id 114083, :first-name nil, :last-name nil, :full-name "David Heckerman"} #search_api.search_api.Author{:id 330355, :first-name nil, :last-name nil, :full-name "Dan Geiger"} #search_api.search_api.Author{:id 1215050, :first-name nil, :last-name nil, :full-name "David Maxwell Chickering"}), :year 1994, :venue "KDD Workshop", :ncit 3837, :string "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.. We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :doc-id "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data. 1994 David Heckerman, Dan Geiger, David Maxwell Chickering"}, 952368 #search_api.search_api.Paper{:id 952368, :key "journals/jair/DaumeM06", :title "Domain Adaptation for Statistical Classifiers.", :abstract "The most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution. Unfortunately, in many applications, the \"in-domain\" test data is drawn from a distribution that is related, but not identical, to the \"out-of-domain\" distribution of the training data. We consider the common case in which labeled out-of-domain data is plentiful, but labeled in-domain data is scarce. We introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts. We present efficient inference algorithms for this special case based on the technique of conditional expectation maximization. Our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain.", :author (#search_api.search_api.Author{:id 820633, :first-name nil, :last-name nil, :full-name "Hal Daumé III"} #search_api.search_api.Author{:id 911330, :first-name nil, :last-name nil, :full-name "Daniel Marcu"}), :year 2006, :venue "J. Artif. Intell. Res. (JAIR)", :ncit 250, :string "Domain Adaptation for Statistical Classifiers.. The most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution. Unfortunately, in many applications, the \"in-domain\" test data is drawn from a distribution that is related, but not identical, to the \"out-of-domain\" distribution of the training data. We consider the common case in which labeled out-of-domain data is plentiful, but labeled in-domain data is scarce. We introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts. We present efficient inference algorithms for this special case based on the technique of conditional expectation maximization. Our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain.", :doc-id "Domain Adaptation for Statistical Classifiers. 2006 Hal Daumé III, Daniel Marcu"}, 2775184 #search_api.search_api.Paper{:id 2775184, :key "conf/ai/KhosraviB10", :title "A Survey on Statistical Relational Learning.", :abstract "Frequent subgraph mining is an active research topic in the data mining community. A graph is a general model to represent data and has been used in many domains like cheminformatics and bioinformatics. Mining patterns from graph databases is challenging since graph related operations, such as subgraph testing, generally have higher time complexity than the corresponding operations on itemsets, sequences, and trees, which have been studied extensively. We propose a novel frequent subgraph mining algorithm: FFSM, which employs a vertical search scheme within an algebraic graph framework we have developed to reduce the number of redundant candidates proposed. Our empirical study on synthetic and real datasets demonstrates that FFSM achieves a substantial performance gain over the current start-of-the-art subgraph mining algorithm gSpan.", :author (#search_api.search_api.Author{:id 133910, :first-name nil, :last-name nil, :full-name "Hassan Khosravi"} #search_api.search_api.Author{:id 84842, :first-name nil, :last-name nil, :full-name "Bahareh Bina"}), :year 2010, :venue "Canadian Conference on AI", :ncit 1, :string "A Survey on Statistical Relational Learning.. Frequent subgraph mining is an active research topic in the data mining community. A graph is a general model to represent data and has been used in many domains like cheminformatics and bioinformatics. Mining patterns from graph databases is challenging since graph related operations, such as subgraph testing, generally have higher time complexity than the corresponding operations on itemsets, sequences, and trees, which have been studied extensively. We propose a novel frequent subgraph mining algorithm: FFSM, which employs a vertical search scheme within an algebraic graph framework we have developed to reduce the number of redundant candidates proposed. Our empirical study on synthetic and real datasets demonstrates that FFSM achieves a substantial performance gain over the current start-of-the-art subgraph mining algorithm gSpan.", :doc-id "A Survey on Statistical Relational Learning. 2010 Hassan Khosravi, Bahareh Bina"}, 1319504 #search_api.search_api.Paper{:id 1319504, :key "conf/icdm/NevilleGE09", :title "Evaluating Statistical Tests for Within-Network Classifiers of Relational Data.", :abstract "Recently a number of modeling techniques have been developed for data mining and machine learning in relational and network domains where the instances are not independent and identically distributed (i.i.d.). These methods specifically exploit the statistical dependencies among instances in order to improve classification accuracy. However, there has been little focus on how these same dependencies affect our ability to draw accurate conclusions about the performance of the models. More specifically, the complex link structure and attribute dependencies in network data violate the assumptions of many conventional statistical tests and make it difficult to use these tests to assess the models in an unbiased manner. In this work, we examine the task of within-network classification and the question of whether two algorithms will learn models which will result in significantly different levels of performance. We show that the commonly-used form of evaluation (paired t-test on overlapping network samples) can result in an unacceptable level of Type I error. Furthermore we show that Type I error increases as (1) the correlation among instances increases and (2) the size of the evaluation set increases (i.e., the proportion of labeled nodes in the network decreases). We propose a method for network cross-validation that combined with paired t-tests produces more acceptable levels of Type I error while still providing reasonable levels of statistical power (i.e., Type II error).", :author (#search_api.search_api.Author{:id 150768, :first-name nil, :last-name nil, :full-name "Jennifer Neville"} #search_api.search_api.Author{:id 335801, :first-name nil, :last-name nil, :full-name "Brian Gallagher"} #search_api.search_api.Author{:id 806969, :first-name nil, :last-name nil, :full-name "Tina Eliassi-Rad"}), :year 2009, :venue "ICDM", :ncit 12, :string "Evaluating Statistical Tests for Within-Network Classifiers of Relational Data.. Recently a number of modeling techniques have been developed for data mining and machine learning in relational and network domains where the instances are not independent and identically distributed (i.i.d.). These methods specifically exploit the statistical dependencies among instances in order to improve classification accuracy. However, there has been little focus on how these same dependencies affect our ability to draw accurate conclusions about the performance of the models. More specifically, the complex link structure and attribute dependencies in network data violate the assumptions of many conventional statistical tests and make it difficult to use these tests to assess the models in an unbiased manner. In this work, we examine the task of within-network classification and the question of whether two algorithms will learn models which will result in significantly different levels of performance. We show that the commonly-used form of evaluation (paired t-test on overlapping network samples) can result in an unacceptable level of Type I error. Furthermore we show that Type I error increases as (1) the correlation among instances increases and (2) the size of the evaluation set increases (i.e., the proportion of labeled nodes in the network decreases). We propose a method for network cross-validation that combined with paired t-tests produces more acceptable levels of Type I error while still providing reasonable levels of statistical power (i.e., Type II error).", :doc-id "Evaluating Statistical Tests for Within-Network Classifiers of Relational Data. 2009 Jennifer Neville, Brian Gallagher, Tina Eliassi-Rad"}, 190192 #search_api.search_api.Paper{:id 190192, :key "conf/esws/KieferBL08", :title "Adding Data Mining Support to SPARQL Via Statistical Relational Learning Methods.", :abstract "Exploiting the complex structure of relational data enables to build better models by taking into account the additional information provided by the links between objects. We extend this idea to the Semantic Web by introducing our novel SPARQL-ML approach to perform data mining for Semantic Web data. Our approach is based on traditional SPARQL and statistical relational learning methods, such as Relational Probability Trees and Relational Bayesian Classifiers. We analyze our approach thoroughly conducting three sets of experiments on synthetic as well as real-world data sets. Our analytical results show that our approach can be used for any Semantic Web data set to perform instance-based learning and classification. A comparison to kernel methods used in Support Vector Machines shows that our approach is superior in terms of classification accuracy.", :author (#search_api.search_api.Author{:id 1506617, :first-name nil, :last-name nil, :full-name "Christoph Kiefer"} #search_api.search_api.Author{:id 798825, :first-name nil, :last-name nil, :full-name "Abraham Bernstein"} #search_api.search_api.Author{:id 322034, :first-name nil, :last-name nil, :full-name "André Locher"}), :year 2008, :venue "ESWC", :ncit 26, :string "Adding Data Mining Support to SPARQL Via Statistical Relational Learning Methods.. Exploiting the complex structure of relational data enables to build better models by taking into account the additional information provided by the links between objects. We extend this idea to the Semantic Web by introducing our novel SPARQL-ML approach to perform data mining for Semantic Web data. Our approach is based on traditional SPARQL and statistical relational learning methods, such as Relational Probability Trees and Relational Bayesian Classifiers. We analyze our approach thoroughly conducting three sets of experiments on synthetic as well as real-world data sets. Our analytical results show that our approach can be used for any Semantic Web data set to perform instance-based learning and classification. A comparison to kernel methods used in Support Vector Machines shows that our approach is superior in terms of classification accuracy.", :doc-id "Adding Data Mining Support to SPARQL Via Statistical Relational Learning Methods. 2008 Christoph Kiefer, Abraham Bernstein, André Locher"}, 1279856 #search_api.search_api.Paper{:id 1279856, :key "journals/neco/AmariM93", :title "Statistical Theory of Learning Curves under Entropic Loss Criterion.", :abstract "The present paper elucidates a universal property of learning curves, which shows how the generalization error, training error, and the complexity of the underlying stochastic machine are related and how the behavior of a stochastic machine is improved as the number of training examples increases. The error is measured by the entropic loss. It is proved that the generalization error converges to H0, the entropy of the conditional distribution of the true machine, as H0 + m*/(2t), while the training error converges as H0-m*/(2t), where t is the number of examples and m* shows the complexity of the network. When the model is faithful, implying that the true machine is in the model, m* is reduced to m, the number of modifiable parameters. This is a universal law because it holds for any regular machine irrespective of its structure under the maximum likelihood estimator. Similar relations are obtained for the Bayes and Gibbs learning algorithms. These learning curves show the relation among the accuracy of learning, the complexity of a model, and the number of training examples.", :author (#search_api.search_api.Author{:id 99365, :first-name nil, :last-name nil, :full-name "Shun-ichi Amari"} #search_api.search_api.Author{:id 918942, :first-name nil, :last-name nil, :full-name "Noboru Murata"}), :year 1993, :venue "Neural Computation", :ncit 143, :string "Statistical Theory of Learning Curves under Entropic Loss Criterion.. The present paper elucidates a universal property of learning curves, which shows how the generalization error, training error, and the complexity of the underlying stochastic machine are related and how the behavior of a stochastic machine is improved as the number of training examples increases. The error is measured by the entropic loss. It is proved that the generalization error converges to H0, the entropy of the conditional distribution of the true machine, as H0 + m*/(2t), while the training error converges as H0-m*/(2t), where t is the number of examples and m* shows the complexity of the network. When the model is faithful, implying that the true machine is in the model, m* is reduced to m, the number of modifiable parameters. This is a universal law because it holds for any regular machine irrespective of its structure under the maximum likelihood estimator. Similar relations are obtained for the Bayes and Gibbs learning algorithms. These learning curves show the relation among the accuracy of learning, the complexity of a model, and the number of training examples.", :doc-id "Statistical Theory of Learning Curves under Entropic Loss Criterion. 1993 Shun-ichi Amari, Noboru Murata"}, 498833 #search_api.search_api.Paper{:id 498833, :key "conf/miccai/UnalNSF08", :title "Customized Design of Hearing Aids Using Statistical Shape Learning.", :abstract "3D shape modeling is a crucial component of rapid prototyping systems that customize shapes of implants and prosthetic devices to a patient's anatomy. In this paper, we present a solution to the problem of customized 3D shape modeling using a statistical shape analysis framework. We design a novel method to learn the relationship between two classes of shapes, which are related by certain operations or transformation. The two associated shape classes are represented in a lower dimensional manifold, and the reduced set of parameters obtained in this subspace is utilized in an estimation, which is exemplified by a multivariate regression in this paper. We demonstrate our method with a felicitous application to estimation of customized hearing aid devices.", :author (#search_api.search_api.Author{:id 987788, :first-name nil, :last-name nil, :full-name "Gozde B. Unal"} #search_api.search_api.Author{:id 1388067, :first-name nil, :last-name nil, :full-name "Delphine Nain"} #search_api.search_api.Author{:id 174234, :first-name nil, :last-name nil, :full-name "Gregory G. Slabaugh"} #search_api.search_api.Author{:id 303194, :first-name nil, :last-name nil, :full-name "Tong Fang"}), :year 2008, :venue "MICCAI (1)", :ncit 6, :string "Customized Design of Hearing Aids Using Statistical Shape Learning.. 3D shape modeling is a crucial component of rapid prototyping systems that customize shapes of implants and prosthetic devices to a patient's anatomy. In this paper, we present a solution to the problem of customized 3D shape modeling using a statistical shape analysis framework. We design a novel method to learn the relationship between two classes of shapes, which are related by certain operations or transformation. The two associated shape classes are represented in a lower dimensional manifold, and the reduced set of parameters obtained in this subspace is utilized in an estimation, which is exemplified by a multivariate regression in this paper. We demonstrate our method with a felicitous application to estimation of customized hearing aid devices.", :doc-id "Customized Design of Hearing Aids Using Statistical Shape Learning. 2008 Gozde B. Unal, Delphine Nain, Gregory G. Slabaugh, Tong Fang"}, 2784531 #search_api.search_api.Paper{:id 2784531, :key "conf/ecai/JainBB10", :title "Adaptive Markov Logic Networks: Learning Statistical Relational Models with Dynamic Parameters.", :abstract "Statistical relational models, such as Markov logic networks, seek to compactly describe properties of relational domains by representing general principles about objects belonging to particular classes. Models are intended to be independent of the set of objects to which these principles can be applied, and it is assumed that the principles will soundly generalize across arbitrary sets of objects. In this paper, we point out limitations of models that seek to represent the corresponding principles with a fixed set of parameters and discuss the conditions under which the soundness of fixed parameters is indeed questionable. We propose a novel representation formalism called adaptive Markov logic networks to allow more flexible representations of relational domains, which involve parameters that are dynamically adjusted to fit the properties of an instantiation by phrasing the model's parameters as functions over attributes of the instantiation at hand. We empirically demonstrate the value of our learning and representation system on a simple but well-motivated example domain. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :author (#search_api.search_api.Author{:id 639688, :first-name nil, :last-name nil, :full-name "Dominik Jain"} #search_api.search_api.Author{:id 1005331, :first-name nil, :last-name nil, :full-name "Andreas Barthels"} #search_api.search_api.Author{:id 361314, :first-name nil, :last-name nil, :full-name "Michael Beetz"}), :year 2010, :venue "ECAI", :ncit 8, :string "Adaptive Markov Logic Networks: Learning Statistical Relational Models with Dynamic Parameters.. Statistical relational models, such as Markov logic networks, seek to compactly describe properties of relational domains by representing general principles about objects belonging to particular classes. Models are intended to be independent of the set of objects to which these principles can be applied, and it is assumed that the principles will soundly generalize across arbitrary sets of objects. In this paper, we point out limitations of models that seek to represent the corresponding principles with a fixed set of parameters and discuss the conditions under which the soundness of fixed parameters is indeed questionable. We propose a novel representation formalism called adaptive Markov logic networks to allow more flexible representations of relational domains, which involve parameters that are dynamically adjusted to fit the properties of an instantiation by phrasing the model's parameters as functions over attributes of the instantiation at hand. We empirically demonstrate the value of our learning and representation system on a simple but well-motivated example domain. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :doc-id "Adaptive Markov Logic Networks: Learning Statistical Relational Models with Dynamic Parameters. 2010 Dominik Jain, Andreas Barthels, Michael Beetz"}, 1009299 #search_api.search_api.Paper{:id 1009299, :key "journals/ml/BeefermanBL99", :title "Statistical Models for Text Segmentation.", :abstract "This paper introduces a new statistical approach to automatically partitioning text into coherent segments. The approach is based on a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text. The models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may be domain-specific, that tend to be used near segment boundaries. Assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, Wall Street Journal news articles and television broadcast news story transcripts. Quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. This metric is used to make a quantitative assessment of the relative contributions of the different feature types, as well as a comparison with decision trees and previously proposed text segmentation algorithms.", :author (#search_api.search_api.Author{:id 1914, :first-name nil, :last-name nil, :full-name "Doug Beeferman"} #search_api.search_api.Author{:id 109818, :first-name nil, :last-name nil, :full-name "Adam L. Berger"} #search_api.search_api.Author{:id 1463112, :first-name nil, :last-name nil, :full-name "John D. Lafferty"}), :year 1999, :venue "Machine Learning", :ncit 488, :string "Statistical Models for Text Segmentation.. This paper introduces a new statistical approach to automatically partitioning text into coherent segments. The approach is based on a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text. The models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may be domain-specific, that tend to be used near segment boundaries. Assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, Wall Street Journal news articles and television broadcast news story transcripts. Quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. This metric is used to make a quantitative assessment of the relative contributions of the different feature types, as well as a comparison with decision trees and previously proposed text segmentation algorithms.", :doc-id "Statistical Models for Text Segmentation. 1999 Doug Beeferman, Adam L. Berger, John D. Lafferty"}, 952340 #search_api.search_api.Paper{:id 952340, :key "journals/jair/Jaeger05", :title "Ignorability in Statistical and Probabilistic Inference.", :abstract "When dealing with incomplete data in statistical learning, or incomplete observations in probabilistic inference, one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened. Since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive, one asks for conditions under which it can be safely ignored. Such conditions are given by the missing at random (mar) and coarsened at random (car) assumptions. In this paper we provide an in-depth analysis of several questions relating to mar/car assumptions. Main purpose of our study is to provide criteria by which one may evaluate whether a car assumption is reasonable for a particular data collecting or observational process. This question is complicated by the fact that several distinct versions of mar/car assumptions exist. We therefore first provide an overview over these different versions, in which we highlight the distinction between distributional and coarsening variable induced versions. We show that distributional versions are less restrictive and sufficient for most applications. We then address from two different perspectives the question of when the mar/car assumption is warranted. First we provide a \"static\" analysis that characterizes the admissibility of the car assumption in terms of the support structure of the joint probability distribution of complete data and incomplete observations. Here we obtain an equivalence characterization that improves and extends a recent result by Grünwald and Halpern. We then turn to a \"procedural\" analysis that characterizes the admissibility of the car assumption in terms of procedural models for the actual data (or observation) generating process. The main result of this analysis is that the stronger coarsened completely at random (ccar) condition is arguably the most reasonable assumption, as it alone corresponds to data coarsening procedures that satisfy a natural robustness property.", :author (#search_api.search_api.Author{:id 252997, :first-name nil, :last-name nil, :full-name "Manfred Jaeger"}), :year 2005, :venue "J. Artif. Intell. Res. (JAIR)", :ncit 7, :string "Ignorability in Statistical and Probabilistic Inference.. When dealing with incomplete data in statistical learning, or incomplete observations in probabilistic inference, one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened. Since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive, one asks for conditions under which it can be safely ignored. Such conditions are given by the missing at random (mar) and coarsened at random (car) assumptions. In this paper we provide an in-depth analysis of several questions relating to mar/car assumptions. Main purpose of our study is to provide criteria by which one may evaluate whether a car assumption is reasonable for a particular data collecting or observational process. This question is complicated by the fact that several distinct versions of mar/car assumptions exist. We therefore first provide an overview over these different versions, in which we highlight the distinction between distributional and coarsening variable induced versions. We show that distributional versions are less restrictive and sufficient for most applications. We then address from two different perspectives the question of when the mar/car assumption is warranted. First we provide a \"static\" analysis that characterizes the admissibility of the car assumption in terms of the support structure of the joint probability distribution of complete data and incomplete observations. Here we obtain an equivalence characterization that improves and extends a recent result by Grünwald and Halpern. We then turn to a \"procedural\" analysis that characterizes the admissibility of the car assumption in terms of procedural models for the actual data (or observation) generating process. The main result of this analysis is that the stronger coarsened completely at random (ccar) condition is arguably the most reasonable assumption, as it alone corresponds to data coarsening procedures that satisfy a natural robustness property.", :doc-id "Ignorability in Statistical and Probabilistic Inference. 2005 Manfred Jaeger"}, 985236 #search_api.search_api.Paper{:id 985236, :key "journals/jmlr/PasseriniFR06", :title "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.", :abstract "We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :author (#search_api.search_api.Author{:id 1520074, :first-name nil, :last-name nil, :full-name "Andrea Passerini"} #search_api.search_api.Author{:id 34394, :first-name nil, :last-name nil, :full-name "Paolo Frasconi"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2006, :venue "Journal of Machine Learning Research", :ncit 31, :string "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.. We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :doc-id "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting. 2006 Andrea Passerini, Paolo Frasconi, Luc De Raedt"}, 1017236 #search_api.search_api.Paper{:id 1017236, :key "journals/mta/HanLL08", :title "Semantic image classification using statistical local spatial relations model.", :abstract "In this paper, a statistical model called statistical local spatial relations (SLSR) is presented as a novel technique of a learning model with spatial and statistical information for semantic image classification. The model is inspired by probabilistic Latent Semantic Analysis (PLSA) for text mining. In text analysis, PLSA is used to discover topics in a corpus using the bag-of-word document representation. In SLSR, we treat image categories as topics, therefore an image containing instances of multiple categories can be modeled as a mixture of topics. More significantly, SLSR introduces spatial relation information as a factor which is not present in PLSA. SLSR has rotation, scale, translation and affine invariant properties and can solve partial occlusion problems. Using the Dirichlet process and variational Expectation-Maximization learning algorithm, SLSR is developed as an implementation of an image classification algorithm. SLSR uses an unsupervised process which can capture both spatial relations and statistical information simultaneously. The experiments are demonstrated on some standard data sets and show that the SLSR model is a promising model for semantic image classification problems.", :author (#search_api.search_api.Author{:id 51919, :first-name nil, :last-name nil, :full-name "Dongfeng Han"} #search_api.search_api.Author{:id 589651, :first-name nil, :last-name nil, :full-name "Wenhui Li"} #search_api.search_api.Author{:id 1381040, :first-name nil, :last-name nil, :full-name "Zongcheng Li"}), :year 2008, :venue "Multimedia Tools Appl.", :ncit 11, :string "Semantic image classification using statistical local spatial relations model.. In this paper, a statistical model called statistical local spatial relations (SLSR) is presented as a novel technique of a learning model with spatial and statistical information for semantic image classification. The model is inspired by probabilistic Latent Semantic Analysis (PLSA) for text mining. In text analysis, PLSA is used to discover topics in a corpus using the bag-of-word document representation. In SLSR, we treat image categories as topics, therefore an image containing instances of multiple categories can be modeled as a mixture of topics. More significantly, SLSR introduces spatial relation information as a factor which is not present in PLSA. SLSR has rotation, scale, translation and affine invariant properties and can solve partial occlusion problems. Using the Dirichlet process and variational Expectation-Maximization learning algorithm, SLSR is developed as an implementation of an image classification algorithm. SLSR uses an unsupervised process which can capture both spatial relations and statistical information simultaneously. The experiments are demonstrated on some standard data sets and show that the SLSR model is a promising model for semantic image classification problems.", :doc-id "Semantic image classification using statistical local spatial relations model. 2008 Dongfeng Han, Wenhui Li, Zongcheng Li"}, 3623508 #search_api.search_api.Paper{:id 3623508, :key "journals/ijsnm/EspositoFBM12", :title "Social networks and statistical relational learning: a survey.", :abstract nil, :author (#search_api.search_api.Author{:id 769268, :first-name nil, :last-name nil, :full-name "Floriana Esposito"} #search_api.search_api.Author{:id 1126564, :first-name nil, :last-name nil, :full-name "Stefano Ferilli"} #search_api.search_api.Author{:id 1078358, :first-name nil, :last-name nil, :full-name "Teresa Maria Altomare Basile"} #search_api.search_api.Author{:id 1294122, :first-name nil, :last-name nil, :full-name "Nicola Di Mauro"}), :year 2012, :venue "IJSNM", :ncit 0, :string "Social networks and statistical relational learning: a survey.. ", :doc-id "Social networks and statistical relational learning: a survey. 2012 Floriana Esposito, Stefano Ferilli, Teresa Maria Altomare Basile, Nicola Di Mauro"}, 1009524 #search_api.search_api.Paper{:id 1009524, :key "journals/ml/HeckermanGC95", :title "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.", :abstract "We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :author (#search_api.search_api.Author{:id 114083, :first-name nil, :last-name nil, :full-name "David Heckerman"} #search_api.search_api.Author{:id 330355, :first-name nil, :last-name nil, :full-name "Dan Geiger"} #search_api.search_api.Author{:id 1215050, :first-name nil, :last-name nil, :full-name "David Maxwell Chickering"}), :year 1995, :venue "Machine Learning", :ncit 3837, :string "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.. We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :doc-id "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data. 1995 David Heckerman, Dan Geiger, David Maxwell Chickering"}, 192565 #search_api.search_api.Paper{:id 192565, :key "conf/eurocolt/ShamirS95", :title "Learning by extended statistical queries and its relation to PAC learning.", :abstract nil, :author (#search_api.search_api.Author{:id 1191060, :first-name nil, :last-name nil, :full-name "Eli Shamir"} #search_api.search_api.Author{:id 690500, :first-name nil, :last-name nil, :full-name "Clara Shwartzman"}), :year 1995, :venue "EuroCOLT", :ncit 8, :string "Learning by extended statistical queries and its relation to PAC learning.. ", :doc-id "Learning by extended statistical queries and its relation to PAC learning. 1995 Eli Shamir, Clara Shwartzman"}, 1236661 #search_api.search_api.Paper{:id 1236661, :key "conf/RelMiCS/Muller09", :title "Modalities, Relations, and Learning.", :abstract "While the popularity of statistical, probabilistic and exhaustive machine learning techniques still increases, relational and logic approaches are still a niche market in research. While the former approaches focus on predictive accuracy, the latter ones prove to be indispensable in knowledge discovery.In this paper we present a relational description of machine learning problems. We demonstrate how common ensemble learning methods as used in classifier learning can be reformulated in a relational setting. It is shown that multimodal logics and relational data analysis with rough sets are closely related. Finally, we give an interpretation of logic programs as approximations of hypotheses.It is demonstrated that at a certain level of abstraction all these methods unify into one and the same formalisation which nicely connects to multimodal operators. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :author (#search_api.search_api.Author{:id 1161107, :first-name nil, :last-name nil, :full-name "Martin Eric Müller"}), :year 2009, :venue "RelMiCS", :ncit 0, :string "Modalities, Relations, and Learning.. While the popularity of statistical, probabilistic and exhaustive machine learning techniques still increases, relational and logic approaches are still a niche market in research. While the former approaches focus on predictive accuracy, the latter ones prove to be indispensable in knowledge discovery.In this paper we present a relational description of machine learning problems. We demonstrate how common ensemble learning methods as used in classifier learning can be reformulated in a relational setting. It is shown that multimodal logics and relational data analysis with rough sets are closely related. Finally, we give an interpretation of logic programs as approximations of hypotheses.It is demonstrated that at a certain level of abstraction all these methods unify into one and the same formalisation which nicely connects to multimodal operators. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :doc-id "Modalities, Relations, and Learning. 2009 Martin Eric Müller"}, 302837 #search_api.search_api.Paper{:id 302837, :key "conf/icdm/PopesculULP03", :title "Statistical Relational Learning for Document Mining.", :abstract "A major obstacle to fully integrated deployment of manydata mining algorithms is the assumption that data sitsin a single table, even though most real-world databaseshave complex relational structures. We propose an integratedapproach to statistical modeling from relationaldatabases. We structure the search space based on \"refinementgraphs\", which are widely used in inductive logic programmingfor learning logic descriptions. The use of statisticsallows us to extend the search space to include richerset of features, including many which are not boolean.Search and model selection are integrated into a single process,allowing information criteria native to the statisticalmodel, for example logistic regression, to make feature selectiondecisions in a step-wise manner. We present experimentalresults for the task of predicting where scientific paperswill be published based on relational data taken fromCiteSeer. Our approach results in classification accuraciessuperior to those achieved when using classical \"flat\" features.The resulting classifier can be used to recommendwhere to publish articles.", :author (#search_api.search_api.Author{:id 597076, :first-name nil, :last-name nil, :full-name "Alexandrin Popescul"} #search_api.search_api.Author{:id 352656, :first-name nil, :last-name nil, :full-name "Lyle H. Ungar"} #search_api.search_api.Author{:id 599072, :first-name nil, :last-name nil, :full-name "Steve Lawrence"} #search_api.search_api.Author{:id 80443, :first-name nil, :last-name nil, :full-name "David M. Pennock"}), :year 2003, :venue "ICDM", :ncit 51, :string "Statistical Relational Learning for Document Mining.. A major obstacle to fully integrated deployment of manydata mining algorithms is the assumption that data sitsin a single table, even though most real-world databaseshave complex relational structures. We propose an integratedapproach to statistical modeling from relationaldatabases. We structure the search space based on \"refinementgraphs\", which are widely used in inductive logic programmingfor learning logic descriptions. The use of statisticsallows us to extend the search space to include richerset of features, including many which are not boolean.Search and model selection are integrated into a single process,allowing information criteria native to the statisticalmodel, for example logistic regression, to make feature selectiondecisions in a step-wise manner. We present experimentalresults for the task of predicting where scientific paperswill be published based on relational data taken fromCiteSeer. Our approach results in classification accuraciessuperior to those achieved when using classical \"flat\" features.The resulting classifier can be used to recommendwhere to publish articles.", :doc-id "Statistical Relational Learning for Document Mining. 2003 Alexandrin Popescul, Lyle H. Ungar, Steve Lawrence, David M. Pennock"}, 1032117 #search_api.search_api.Paper{:id 1032117, :key "journals/pami/KonishiYCZ03", :title "Statistical Edge Detection: Learning and Evaluating Edge Cues.", :abstract "We formulate edge detection as statistical inference. This statistical edge detection is data driven, unlike standard methods for edge detection which are model based. For any set of edge detection filters (implementing local edge cues), we use presegmented images to learn the probability distributions of filter responses conditioned on whether they are evaluated on or off an edge. Edge detection is formulated as a discrimination task specified by a likelihood ratio test on the filter responses. This approach emphasizes the necessity of modeling the image background (the off-edges). We represent the conditional probability distributions nonparametrically and illustrate them on two different data sets of 100 (Sowerby) and 50 (South Florida) images. Multiple edges cues, including chrominance and multiple-scale, are combined by using their joint distributions. Hence, this cue combination is optimal in the statistical sense. We evaluate the effectiveness of different visual cues using the Chernoff information and Receiver Operator Characteristic (ROC) curves. This shows that our approach gives quantitatively better results than the Canny edge detector when the image background contains significant clutter. In addition, it enables us to determine the effectiveness of different edge cues and gives quantitative measures for the advantages of multilevel processing, for the use of chrominance, and for the relative effectiveness of different detectors. Furthermore, we show that we can learn these conditional distributions on one data set and adapt them to the other with only slight degradation of performance without knowing the ground truth on the second data set. This shows that our results are not purely domain specific. We apply the same approach to the spatial grouping of edge cues and obtain analogies to nonmaximal suppression and hysteresis.", :author (#search_api.search_api.Author{:id 696497, :first-name nil, :last-name nil, :full-name "Scott Konishi"} #search_api.search_api.Author{:id 248255, :first-name nil, :last-name nil, :full-name "Alan L. Yuille"} #search_api.search_api.Author{:id 589169, :first-name nil, :last-name nil, :full-name "James M. Coughlan"} #search_api.search_api.Author{:id 589996, :first-name nil, :last-name nil, :full-name "Song Chun Zhu"}), :year 2003, :venue "IEEE Trans. Pattern Anal. Mach. Intell.", :ncit 187, :string "Statistical Edge Detection: Learning and Evaluating Edge Cues.. We formulate edge detection as statistical inference. This statistical edge detection is data driven, unlike standard methods for edge detection which are model based. For any set of edge detection filters (implementing local edge cues), we use presegmented images to learn the probability distributions of filter responses conditioned on whether they are evaluated on or off an edge. Edge detection is formulated as a discrimination task specified by a likelihood ratio test on the filter responses. This approach emphasizes the necessity of modeling the image background (the off-edges). We represent the conditional probability distributions nonparametrically and illustrate them on two different data sets of 100 (Sowerby) and 50 (South Florida) images. Multiple edges cues, including chrominance and multiple-scale, are combined by using their joint distributions. Hence, this cue combination is optimal in the statistical sense. We evaluate the effectiveness of different visual cues using the Chernoff information and Receiver Operator Characteristic (ROC) curves. This shows that our approach gives quantitatively better results than the Canny edge detector when the image background contains significant clutter. In addition, it enables us to determine the effectiveness of different edge cues and gives quantitative measures for the advantages of multilevel processing, for the use of chrominance, and for the relative effectiveness of different detectors. Furthermore, we show that we can learn these conditional distributions on one data set and adapt them to the other with only slight degradation of performance without knowing the ground truth on the second data set. This shows that our results are not purely domain specific. We apply the same approach to the spatial grouping of edge cues and obtain analogies to nonmaximal suppression and hysteresis.", :doc-id "Statistical Edge Detection: Learning and Evaluating Edge Cues. 2003 Scott Konishi, Alan L. Yuille, James M. Coughlan, Song Chun Zhu"}, 574389 #search_api.search_api.Paper{:id 574389, :key "conf/sbia/Raedt08a", :title "Logic, Probability and Learning, or an Introduction to Statistical Relational Learning.", :abstract "Probabilistic inductive logic programming (PILP), sometimes also called statistical relational learning, addresses one of the central questions of artificial intelligence: the integration of probabilistic reasoning with first order logic representations and machine learning. A rich variety of different formalisms and learning techniques have been developed and they are being applied on applications in network analysis, robotics, bio-informatics, intelligent agents, etc. This tutorial starts with an introduction to probabilistic representations and machine learning, and then continues with an overview of the state-of-the-art in statistical relational learning. We start from classical settings for logic learning (or inductive logic programming) namely learning from entailment, learning from interpretations, and learning from proofs, and show how they can be extended with probabilistic methods. While doing so, we review state-of-the-art statistical relational learning approaches and show how they fit the discussed learning settings for probabilistic inductive logic programming.", :author (#search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2008, :venue "SBIA", :ncit 0, :string "Logic, Probability and Learning, or an Introduction to Statistical Relational Learning.. Probabilistic inductive logic programming (PILP), sometimes also called statistical relational learning, addresses one of the central questions of artificial intelligence: the integration of probabilistic reasoning with first order logic representations and machine learning. A rich variety of different formalisms and learning techniques have been developed and they are being applied on applications in network analysis, robotics, bio-informatics, intelligent agents, etc. This tutorial starts with an introduction to probabilistic representations and machine learning, and then continues with an overview of the state-of-the-art in statistical relational learning. We start from classical settings for logic learning (or inductive logic programming) namely learning from entailment, learning from interpretations, and learning from proofs, and show how they can be extended with probabilistic methods. While doing so, we review state-of-the-art statistical relational learning approaches and show how they fit the discussed learning settings for probabilistic inductive logic programming.", :doc-id "Logic, Probability and Learning, or an Introduction to Statistical Relational Learning. 2008 Luc De Raedt"}, 57463 #search_api.search_api.Paper{:id 57463, :key "conf/atal/RettingerNT08", :title "A statistical relational model for trust learning.", :abstract "We address the learning of trust based on past observations and context information. We argue that from the truster's point of view trust is best expressed as one of several relations that exist between the agent to be trusted (trustee) and the state of the environment. Besides attributes expressing trustworthiness, additional relations might describe commitments made by the trustee with regard to the current situation, like: a seller offers a certain price for a specific product. We show how to implement and learn contextsensitive trust using statistical relational learning in form of the Infinite Hidden Relational Trust Model (IHRTM). The practicability and effectiveness of our approach is evaluated empirically on user-ratings gathered from eBay. Our results suggest that (i) the inherent clustering achieved in the algorithm allows the truster to characterize the structure of a trust-situation and provides meaningful trust assessments; (ii) utilizing the collaborative filtering effect associated with relational data does improve trust assessment performance; (iii) by learning faster and transferring knowledge more effectively we improve cold start performance and can cope better with dynamic behavior in open multiagent systems. The later is demonstrated with interactions recorded from a strategic two-player negotiation scenario.", :author (#search_api.search_api.Author{:id 444383, :first-name nil, :last-name nil, :full-name "Achim Rettinger"} #search_api.search_api.Author{:id 222492, :first-name nil, :last-name nil, :full-name "Matthias Nickles"} #search_api.search_api.Author{:id 680469, :first-name nil, :last-name nil, :full-name "Volker Tresp"}), :year 2008, :venue "AAMAS (2)", :ncit 14, :string "A statistical relational model for trust learning.. We address the learning of trust based on past observations and context information. We argue that from the truster's point of view trust is best expressed as one of several relations that exist between the agent to be trusted (trustee) and the state of the environment. Besides attributes expressing trustworthiness, additional relations might describe commitments made by the trustee with regard to the current situation, like: a seller offers a certain price for a specific product. We show how to implement and learn contextsensitive trust using statistical relational learning in form of the Infinite Hidden Relational Trust Model (IHRTM). The practicability and effectiveness of our approach is evaluated empirically on user-ratings gathered from eBay. Our results suggest that (i) the inherent clustering achieved in the algorithm allows the truster to characterize the structure of a trust-situation and provides meaningful trust assessments; (ii) utilizing the collaborative filtering effect associated with relational data does improve trust assessment performance; (iii) by learning faster and transferring knowledge more effectively we improve cold start performance and can cope better with dynamic behavior in open multiagent systems. The later is demonstrated with interactions recorded from a strategic two-player negotiation scenario.", :doc-id "A statistical relational model for trust learning. 2008 Achim Rettinger, Matthias Nickles, Volker Tresp"}, 746711 #search_api.search_api.Paper{:id 746711, :key "journals/aicom/Kersting06", :title "An inductive logic programming approach to statistical relational learning.", :abstract "It has been a great pleasure to be asked to write the preface for the book based on Kristian Kersting's thesis. There is no doubt in my mind that this is a remarkable and outstanding piece of work. In his thesis Kristian has made an assault on one of the hardest integration problems at the heart of Artificial Intelligence research. This involves taking three disparate major areas of research and attempting a fusion among them. The three areas are: Logic Programming, Uncertainty Reasoning and Machine Learning. Every one of these is a major sub-area of research with its own associated international research conferences. Having taken on such a Herculean task, Kristian has produced a series of widely published results which are now at the core of a newly emerging area: Probabilistic Inductive Logic Programming. The new area is closely tied to, though strictly subsumes, a new field known as “Statistical Relational Learning” which has in the last few years gained major prominence in the American Artificial Intelligence research community. Within his thesis Kristian makes several major contributions, many of which have already been published in refereed conference and journal papers. Firstly, Kristian introduces a series of definitions which circumscribe the new area formed by extending Inductive Logic Programming to the case in which clauses are annotated with probability values. This represents a new and powerful framework which supersedes a number of influential papers and research areas in Artificial Intelligence. Secondly, Kristian introduces Bayesian Logic Programs (BLPs). These represent an elegantly defined lifting of Judea Pearl's Bayesian networks to the logic programming level. Since Kristian's introduction of BLPs, a number of results indicate that BLPs generalise many previously defined representations, not the least of which are Bayesian networks, Logic Programs, Probabilistic Relation Models and Stochastic Logic Programs. Next Kristian investigates the approach of Learning from proofs. This is an interesting new learning framework which is the first to go beyond the two standard semantic frameworks of Inductive Logic Programming. Kristian then looks at the problem of upgrading HMMs to logical HMMs. Hidden Markov Models (HMMs) are one of the most widely used machine learning technologies in Statistical Linguistics and Bioinformatics, and allow the representation of probabilistic finite automata. Kristian has upgraded standard HMMs to allow relational descriptions to be included within the description of the automata. The three standard HMM estimation algorithms are also upgraded. He has demonstrated the power of such representations using biological predictive modelling problems, and shown performance increases over alternative approaches. Kristian next considers the issue of upgrading Fisher Kernels to Relational Fisher kernels. Fisher kernels have been widely used within statistics and more recently in support vector machines. Buildin...", :author (#search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"}), :year 2006, :venue "AI Commun.", :ncit 15, :string "An inductive logic programming approach to statistical relational learning.. It has been a great pleasure to be asked to write the preface for the book based on Kristian Kersting's thesis. There is no doubt in my mind that this is a remarkable and outstanding piece of work. In his thesis Kristian has made an assault on one of the hardest integration problems at the heart of Artificial Intelligence research. This involves taking three disparate major areas of research and attempting a fusion among them. The three areas are: Logic Programming, Uncertainty Reasoning and Machine Learning. Every one of these is a major sub-area of research with its own associated international research conferences. Having taken on such a Herculean task, Kristian has produced a series of widely published results which are now at the core of a newly emerging area: Probabilistic Inductive Logic Programming. The new area is closely tied to, though strictly subsumes, a new field known as “Statistical Relational Learning” which has in the last few years gained major prominence in the American Artificial Intelligence research community. Within his thesis Kristian makes several major contributions, many of which have already been published in refereed conference and journal papers. Firstly, Kristian introduces a series of definitions which circumscribe the new area formed by extending Inductive Logic Programming to the case in which clauses are annotated with probability values. This represents a new and powerful framework which supersedes a number of influential papers and research areas in Artificial Intelligence. Secondly, Kristian introduces Bayesian Logic Programs (BLPs). These represent an elegantly defined lifting of Judea Pearl's Bayesian networks to the logic programming level. Since Kristian's introduction of BLPs, a number of results indicate that BLPs generalise many previously defined representations, not the least of which are Bayesian networks, Logic Programs, Probabilistic Relation Models and Stochastic Logic Programs. Next Kristian investigates the approach of Learning from proofs. This is an interesting new learning framework which is the first to go beyond the two standard semantic frameworks of Inductive Logic Programming. Kristian then looks at the problem of upgrading HMMs to logical HMMs. Hidden Markov Models (HMMs) are one of the most widely used machine learning technologies in Statistical Linguistics and Bioinformatics, and allow the representation of probabilistic finite automata. Kristian has upgraded standard HMMs to allow relational descriptions to be included within the description of the automata. The three standard HMM estimation algorithms are also upgraded. He has demonstrated the power of such representations using biological predictive modelling problems, and shown performance increases over alternative approaches. Kristian next considers the issue of upgrading Fisher Kernels to Relational Fisher kernels. Fisher kernels have been widely used within statistics and more recently in support vector machines. Buildin...", :doc-id "An inductive logic programming approach to statistical relational learning. 2006 Kristian Kersting"}, 388504 #search_api.search_api.Paper{:id 388504, :key "conf/ijcai/FriedmanGKP99", :title "Learning Probabilistic Relational Models.", :abstract "A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with \"flat\" data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases.", :author (#search_api.search_api.Author{:id 61431, :first-name nil, :last-name nil, :full-name "Nir Friedman"} #search_api.search_api.Author{:id 1449720, :first-name nil, :last-name nil, :full-name "Lise Getoor"} #search_api.search_api.Author{:id 560995, :first-name nil, :last-name nil, :full-name "Daphne Koller"} #search_api.search_api.Author{:id 154856, :first-name nil, :last-name nil, :full-name "Avi Pfeffer"}), :year 1999, :venue "IJCAI", :ncit 843, :string "Learning Probabilistic Relational Models.. A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with \"flat\" data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases.", :doc-id "Learning Probabilistic Relational Models. 1999 Nir Friedman, Lise Getoor, Daphne Koller, Avi Pfeffer"}, 644121 #search_api.search_api.Paper{:id 644121, :key "conf/vldb/ZhangHJLZ05", :title "Statistical Learning Techniques for Costing XML Queries.", :abstract "Developing cost models for query optimization is significantly harder for XML queries than for traditional relational queries. The reason is that XML query operators are much more complex than relational operators such as table scans and joins. In this paper, we propose a new approach, called COMET, to modeling the cost of XML operators; to our knowledge, COMET is the first method ever proposed for addressing the XML query costing problem. As in relational cost estimation, COMET exploits a set of system catalog statistics that summarizes the XML data; the set of \"simple path\" statistics that we propose is new, and is well suited to the XML setting. Unlike the traditional approach, COMET uses a new statistical learning technique called \"transform regression\" instead of detailed analytical models to predict the overall cost. Besides rendering the cost estimation problem tractable for XML queries, COMET has the further advantage of enabling the query optimizer to be self-tuning, automatically adapting to changes over time in the query workload and in the system environment. We demonstrate COMET's feasibility by developing a cost model for the recently proposed XNAV navigational operator. Empirical studies with synthetic, benchmark, and real-world data sets show that COMET can quickly obtain accurate cost estimates for a variety of XML queries and data sets.", :author (#search_api.search_api.Author{:id 39154, :first-name nil, :last-name nil, :full-name "Ning Zhang"} #search_api.search_api.Author{:id 832799, :first-name nil, :last-name nil, :full-name "Peter J. Haas"} #search_api.search_api.Author{:id 875466, :first-name nil, :last-name nil, :full-name "Vanja Josifovski"} #search_api.search_api.Author{:id 119924, :first-name nil, :last-name nil, :full-name "Guy M. Lohman"} #search_api.search_api.Author{:id 691076, :first-name nil, :last-name nil, :full-name "Chun Zhang"}), :year 2005, :venue "VLDB", :ncit 56, :string "Statistical Learning Techniques for Costing XML Queries.. Developing cost models for query optimization is significantly harder for XML queries than for traditional relational queries. The reason is that XML query operators are much more complex than relational operators such as table scans and joins. In this paper, we propose a new approach, called COMET, to modeling the cost of XML operators; to our knowledge, COMET is the first method ever proposed for addressing the XML query costing problem. As in relational cost estimation, COMET exploits a set of system catalog statistics that summarizes the XML data; the set of \"simple path\" statistics that we propose is new, and is well suited to the XML setting. Unlike the traditional approach, COMET uses a new statistical learning technique called \"transform regression\" instead of detailed analytical models to predict the overall cost. Besides rendering the cost estimation problem tractable for XML queries, COMET has the further advantage of enabling the query optimizer to be self-tuning, automatically adapting to changes over time in the query workload and in the system environment. We demonstrate COMET's feasibility by developing a cost model for the recently proposed XNAV navigational operator. Empirical studies with synthetic, benchmark, and real-world data sets show that COMET can quickly obtain accurate cost estimates for a variety of XML queries and data sets.", :doc-id "Statistical Learning Techniques for Costing XML Queries. 2005 Ning Zhang, Peter J. Haas, Vanja Josifovski, Guy M. Lohman, Chun Zhang"}, 1250938 #search_api.search_api.Paper{:id 1250938, :key "conf/ismis/BibaFE09", :title "Efficient MAP Inference for Statistical Relational Models through Hybrid Metaheuristics.", :abstract "Statistical Relational Models are state-of-the-art representation formalisms at the intersection of logical and statistical machine learning. One of the most promising models is Markov Logic (ML) which combines Markov networks (MNs) and first-order logic by attaching weights to first-order formulas and using these as templates for features of MNs. MAP inference in ML is the task of finding the most likely state of a set of output variables given the state of the input variables and this problem is NP-hard. In this paper we present an algorithm for this inference task based on the Iterated Local Search (ILS) and Robust Tabu Search (RoTS) metaheuristics. The algorithm performs a biased sampling of the set of local optima by using RoTS as a local search procedure and repetitively jumping in the search space through a perturbation operator, focusing the search not on the full space of solutions but on a smaller subspace defined by the solutions that are locally optimal for the optimization engine. We show through extensive experiments in real-world domains that it improves over the state-of-the-art algorithm in terms of solution quality and inference time.", :author (#search_api.search_api.Author{:id 1482903, :first-name nil, :last-name nil, :full-name "Marenglen Biba"} #search_api.search_api.Author{:id 1126564, :first-name nil, :last-name nil, :full-name "Stefano Ferilli"} #search_api.search_api.Author{:id 769268, :first-name nil, :last-name nil, :full-name "Floriana Esposito"}), :year 2009, :venue "ISMIS", :ncit 0, :string "Efficient MAP Inference for Statistical Relational Models through Hybrid Metaheuristics.. Statistical Relational Models are state-of-the-art representation formalisms at the intersection of logical and statistical machine learning. One of the most promising models is Markov Logic (ML) which combines Markov networks (MNs) and first-order logic by attaching weights to first-order formulas and using these as templates for features of MNs. MAP inference in ML is the task of finding the most likely state of a set of output variables given the state of the input variables and this problem is NP-hard. In this paper we present an algorithm for this inference task based on the Iterated Local Search (ILS) and Robust Tabu Search (RoTS) metaheuristics. The algorithm performs a biased sampling of the set of local optima by using RoTS as a local search procedure and repetitively jumping in the search space through a perturbation operator, focusing the search not on the full space of solutions but on a smaller subspace defined by the solutions that are locally optimal for the optimization engine. We show through extensive experiments in real-world domains that it improves over the state-of-the-art algorithm in terms of solution quality and inference time.", :doc-id "Efficient MAP Inference for Statistical Relational Models through Hybrid Metaheuristics. 2009 Marenglen Biba, Stefano Ferilli, Floriana Esposito"}, 294586 #search_api.search_api.Paper{:id 294586, :key "conf/icdar/CeciBM05", :title "Relational Learning techniques for Document Image Understanding: Comparing Statistical and Logical approaches.", :abstract "In this paper, we evaluate and systematically compare two different (multi-)relational learning methods based on a statistical approach and a logical approach for the task of document image understanding. For a fair comparison, both methods are tested on the same real world dataset consisting of multipage articles published in an international journal. An analysis of pros and cons of both approaches is reported.", :author (#search_api.search_api.Author{:id 1294148, :first-name nil, :last-name nil, :full-name "Michelangelo Ceci"} #search_api.search_api.Author{:id 1385113, :first-name nil, :last-name nil, :full-name "Margherita Berardi"} #search_api.search_api.Author{:id 282116, :first-name nil, :last-name nil, :full-name "Donato Malerba"}), :year 2005, :venue "ICDAR", :ncit 3, :string "Relational Learning techniques for Document Image Understanding: Comparing Statistical and Logical approaches.. In this paper, we evaluate and systematically compare two different (multi-)relational learning methods based on a statistical approach and a logical approach for the task of document image understanding. For a fair comparison, both methods are tested on the same real world dataset consisting of multipage articles published in an international journal. An analysis of pros and cons of both approaches is reported.", :doc-id "Relational Learning techniques for Document Image Understanding: Comparing Statistical and Logical approaches. 2005 Michelangelo Ceci, Margherita Berardi, Donato Malerba"}, 1026778 #search_api.search_api.Paper{:id 1026778, :key "journals/nn/Linsker05", :title "Improved local learning rule for information maximization and related applications.", :abstract "For a neural network comprising feedforward and lateral connections, a local learning rule is proposed that causes the lateral connections to learn directly the inverse of a covariance matrix. In contrast to earlier work, the rule involves just one processing pass through the lateral connections for each input presentation, and consists of a simple anti-Hebbian term. This provides an effective and simple method for online network learning algorithms that implement optimization principles, drawn from statistics or from information or control theory, for which a running estimate of the covariance matrix inverse is useful. An application to infomax learning (mutual information maximization) in the presence of input and output noise is used to illustrate the method.", :author (#search_api.search_api.Author{:id 321062, :first-name nil, :last-name nil, :full-name "Ralph Linsker"}), :year 2005, :venue "Neural Networks", :ncit 19, :string "Improved local learning rule for information maximization and related applications.. For a neural network comprising feedforward and lateral connections, a local learning rule is proposed that causes the lateral connections to learn directly the inverse of a covariance matrix. In contrast to earlier work, the rule involves just one processing pass through the lateral connections for each input presentation, and consists of a simple anti-Hebbian term. This provides an effective and simple method for online network learning algorithms that implement optimization principles, drawn from statistics or from information or control theory, for which a running estimate of the covariance matrix inverse is useful. An application to infomax learning (mutual information maximization) in the presence of input and output noise is used to illustrate the method.", :doc-id "Improved local learning rule for information maximization and related applications. 2005 Ralph Linsker"}, 3341018 #search_api.search_api.Paper{:id 3341018, :key "phd/de/Kersting2006", :title "An inductive logic programming approach to statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"}), :year 2006, :venue nil, :ncit 15, :string "An inductive logic programming approach to statistical relational learning.. ", :doc-id "An inductive logic programming approach to statistical relational learning. 2006 Kristian Kersting"}, 3402586 #search_api.search_api.Paper{:id 3402586, :key "series/faia/2005-148", :title "An Inductive Logic Programming Approach to Statistical Relational Learning", :abstract nil, :author (#search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"}), :year 2005, :venue "Frontiers in Artificial Intelligence and Applications", :ncit 15, :string "An Inductive Logic Programming Approach to Statistical Relational Learning. ", :doc-id "An Inductive Logic Programming Approach to Statistical Relational Learning 2005 Kristian Kersting"}, 937563 #search_api.search_api.Paper{:id 937563, :key "journals/ir/Yang99", :title "An Evaluation of Statistical Approaches to Text Categorization.", :abstract "This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance&semi; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.", :author (#search_api.search_api.Author{:id 1066064, :first-name nil, :last-name nil, :full-name "Yiming Yang"}), :year 1999, :venue "Inf. Retr.", :ncit 1871, :string "An Evaluation of Statistical Approaches to Text Categorization.. This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance&semi; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.", :doc-id "An Evaluation of Statistical Approaches to Text Categorization. 1999 Yiming Yang"}, 391548 #search_api.search_api.Paper{:id 391548, :key "conf/ijcai/DavisOSBPC07", :title "Change of Representation for Statistical Relational Learning.", :abstract "Statistical relational learning (SRL) algorithms learn statistical models from relational data, such as that stored in a relational database. We previously introduced view learning for SRL, in which the view of a relational database can be automatically modified, yielding more accurate statistical models. The present paper presents SAYU-VISTA, an algorithm which advances beyond the initial view learning approach in three ways. First, it learns views that introduce new relational tables, rather than merely new fields for an existing table of the database. Second, new tables or new fields are not limited to being approximations to some target concept; instead, the new approach performs a type of predicate invention. The new approach avoids the classical problem with predicate invention, of learning many useless predicates, by keeping only new fields or tables (i.e., new predicates) that immediately improve the performance of the statistical model. Third, retained fields or tables can then be used in the definitions of further new fields or tables. We evaluate the new view learning approach on three relational classification tasks.", :author (#search_api.search_api.Author{:id 747351, :first-name nil, :last-name nil, :full-name "Jesse Davis"} #search_api.search_api.Author{:id 629180, :first-name nil, :last-name nil, :full-name "Irene M. Ong"} #search_api.search_api.Author{:id 1227127, :first-name nil, :last-name nil, :full-name "Jan Struyf"} #search_api.search_api.Author{:id 750446, :first-name nil, :last-name nil, :full-name "Elizabeth S. Burnside"} #search_api.search_api.Author{:id 138183, :first-name nil, :last-name nil, :full-name "David Page"} #search_api.search_api.Author{:id 1352436, :first-name nil, :last-name nil, :full-name "Vítor Santos Costa"}), :year 2007, :venue "IJCAI", :ncit 28, :string "Change of Representation for Statistical Relational Learning.. Statistical relational learning (SRL) algorithms learn statistical models from relational data, such as that stored in a relational database. We previously introduced view learning for SRL, in which the view of a relational database can be automatically modified, yielding more accurate statistical models. The present paper presents SAYU-VISTA, an algorithm which advances beyond the initial view learning approach in three ways. First, it learns views that introduce new relational tables, rather than merely new fields for an existing table of the database. Second, new tables or new fields are not limited to being approximations to some target concept; instead, the new approach performs a type of predicate invention. The new approach avoids the classical problem with predicate invention, of learning many useless predicates, by keeping only new fields or tables (i.e., new predicates) that immediately improve the performance of the statistical model. Third, retained fields or tables can then be used in the definitions of further new fields or tables. We evaluate the new view learning approach on three relational classification tasks.", :doc-id "Change of Representation for Statistical Relational Learning. 2007 Jesse Davis, Irene M. Ong, Jan Struyf, Elizabeth S. Burnside, David Page, Vítor Santos Costa"}, 271964 #search_api.search_api.Paper{:id 271964, :key "conf/icann/KopeczM97", :title "Relative Time Scales in the Self-Organization of Pattern Classification: From One-Shot to Statistical Learning.", :abstract nil, :author (#search_api.search_api.Author{:id 840288, :first-name nil, :last-name nil, :full-name "Klaus Kopecz"} #search_api.search_api.Author{:id 379366, :first-name nil, :last-name nil, :full-name "Karim Mohraz"}), :year 1997, :venue "ICANN", :ncit 0, :string "Relative Time Scales in the Self-Organization of Pattern Classification: From One-Shot to Statistical Learning.. ", :doc-id "Relative Time Scales in the Self-Organization of Pattern Classification: From One-Shot to Statistical Learning. 1997 Klaus Kopecz, Karim Mohraz"}, 1009341 #search_api.search_api.Paper{:id 1009341, :key "journals/ml/BlockeelJK06", :title "Introduction to the special issue on multi-relational data mining and statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 882341, :first-name nil, :last-name nil, :full-name "Hendrik Blockeel"} #search_api.search_api.Author{:id 408384, :first-name nil, :last-name nil, :full-name "David Jensen"} #search_api.search_api.Author{:id 954022, :first-name nil, :last-name nil, :full-name "Stefan Kramer"}), :year 2006, :venue "Machine Learning", :ncit 2, :string "Introduction to the special issue on multi-relational data mining and statistical relational learning.. ", :doc-id "Introduction to the special issue on multi-relational data mining and statistical relational learning. 2006 Hendrik Blockeel, David Jensen, Stefan Kramer"}, 622461 #search_api.search_api.Paper{:id 622461, :key "conf/stoc/Kearns93", :title "Efficient noise-tolerant learning from statistical queries.", :abstract "In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :author (#search_api.search_api.Author{:id 1249307, :first-name nil, :last-name nil, :full-name "Michael J. Kearns"}), :year 1993, :venue "STOC", :ncit 437, :string "Efficient noise-tolerant learning from statistical queries.. In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :doc-id "Efficient noise-tolerant learning from statistical queries. 1993 Michael J. Kearns"}, 3782751 #search_api.search_api.Paper{:id 3782751, :key "conf/ilp/SahaSR12", :title "What Kinds of Relational Features Are Useful for Statistical Learning?", :abstract nil, :author (#search_api.search_api.Author{:id 914315, :first-name nil, :last-name nil, :full-name "Amrita Saha"} #search_api.search_api.Author{:id 1118213, :first-name nil, :last-name nil, :full-name "Ashwin Srinivasan"} #search_api.search_api.Author{:id 376403, :first-name nil, :last-name nil, :full-name "Ganesh Ramakrishnan"}), :year 2012, :venue "ILP", :ncit 0, :string "What Kinds of Relational Features Are Useful for Statistical Learning?. ", :doc-id "What Kinds of Relational Features Are Useful for Statistical Learning? 2012 Amrita Saha, Ashwin Srinivasan, Ganesh Ramakrishnan"}}}}, :interface :search, :results #utils.core.Success{:value (#search_api.search_api.Paper{:id 3074924, :key "journals/ml/RettingerNT11", :title "Statistical relational learning of trust.", :abstract nil, :author (#search_api.search_api.Author{:id 444383, :first-name nil, :last-name nil, :full-name "Achim Rettinger"} #search_api.search_api.Author{:id 222492, :first-name nil, :last-name nil, :full-name "Matthias Nickles"} #search_api.search_api.Author{:id 680469, :first-name nil, :last-name nil, :full-name "Volker Tresp"}), :year 2011, :venue "Machine Learning", :ncit 6, :string "Statistical relational learning of trust.. ", :doc-id "Statistical relational learning of trust. 2011 Achim Rettinger, Matthias Nickles, Volker Tresp"} #search_api.search_api.Paper{:id 1009524, :key "journals/ml/HeckermanGC95", :title "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.", :abstract "We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :author (#search_api.search_api.Author{:id 114083, :first-name nil, :last-name nil, :full-name "David Heckerman"} #search_api.search_api.Author{:id 330355, :first-name nil, :last-name nil, :full-name "Dan Geiger"} #search_api.search_api.Author{:id 1215050, :first-name nil, :last-name nil, :full-name "David Maxwell Chickering"}), :year 1995, :venue "Machine Learning", :ncit 3837, :string "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.. We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :doc-id "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data. 1995 David Heckerman, Dan Geiger, David Maxwell Chickering"} #search_api.search_api.Paper{:id 472783, :key "conf/kdd/HeckermanGC94", :title "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.", :abstract "We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :author (#search_api.search_api.Author{:id 114083, :first-name nil, :last-name nil, :full-name "David Heckerman"} #search_api.search_api.Author{:id 330355, :first-name nil, :last-name nil, :full-name "Dan Geiger"} #search_api.search_api.Author{:id 1215050, :first-name nil, :last-name nil, :full-name "David Maxwell Chickering"}), :year 1994, :venue "KDD Workshop", :ncit 3837, :string "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data.. We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen&mdash;a prior network&mdash;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k &equals; 1 parent. For the general case (k > 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.", :doc-id "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data. 1994 David Heckerman, Dan Geiger, David Maxwell Chickering"} #search_api.search_api.Paper{:id 3258026, :key "journals/ml/RiguzziM12", :title "Applying the information bottleneck to statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 1316705, :first-name nil, :last-name nil, :full-name "Fabrizio Riguzzi"} #search_api.search_api.Author{:id 1294122, :first-name nil, :last-name nil, :full-name "Nicola Di Mauro"}), :year 2012, :venue "Machine Learning", :ncit 14, :string "Applying the information bottleneck to statistical relational learning.. ", :doc-id "Applying the information bottleneck to statistical relational learning. 2012 Fabrizio Riguzzi, Nicola Di Mauro"} #search_api.search_api.Paper{:id 3168190, :key "reference/ml/RaedtK10", :title "Statistical Relational Learning.", :abstract nil, :author (#search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"} #search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"}), :year 2010, :venue "Encyclopedia of Machine Learning", :ncit 1, :string "Statistical Relational Learning.. ", :doc-id "Statistical Relational Learning. 2010 Luc De Raedt, Kristian Kersting"} #search_api.search_api.Paper{:id 3551278, :key "journals/jair/RossiMAN12", :title "Transforming Graph Data for Statistical Relational Learning.", :abstract nil, :author (#search_api.search_api.Author{:id 1373085, :first-name nil, :last-name nil, :full-name "Ryan A. Rossi"} #search_api.search_api.Author{:id 724185, :first-name nil, :last-name nil, :full-name "Luke McDowell"} #search_api.search_api.Author{:id 523082, :first-name nil, :last-name nil, :full-name "David William Aha"} #search_api.search_api.Author{:id 150768, :first-name nil, :last-name nil, :full-name "Jennifer Neville"}), :year 2012, :venue "J. Artif. Intell. Res. (JAIR)", :ncit 5, :string "Transforming Graph Data for Statistical Relational Learning.. ", :doc-id "Transforming Graph Data for Statistical Relational Learning. 2012 Ryan A. Rossi, Luke McDowell, David William Aha, Jennifer Neville"} #search_api.search_api.Paper{:id 391548, :key "conf/ijcai/DavisOSBPC07", :title "Change of Representation for Statistical Relational Learning.", :abstract "Statistical relational learning (SRL) algorithms learn statistical models from relational data, such as that stored in a relational database. We previously introduced view learning for SRL, in which the view of a relational database can be automatically modified, yielding more accurate statistical models. The present paper presents SAYU-VISTA, an algorithm which advances beyond the initial view learning approach in three ways. First, it learns views that introduce new relational tables, rather than merely new fields for an existing table of the database. Second, new tables or new fields are not limited to being approximations to some target concept; instead, the new approach performs a type of predicate invention. The new approach avoids the classical problem with predicate invention, of learning many useless predicates, by keeping only new fields or tables (i.e., new predicates) that immediately improve the performance of the statistical model. Third, retained fields or tables can then be used in the definitions of further new fields or tables. We evaluate the new view learning approach on three relational classification tasks.", :author (#search_api.search_api.Author{:id 747351, :first-name nil, :last-name nil, :full-name "Jesse Davis"} #search_api.search_api.Author{:id 629180, :first-name nil, :last-name nil, :full-name "Irene M. Ong"} #search_api.search_api.Author{:id 1227127, :first-name nil, :last-name nil, :full-name "Jan Struyf"} #search_api.search_api.Author{:id 750446, :first-name nil, :last-name nil, :full-name "Elizabeth S. Burnside"} #search_api.search_api.Author{:id 138183, :first-name nil, :last-name nil, :full-name "David Page"} #search_api.search_api.Author{:id 1352436, :first-name nil, :last-name nil, :full-name "Vítor Santos Costa"}), :year 2007, :venue "IJCAI", :ncit 28, :string "Change of Representation for Statistical Relational Learning.. Statistical relational learning (SRL) algorithms learn statistical models from relational data, such as that stored in a relational database. We previously introduced view learning for SRL, in which the view of a relational database can be automatically modified, yielding more accurate statistical models. The present paper presents SAYU-VISTA, an algorithm which advances beyond the initial view learning approach in three ways. First, it learns views that introduce new relational tables, rather than merely new fields for an existing table of the database. Second, new tables or new fields are not limited to being approximations to some target concept; instead, the new approach performs a type of predicate invention. The new approach avoids the classical problem with predicate invention, of learning many useless predicates, by keeping only new fields or tables (i.e., new predicates) that immediately improve the performance of the statistical model. Third, retained fields or tables can then be used in the definitions of further new fields or tables. We evaluate the new view learning approach on three relational classification tasks.", :doc-id "Change of Representation for Statistical Relational Learning. 2007 Jesse Davis, Irene M. Ong, Jan Struyf, Elizabeth S. Burnside, David Page, Vítor Santos Costa"} #search_api.search_api.Paper{:id 3258022, :key "journals/ml/NatarajanKKGS12", :title "Gradient-based boosting for statistical relational learning: The relational dependency network case.", :abstract nil, :author (#search_api.search_api.Author{:id 883462, :first-name nil, :last-name nil, :full-name "Sriraam Natarajan"} #search_api.search_api.Author{:id 905516, :first-name nil, :last-name nil, :full-name "Tushar Khot"} #search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"} #search_api.search_api.Author{:id 1351938, :first-name nil, :last-name nil, :full-name "Bernd Gutmann"} #search_api.search_api.Author{:id 7, :first-name nil, :last-name nil, :full-name "Jude W. Shavlik"}), :year 2012, :venue "Machine Learning", :ncit 20, :string "Gradient-based boosting for statistical relational learning: The relational dependency network case.. ", :doc-id "Gradient-based boosting for statistical relational learning: The relational dependency network case. 2012 Sriraam Natarajan, Tushar Khot, Kristian Kersting, Bernd Gutmann, Jude W. Shavlik"} #search_api.search_api.Paper{:id 3027908, :key "conf/sigmod/GetoorM11", :title "Learning statistical models from relational data.", :abstract nil, :author (#search_api.search_api.Author{:id 1449720, :first-name nil, :last-name nil, :full-name "Lise Getoor"} #search_api.search_api.Author{:id 421571, :first-name nil, :last-name nil, :full-name "Lilyana Mihalkova"}), :year 2011, :venue "SIGMOD Conference", :ncit 87, :string "Learning statistical models from relational data.. ", :doc-id "Learning statistical models from relational data. 2011 Lise Getoor, Lilyana Mihalkova"} #search_api.search_api.Paper{:id 3382280, :key "journals/cogsci/ArciuliS12", :title "Statistical Learning Is Related to Reading Ability in Children and Adults.", :abstract nil, :author (#search_api.search_api.Author{:id 14233998, :first-name nil, :last-name nil, :full-name "Joanne Arciuli"} #search_api.search_api.Author{:id 14270159, :first-name nil, :last-name nil, :full-name "Ian C. Simpson"}), :year 2012, :venue "Cognitive Science", :ncit 0, :string "Statistical Learning Is Related to Reading Ability in Children and Adults.. ", :doc-id "Statistical Learning Is Related to Reading Ability in Children and Adults. 2012 Joanne Arciuli, Ian C. Simpson"} #search_api.search_api.Paper{:id 2775184, :key "conf/ai/KhosraviB10", :title "A Survey on Statistical Relational Learning.", :abstract "Frequent subgraph mining is an active research topic in the data mining community. A graph is a general model to represent data and has been used in many domains like cheminformatics and bioinformatics. Mining patterns from graph databases is challenging since graph related operations, such as subgraph testing, generally have higher time complexity than the corresponding operations on itemsets, sequences, and trees, which have been studied extensively. We propose a novel frequent subgraph mining algorithm: FFSM, which employs a vertical search scheme within an algebraic graph framework we have developed to reduce the number of redundant candidates proposed. Our empirical study on synthetic and real datasets demonstrates that FFSM achieves a substantial performance gain over the current start-of-the-art subgraph mining algorithm gSpan.", :author (#search_api.search_api.Author{:id 133910, :first-name nil, :last-name nil, :full-name "Hassan Khosravi"} #search_api.search_api.Author{:id 84842, :first-name nil, :last-name nil, :full-name "Bahareh Bina"}), :year 2010, :venue "Canadian Conference on AI", :ncit 1, :string "A Survey on Statistical Relational Learning.. Frequent subgraph mining is an active research topic in the data mining community. A graph is a general model to represent data and has been used in many domains like cheminformatics and bioinformatics. Mining patterns from graph databases is challenging since graph related operations, such as subgraph testing, generally have higher time complexity than the corresponding operations on itemsets, sequences, and trees, which have been studied extensively. We propose a novel frequent subgraph mining algorithm: FFSM, which employs a vertical search scheme within an algebraic graph framework we have developed to reduce the number of redundant candidates proposed. Our empirical study on synthetic and real datasets demonstrates that FFSM achieves a substantial performance gain over the current start-of-the-art subgraph mining algorithm gSpan.", :doc-id "A Survey on Statistical Relational Learning. 2010 Hassan Khosravi, Bahareh Bina"} #search_api.search_api.Paper{:id 388102, :key "conf/ijcai/DavisBDPRCS05", :title "View Learning for Statistical Relational Learning: With an Application to Mammography.", :abstract "Statistical relational learning (SRL) constructs probabilistic models from relational databases. A key capability of SRL is the learning of arcs (in the Bayes net sense) connecting entries in different rows of a relational table, or in different tables. Nevertheless, SRL approaches currently are constrained to use the existing database schema. For many database applications, users find it profitable to define alternative \"views\" of the database, in effect defining new fields or tables. Such new fields or tables can also be highly useful in learning. We provide SRL with the capability of learning new views.", :author (#search_api.search_api.Author{:id 747351, :first-name nil, :last-name nil, :full-name "Jesse Davis"} #search_api.search_api.Author{:id 750446, :first-name nil, :last-name nil, :full-name "Elizabeth S. Burnside"} #search_api.search_api.Author{:id 1157247, :first-name nil, :last-name nil, :full-name "Inês de Castro Dutra"} #search_api.search_api.Author{:id 138183, :first-name nil, :last-name nil, :full-name "David Page"} #search_api.search_api.Author{:id 464920, :first-name nil, :last-name nil, :full-name "Raghu Ramakrishnan"} #search_api.search_api.Author{:id 1352436, :first-name nil, :last-name nil, :full-name "Vítor Santos Costa"} #search_api.search_api.Author{:id 7, :first-name nil, :last-name nil, :full-name "Jude W. Shavlik"}), :year 2005, :venue "IJCAI", :ncit 33, :string "View Learning for Statistical Relational Learning: With an Application to Mammography.. Statistical relational learning (SRL) constructs probabilistic models from relational databases. A key capability of SRL is the learning of arcs (in the Bayes net sense) connecting entries in different rows of a relational table, or in different tables. Nevertheless, SRL approaches currently are constrained to use the existing database schema. For many database applications, users find it profitable to define alternative \"views\" of the database, in effect defining new fields or tables. Such new fields or tables can also be highly useful in learning. We provide SRL with the capability of learning new views.", :doc-id "View Learning for Statistical Relational Learning: With an Application to Mammography. 2005 Jesse Davis, Elizabeth S. Burnside, Inês de Castro Dutra, David Page, Raghu Ramakrishnan, Vítor Santos Costa, Jude W. Shavlik"} #search_api.search_api.Paper{:id 574389, :key "conf/sbia/Raedt08a", :title "Logic, Probability and Learning, or an Introduction to Statistical Relational Learning.", :abstract "Probabilistic inductive logic programming (PILP), sometimes also called statistical relational learning, addresses one of the central questions of artificial intelligence: the integration of probabilistic reasoning with first order logic representations and machine learning. A rich variety of different formalisms and learning techniques have been developed and they are being applied on applications in network analysis, robotics, bio-informatics, intelligent agents, etc. This tutorial starts with an introduction to probabilistic representations and machine learning, and then continues with an overview of the state-of-the-art in statistical relational learning. We start from classical settings for logic learning (or inductive logic programming) namely learning from entailment, learning from interpretations, and learning from proofs, and show how they can be extended with probabilistic methods. While doing so, we review state-of-the-art statistical relational learning approaches and show how they fit the discussed learning settings for probabilistic inductive logic programming.", :author (#search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2008, :venue "SBIA", :ncit 0, :string "Logic, Probability and Learning, or an Introduction to Statistical Relational Learning.. Probabilistic inductive logic programming (PILP), sometimes also called statistical relational learning, addresses one of the central questions of artificial intelligence: the integration of probabilistic reasoning with first order logic representations and machine learning. A rich variety of different formalisms and learning techniques have been developed and they are being applied on applications in network analysis, robotics, bio-informatics, intelligent agents, etc. This tutorial starts with an introduction to probabilistic representations and machine learning, and then continues with an overview of the state-of-the-art in statistical relational learning. We start from classical settings for logic learning (or inductive logic programming) namely learning from entailment, learning from interpretations, and learning from proofs, and show how they can be extended with probabilistic methods. While doing so, we review state-of-the-art statistical relational learning approaches and show how they fit the discussed learning settings for probabilistic inductive logic programming.", :doc-id "Logic, Probability and Learning, or an Introduction to Statistical Relational Learning. 2008 Luc De Raedt"} #search_api.search_api.Paper{:id 1009359, :key "journals/ml/CravenS01", :title "Relational Learning with Statistical Predicate Invention: Better Models for Hypertext.", :abstract "We present a new approach to learning hypertext classifiers that combines a statistical text-learning method with a relational rule learner. This approach is well suited to learning in hypertext domains because its statistical component allows it to characterize text in terms of word frequencies, whereas its relational component is able to describe how neighboring documents are related to each other by hyperlinks that connect them. We evaluate our approach by applying it to tasks that involve learning definitions for (i) classes of pages, (ii) particular relations that exist between pairs of pages, and (iii) locating a particular class of information in the internal structure of pages. Our experiments demonstrate that this new approach is able to learn more accurate classifiers than either of its constituent methods alone.", :author (#search_api.search_api.Author{:id 305395, :first-name nil, :last-name nil, :full-name "Mark Craven"} #search_api.search_api.Author{:id 872987, :first-name nil, :last-name nil, :full-name "Seán Slattery"}), :year 2001, :venue "Machine Learning", :ncit 121, :string "Relational Learning with Statistical Predicate Invention: Better Models for Hypertext.. We present a new approach to learning hypertext classifiers that combines a statistical text-learning method with a relational rule learner. This approach is well suited to learning in hypertext domains because its statistical component allows it to characterize text in terms of word frequencies, whereas its relational component is able to describe how neighboring documents are related to each other by hyperlinks that connect them. We evaluate our approach by applying it to tasks that involve learning definitions for (i) classes of pages, (ii) particular relations that exist between pairs of pages, and (iii) locating a particular class of information in the internal structure of pages. Our experiments demonstrate that this new approach is able to learn more accurate classifiers than either of its constituent methods alone.", :doc-id "Relational Learning with Statistical Predicate Invention: Better Models for Hypertext. 2001 Mark Craven, Seán Slattery"} #search_api.search_api.Paper{:id 1254526, :key "conf/pkdd/RettingerNT09", :title "Statistical Relational Learning with Formal Ontologies.", :abstract "We propose a learning approach for integrating formal knowledge into statistical inference by exploiting ontologies as a semantically rich and fully formal representation of prior knowledge. The logical constraints deduced from ontologies can be utilized to enhance and control the learning task by enforcing description logic satisfiability in a latent multi-relational graphical model. To demonstrate the feasibility of our approach we provide experiments using real world social network data in form of a $\\mathcal{SHOIN}(D)$ ontology. The results illustrate two main practical advancements: First, entities and entity relationships can be analyzed via the latent model structure. Second, enforcing the ontological constraints guarantees that the learned model does not predict inconsistent relations. In our experiments, this leads to an improved predictive performance.", :author (#search_api.search_api.Author{:id 444383, :first-name nil, :last-name nil, :full-name "Achim Rettinger"} #search_api.search_api.Author{:id 222492, :first-name nil, :last-name nil, :full-name "Matthias Nickles"} #search_api.search_api.Author{:id 680469, :first-name nil, :last-name nil, :full-name "Volker Tresp"}), :year 2009, :venue "ECML/PKDD (2)", :ncit 13, :string "Statistical Relational Learning with Formal Ontologies.. We propose a learning approach for integrating formal knowledge into statistical inference by exploiting ontologies as a semantically rich and fully formal representation of prior knowledge. The logical constraints deduced from ontologies can be utilized to enhance and control the learning task by enforcing description logic satisfiability in a latent multi-relational graphical model. To demonstrate the feasibility of our approach we provide experiments using real world social network data in form of a $\\mathcal{SHOIN}(D)$ ontology. The results illustrate two main practical advancements: First, entities and entity relationships can be analyzed via the latent model structure. Second, enforcing the ontological constraints guarantees that the learned model does not predict inconsistent relations. In our experiments, this leads to an improved predictive performance.", :doc-id "Statistical Relational Learning with Formal Ontologies. 2009 Achim Rettinger, Matthias Nickles, Volker Tresp"} #search_api.search_api.Paper{:id 395140, :key "conf/ilp/Getoor05", :title "Tutorial on Statistical Relational Learning.", :abstract nil, :author (#search_api.search_api.Author{:id 1449720, :first-name nil, :last-name nil, :full-name "Lise Getoor"}), :year 2005, :venue "ILP", :ncit 7, :string "Tutorial on Statistical Relational Learning.. ", :doc-id "Tutorial on Statistical Relational Learning. 2005 Lise Getoor"} #search_api.search_api.Paper{:id 57463, :key "conf/atal/RettingerNT08", :title "A statistical relational model for trust learning.", :abstract "We address the learning of trust based on past observations and context information. We argue that from the truster's point of view trust is best expressed as one of several relations that exist between the agent to be trusted (trustee) and the state of the environment. Besides attributes expressing trustworthiness, additional relations might describe commitments made by the trustee with regard to the current situation, like: a seller offers a certain price for a specific product. We show how to implement and learn contextsensitive trust using statistical relational learning in form of the Infinite Hidden Relational Trust Model (IHRTM). The practicability and effectiveness of our approach is evaluated empirically on user-ratings gathered from eBay. Our results suggest that (i) the inherent clustering achieved in the algorithm allows the truster to characterize the structure of a trust-situation and provides meaningful trust assessments; (ii) utilizing the collaborative filtering effect associated with relational data does improve trust assessment performance; (iii) by learning faster and transferring knowledge more effectively we improve cold start performance and can cope better with dynamic behavior in open multiagent systems. The later is demonstrated with interactions recorded from a strategic two-player negotiation scenario.", :author (#search_api.search_api.Author{:id 444383, :first-name nil, :last-name nil, :full-name "Achim Rettinger"} #search_api.search_api.Author{:id 222492, :first-name nil, :last-name nil, :full-name "Matthias Nickles"} #search_api.search_api.Author{:id 680469, :first-name nil, :last-name nil, :full-name "Volker Tresp"}), :year 2008, :venue "AAMAS (2)", :ncit 14, :string "A statistical relational model for trust learning.. We address the learning of trust based on past observations and context information. We argue that from the truster's point of view trust is best expressed as one of several relations that exist between the agent to be trusted (trustee) and the state of the environment. Besides attributes expressing trustworthiness, additional relations might describe commitments made by the trustee with regard to the current situation, like: a seller offers a certain price for a specific product. We show how to implement and learn contextsensitive trust using statistical relational learning in form of the Infinite Hidden Relational Trust Model (IHRTM). The practicability and effectiveness of our approach is evaluated empirically on user-ratings gathered from eBay. Our results suggest that (i) the inherent clustering achieved in the algorithm allows the truster to characterize the structure of a trust-situation and provides meaningful trust assessments; (ii) utilizing the collaborative filtering effect associated with relational data does improve trust assessment performance; (iii) by learning faster and transferring knowledge more effectively we improve cold start performance and can cope better with dynamic behavior in open multiagent systems. The later is demonstrated with interactions recorded from a strategic two-player negotiation scenario.", :doc-id "A statistical relational model for trust learning. 2008 Achim Rettinger, Matthias Nickles, Volker Tresp"} #search_api.search_api.Paper{:id 3382704, :key "journals/corr/abs-1204-0033", :title "Transforming Graph Representations for Statistical Relational Learning", :abstract nil, :author (#search_api.search_api.Author{:id 1373085, :first-name nil, :last-name nil, :full-name "Ryan A. Rossi"} #search_api.search_api.Author{:id 724185, :first-name nil, :last-name nil, :full-name "Luke McDowell"} #search_api.search_api.Author{:id 523082, :first-name nil, :last-name nil, :full-name "David W. Aha"} #search_api.search_api.Author{:id 150768, :first-name nil, :last-name nil, :full-name "Jennifer Neville"}), :year 2012, :venue "CoRR", :ncit 1, :string "Transforming Graph Representations for Statistical Relational Learning. ", :doc-id "Transforming Graph Representations for Statistical Relational Learning 2012 Ryan A. Rossi, Luke McDowell, David W. Aha, Jennifer Neville"} #search_api.search_api.Paper{:id 472937, :key "conf/kdd/Koller03", :title "Statistical learning from relational data.", :abstract nil, :author (#search_api.search_api.Author{:id 560995, :first-name nil, :last-name nil, :full-name "Daphne Koller"}), :year 2003, :venue "KDD", :ncit 0, :string "Statistical learning from relational data.. ", :doc-id "Statistical learning from relational data. 2003 Daphne Koller"} #search_api.search_api.Paper{:id 1009341, :key "journals/ml/BlockeelJK06", :title "Introduction to the special issue on multi-relational data mining and statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 882341, :first-name nil, :last-name nil, :full-name "Hendrik Blockeel"} #search_api.search_api.Author{:id 408384, :first-name nil, :last-name nil, :full-name "David Jensen"} #search_api.search_api.Author{:id 954022, :first-name nil, :last-name nil, :full-name "Stefan Kramer"}), :year 2006, :venue "Machine Learning", :ncit 2, :string "Introduction to the special issue on multi-relational data mining and statistical relational learning.. ", :doc-id "Introduction to the special issue on multi-relational data mining and statistical relational learning. 2006 Hendrik Blockeel, David Jensen, Stefan Kramer"} #search_api.search_api.Paper{:id 1301282, :key "series/sci/BibaFE10", :title "Towards Multistrategic Statistical Relational Learning.", :abstract nil, :author (#search_api.search_api.Author{:id 1482903, :first-name nil, :last-name nil, :full-name "Marenglen Biba"} #search_api.search_api.Author{:id 1126564, :first-name nil, :last-name nil, :full-name "Stefano Ferilli"} #search_api.search_api.Author{:id 769268, :first-name nil, :last-name nil, :full-name "Floriana Esposito"}), :year 2010, :venue "Advances in Machine Learning II", :ncit 0, :string "Towards Multistrategic Statistical Relational Learning.. ", :doc-id "Towards Multistrategic Statistical Relational Learning. 2010 Marenglen Biba, Stefano Ferilli, Floriana Esposito"} #search_api.search_api.Paper{:id 302837, :key "conf/icdm/PopesculULP03", :title "Statistical Relational Learning for Document Mining.", :abstract "A major obstacle to fully integrated deployment of manydata mining algorithms is the assumption that data sitsin a single table, even though most real-world databaseshave complex relational structures. We propose an integratedapproach to statistical modeling from relationaldatabases. We structure the search space based on \"refinementgraphs\", which are widely used in inductive logic programmingfor learning logic descriptions. The use of statisticsallows us to extend the search space to include richerset of features, including many which are not boolean.Search and model selection are integrated into a single process,allowing information criteria native to the statisticalmodel, for example logistic regression, to make feature selectiondecisions in a step-wise manner. We present experimentalresults for the task of predicting where scientific paperswill be published based on relational data taken fromCiteSeer. Our approach results in classification accuraciessuperior to those achieved when using classical \"flat\" features.The resulting classifier can be used to recommendwhere to publish articles.", :author (#search_api.search_api.Author{:id 597076, :first-name nil, :last-name nil, :full-name "Alexandrin Popescul"} #search_api.search_api.Author{:id 352656, :first-name nil, :last-name nil, :full-name "Lyle H. Ungar"} #search_api.search_api.Author{:id 599072, :first-name nil, :last-name nil, :full-name "Steve Lawrence"} #search_api.search_api.Author{:id 80443, :first-name nil, :last-name nil, :full-name "David M. Pennock"}), :year 2003, :venue "ICDM", :ncit 51, :string "Statistical Relational Learning for Document Mining.. A major obstacle to fully integrated deployment of manydata mining algorithms is the assumption that data sitsin a single table, even though most real-world databaseshave complex relational structures. We propose an integratedapproach to statistical modeling from relationaldatabases. We structure the search space based on \"refinementgraphs\", which are widely used in inductive logic programmingfor learning logic descriptions. The use of statisticsallows us to extend the search space to include richerset of features, including many which are not boolean.Search and model selection are integrated into a single process,allowing information criteria native to the statisticalmodel, for example logistic regression, to make feature selectiondecisions in a step-wise manner. We present experimentalresults for the task of predicting where scientific paperswill be published based on relational data taken fromCiteSeer. Our approach results in classification accuraciessuperior to those achieved when using classical \"flat\" features.The resulting classifier can be used to recommendwhere to publish articles.", :doc-id "Statistical Relational Learning for Document Mining. 2003 Alexandrin Popescul, Lyle H. Ungar, Steve Lawrence, David M. Pennock"} #search_api.search_api.Paper{:id 14273, :key "conf/aaai/Neville05", :title "Structure Learning for Statistical Relational Models.", :abstract nil, :author (#search_api.search_api.Author{:id 150768, :first-name nil, :last-name nil, :full-name "Jennifer Neville"}), :year 2005, :venue "AAAI", :ncit 0, :string "Structure Learning for Statistical Relational Models.. ", :doc-id "Structure Learning for Statistical Relational Models. 2005 Jennifer Neville"} #search_api.search_api.Paper{:id 2866177, :key "journals/ml/LandwehrPRF10", :title "Fast learning of relational kernels.", :abstract "We develop a general theoretical framework for statistical logical learning with kernels based on dynamic propositionalization, where structure learning corresponds to inferring a suitable kernel on logical objects, and parameter learning corresponds to function learning in the resulting reproducing kernel Hilbert space. In particular, we study the case where structure learning is performed by a simple FOIL-like algorithm, and propose alternative scoring functions for guiding the search process. We present an empirical evaluation on several data sets in the single-task as well as in the multi-task setting.", :author (#search_api.search_api.Author{:id 127590, :first-name nil, :last-name nil, :full-name "Niels Landwehr"} #search_api.search_api.Author{:id 1520074, :first-name nil, :last-name nil, :full-name "Andrea Passerini"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"} #search_api.search_api.Author{:id 34394, :first-name nil, :last-name nil, :full-name "Paolo Frasconi"}), :year 2010, :venue "Machine Learning", :ncit 12, :string "Fast learning of relational kernels.. We develop a general theoretical framework for statistical logical learning with kernels based on dynamic propositionalization, where structure learning corresponds to inferring a suitable kernel on logical objects, and parameter learning corresponds to function learning in the resulting reproducing kernel Hilbert space. In particular, we study the case where structure learning is performed by a simple FOIL-like algorithm, and propose alternative scoring functions for guiding the search process. We present an empirical evaluation on several data sets in the single-task as well as in the multi-task setting.", :doc-id "Fast learning of relational kernels. 2010 Niels Landwehr, Andrea Passerini, Luc De Raedt, Paolo Frasconi"} #search_api.search_api.Paper{:id 3623508, :key "journals/ijsnm/EspositoFBM12", :title "Social networks and statistical relational learning: a survey.", :abstract nil, :author (#search_api.search_api.Author{:id 769268, :first-name nil, :last-name nil, :full-name "Floriana Esposito"} #search_api.search_api.Author{:id 1126564, :first-name nil, :last-name nil, :full-name "Stefano Ferilli"} #search_api.search_api.Author{:id 1078358, :first-name nil, :last-name nil, :full-name "Teresa Maria Altomare Basile"} #search_api.search_api.Author{:id 1294122, :first-name nil, :last-name nil, :full-name "Nicola Di Mauro"}), :year 2012, :venue "IJSNM", :ncit 0, :string "Social networks and statistical relational learning: a survey.. ", :doc-id "Social networks and statistical relational learning: a survey. 2012 Floriana Esposito, Stefano Ferilli, Teresa Maria Altomare Basile, Nicola Di Mauro"} #search_api.search_api.Paper{:id 687201, :key "conf/grc/Chen07", :title "Research on Statistical Relational Learning and Rough Set in SRL.", :abstract "Statistical relational learning constructs statistical mod- els from relational databases, combining the powers of re- lational learning and statistical learning. Its strong abil- ity and special property make statistical relational learn- ing become one of the important areas in machine learn- ing. In this paper, the general concepts and characteris- tics of statistical relational learning are presented firstly. Then some major branches of this newly emerging field are discussed, including logic and rule-based approaches, frame and object-oriented approaches, and several other important approaches. After that some methods of apply- ing rough set in statistical relational learning are described, such as gRS-ILP and VPRSILP. Finally applications of sta- tistical relational learning are briefly introduced and some future directions of statistical relational learning and the prospects of rough set in this area are pointed out.", :author (#search_api.search_api.Author{:id 74552, :first-name nil, :last-name nil, :full-name "Fei Chen"}), :year 2007, :venue "GrC", :ncit 0, :string "Research on Statistical Relational Learning and Rough Set in SRL.. Statistical relational learning constructs statistical mod- els from relational databases, combining the powers of re- lational learning and statistical learning. Its strong abil- ity and special property make statistical relational learn- ing become one of the important areas in machine learn- ing. In this paper, the general concepts and characteris- tics of statistical relational learning are presented firstly. Then some major branches of this newly emerging field are discussed, including logic and rule-based approaches, frame and object-oriented approaches, and several other important approaches. After that some methods of apply- ing rough set in statistical relational learning are described, such as gRS-ILP and VPRSILP. Finally applications of sta- tistical relational learning are briefly introduced and some future directions of statistical relational learning and the prospects of rough set in this area are pointed out.", :doc-id "Research on Statistical Relational Learning and Rough Set in SRL. 2007 Fei Chen"} #search_api.search_api.Paper{:id 2927018, :key "journals/sigkdd/RamonCFK09", :title "StReBio'09: statistical relational learning and mining in bioinformatics.", :abstract nil, :author (#search_api.search_api.Author{:id 1015109, :first-name nil, :last-name nil, :full-name "Jan Ramon"} #search_api.search_api.Author{:id 892400, :first-name nil, :last-name nil, :full-name "Fabrizio Costa"} #search_api.search_api.Author{:id 981793, :first-name nil, :last-name nil, :full-name "Christophe Costa Florêncio"} #search_api.search_api.Author{:id 19675, :first-name nil, :last-name nil, :full-name "Joost N. Kok"}), :year 2009, :venue "SIGKDD Explorations", :ncit 0, :string "StReBio'09: statistical relational learning and mining in bioinformatics.. ", :doc-id "StReBio'09: statistical relational learning and mining in bioinformatics. 2009 Jan Ramon, Fabrizio Costa, Christophe Costa Florêncio, Joost N. Kok"} #search_api.search_api.Paper{:id 3074945, :key "journals/ml/BlockeelBRDKY11", :title "Guest editorial to the special issue on inductive logic programming, mining and learning in graphs and statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 882341, :first-name nil, :last-name nil, :full-name "Hendrik Blockeel"} #search_api.search_api.Author{:id 272602, :first-name nil, :last-name nil, :full-name "Karsten M. Borgwardt"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"} #search_api.search_api.Author{:id 668085, :first-name nil, :last-name nil, :full-name "Pedro Domingos"} #search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"} #search_api.search_api.Author{:id 324907, :first-name nil, :last-name nil, :full-name "Xifeng Yan"}), :year 2011, :venue "Machine Learning", :ncit 0, :string "Guest editorial to the special issue on inductive logic programming, mining and learning in graphs and statistical relational learning.. ", :doc-id "Guest editorial to the special issue on inductive logic programming, mining and learning in graphs and statistical relational learning. 2011 Hendrik Blockeel, Karsten M. Borgwardt, Luc De Raedt, Pedro Domingos, Kristian Kersting, Xifeng Yan"} #search_api.search_api.Paper{:id 746727, :key "journals/aicom/Tian06", :title "Context-based statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 260142, :first-name nil, :last-name nil, :full-name "YongHong Tian"}), :year 2006, :venue "AI Commun.", :ncit 0, :string "Context-based statistical relational learning.. ", :doc-id "Context-based statistical relational learning. 2006 YongHong Tian"} #search_api.search_api.Paper{:id 3782751, :key "conf/ilp/SahaSR12", :title "What Kinds of Relational Features Are Useful for Statistical Learning?", :abstract nil, :author (#search_api.search_api.Author{:id 914315, :first-name nil, :last-name nil, :full-name "Amrita Saha"} #search_api.search_api.Author{:id 1118213, :first-name nil, :last-name nil, :full-name "Ashwin Srinivasan"} #search_api.search_api.Author{:id 376403, :first-name nil, :last-name nil, :full-name "Ganesh Ramakrishnan"}), :year 2012, :venue "ILP", :ncit 0, :string "What Kinds of Relational Features Are Useful for Statistical Learning?. ", :doc-id "What Kinds of Relational Features Are Useful for Statistical Learning? 2012 Amrita Saha, Ashwin Srinivasan, Ganesh Ramakrishnan"} #search_api.search_api.Paper{:id 473199, :key "conf/kdd/PopesculU04", :title "Cluster-based concept invention for statistical relational learning.", :abstract "We use clustering to derive new relations which augment database schema used in automatic generation of predictive features in statistical relational learning. Entities derived from clusters increase the expressivity of feature spaces by creating new first-class concepts which contribute to the creation of new features. For example, in CiteSeer, papers can be clustered based on words or citations giving \"topics\", and authors can be clustered based on documents they co-author giving \"communities\". Such cluster-derived concepts become part of more complex feature expressions. Out of the large number of generated features, those which improve predictive accuracy are kept in the model, as decided by statistical feature selection criteria. We present results demonstrating improved accuracy on two tasks, venue prediction and link prediction, using CiteSeer data.", :author (#search_api.search_api.Author{:id 597076, :first-name nil, :last-name nil, :full-name "Alexandrin Popescul"} #search_api.search_api.Author{:id 352656, :first-name nil, :last-name nil, :full-name "Lyle H. Ungar"}), :year 2004, :venue "KDD", :ncit 31, :string "Cluster-based concept invention for statistical relational learning.. We use clustering to derive new relations which augment database schema used in automatic generation of predictive features in statistical relational learning. Entities derived from clusters increase the expressivity of feature spaces by creating new first-class concepts which contribute to the creation of new features. For example, in CiteSeer, papers can be clustered based on words or citations giving \"topics\", and authors can be clustered based on documents they co-author giving \"communities\". Such cluster-derived concepts become part of more complex feature expressions. Out of the large number of generated features, those which improve predictive accuracy are kept in the model, as decided by statistical feature selection criteria. We present results demonstrating improved accuracy on two tasks, venue prediction and link prediction, using CiteSeer data.", :doc-id "Cluster-based concept invention for statistical relational learning. 2004 Alexandrin Popescul, Lyle H. Ungar"} #search_api.search_api.Paper{:id 1185824, :key "journals/jocn/AblaKO08", :title "On-line Assessment of Statistical Learning by Event-related Potentials.", :abstract "We investigated the neural processes involved in on-line statistical learning and word segmentation. Auditory event-related potentials (ERPs) were recorded while participants were exposed to continuous, nonlinguistic auditory sequences, the elements of which were organized into “tritone words” that were sequenced in random order, with no silent spaces between them. After listening to three 6.6-min sessions of sequences, the participants performed a behavioral choice test, in which they were instructed to indicate the most familiar tone sequence in each test trial by pressing buttons. The participants were divided into three groups (high, middle, and low learners) based on their behavioral performance. The overall mean performance was 74.4%, indicating that the tone sequence was segmented and that the participants learned the tone words statistically. Grand-averaged ERPs showed that word onset (initial tone) elicited the largest N100 and N400 in the early learning session of high learners, but in middle learners, the word-onset effect was elicited in a later session, and there was no effect in low learners. The N400 amplitudes significantly differed between the three learning sessions in the high-and middle-learner groups. The results suggest that the N400 effect indicates not only on-line word segmentation but also the degree of statistical learning. This study provides insight into the neural mechanisms underlying on-line statistical learning processes.", :author (#search_api.search_api.Author{:id 85341, :first-name nil, :last-name nil, :full-name "Dilshat Abla"} #search_api.search_api.Author{:id 167451, :first-name nil, :last-name nil, :full-name "Kentaro Katahira"} #search_api.search_api.Author{:id 883499, :first-name nil, :last-name nil, :full-name "Kazuo Okanoya"}), :year 2008, :venue "J. Cognitive Neuroscience", :ncit 0, :string "On-line Assessment of Statistical Learning by Event-related Potentials.. We investigated the neural processes involved in on-line statistical learning and word segmentation. Auditory event-related potentials (ERPs) were recorded while participants were exposed to continuous, nonlinguistic auditory sequences, the elements of which were organized into “tritone words” that were sequenced in random order, with no silent spaces between them. After listening to three 6.6-min sessions of sequences, the participants performed a behavioral choice test, in which they were instructed to indicate the most familiar tone sequence in each test trial by pressing buttons. The participants were divided into three groups (high, middle, and low learners) based on their behavioral performance. The overall mean performance was 74.4%, indicating that the tone sequence was segmented and that the participants learned the tone words statistically. Grand-averaged ERPs showed that word onset (initial tone) elicited the largest N100 and N400 in the early learning session of high learners, but in middle learners, the word-onset effect was elicited in a later session, and there was no effect in low learners. The N400 amplitudes significantly differed between the three learning sessions in the high-and middle-learner groups. The results suggest that the N400 effect indicates not only on-line word segmentation but also the degree of statistical learning. This study provides insight into the neural mechanisms underlying on-line statistical learning processes.", :doc-id "On-line Assessment of Statistical Learning by Event-related Potentials. 2008 Dilshat Abla, Kentaro Katahira, Kazuo Okanoya"} #search_api.search_api.Paper{:id 3786794, :key "conf/fusion/JandelSW12", :title "Online learnability of Statistical Relational Learning in anomaly detection.", :abstract nil, :author (#search_api.search_api.Author{:id 1344508, :first-name nil, :last-name nil, :full-name "Magnus Jändel"} #search_api.search_api.Author{:id 1319563, :first-name nil, :last-name nil, :full-name "Pontus Svenson"} #search_api.search_api.Author{:id 14343624, :first-name nil, :last-name nil, :full-name "Niclas Wadströmer"}), :year 2012, :venue "FUSION", :ncit 0, :string "Online learnability of Statistical Relational Learning in anomaly detection.. ", :doc-id "Online learnability of Statistical Relational Learning in anomaly detection. 2012 Magnus Jändel, Pontus Svenson, Niclas Wadströmer"} #search_api.search_api.Paper{:id 192565, :key "conf/eurocolt/ShamirS95", :title "Learning by extended statistical queries and its relation to PAC learning.", :abstract nil, :author (#search_api.search_api.Author{:id 1191060, :first-name nil, :last-name nil, :full-name "Eli Shamir"} #search_api.search_api.Author{:id 690500, :first-name nil, :last-name nil, :full-name "Clara Shwartzman"}), :year 1995, :venue "EuroCOLT", :ncit 8, :string "Learning by extended statistical queries and its relation to PAC learning.. ", :doc-id "Learning by extended statistical queries and its relation to PAC learning. 1995 Eli Shamir, Clara Shwartzman"} #search_api.search_api.Paper{:id 388504, :key "conf/ijcai/FriedmanGKP99", :title "Learning Probabilistic Relational Models.", :abstract "A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with \"flat\" data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases.", :author (#search_api.search_api.Author{:id 61431, :first-name nil, :last-name nil, :full-name "Nir Friedman"} #search_api.search_api.Author{:id 1449720, :first-name nil, :last-name nil, :full-name "Lise Getoor"} #search_api.search_api.Author{:id 560995, :first-name nil, :last-name nil, :full-name "Daphne Koller"} #search_api.search_api.Author{:id 154856, :first-name nil, :last-name nil, :full-name "Avi Pfeffer"}), :year 1999, :venue "IJCAI", :ncit 843, :string "Learning Probabilistic Relational Models.. A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with \"flat\" data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning -- the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases.", :doc-id "Learning Probabilistic Relational Models. 1999 Nir Friedman, Lise Getoor, Daphne Koller, Avi Pfeffer"} #search_api.search_api.Paper{:id 574400, :key "conf/sbia/Raedt08", :title "Logical and Relational Learning.", :abstract "I use the term logical and relational learning (LRL) to refer to the subfield of machine learning and data mining that is concerned with learning in expressive logical or relational representations. It is the union of inductive logic programming, (statistical) relational learning and multi-relational data mining and constitutes a general class of techniques and methodology for learning from structured data (such as graphs, networks, relational databases) and background knowledge. During the course of its existence, logical and relational learning has changed dramatically. Whereas early work was mainly concerned with logical issues (and even program synthesis from examples), in the 90s its focus was on the discovery of new and interpretable knowledge from structured data, often in the form of rules or patterns. Since then the range of tasks to which logical and relational learning has been applied has significantly broadened and now covers almost all machine learning problems and settings. Today, there exist logical and relational learning methods for reinforcement learning, statistical learning, distance- and kernel-based learning in addition to traditional symbolic machine learning approaches. At the same time, logical and relational learning problems are appearing everywhere. Advances in intelligent systems are enabling the generation of high-level symbolic and structured data in a wide variety of domains, including the semantic web, robotics, vision, social networks, and the life sciences, which in turn raises new challenges and opportunities for logical and relational learning. These developments have led to a new view on logical and relational learning and its role in machine learning and artificial intelligence. In this talk, I shall reflect on this view by identifying some of the lessons learned in logical and relational learning and formulating some challenges for future developments.", :author (#search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2008, :venue "SBIA", :ncit 173, :string "Logical and Relational Learning.. I use the term logical and relational learning (LRL) to refer to the subfield of machine learning and data mining that is concerned with learning in expressive logical or relational representations. It is the union of inductive logic programming, (statistical) relational learning and multi-relational data mining and constitutes a general class of techniques and methodology for learning from structured data (such as graphs, networks, relational databases) and background knowledge. During the course of its existence, logical and relational learning has changed dramatically. Whereas early work was mainly concerned with logical issues (and even program synthesis from examples), in the 90s its focus was on the discovery of new and interpretable knowledge from structured data, often in the form of rules or patterns. Since then the range of tasks to which logical and relational learning has been applied has significantly broadened and now covers almost all machine learning problems and settings. Today, there exist logical and relational learning methods for reinforcement learning, statistical learning, distance- and kernel-based learning in addition to traditional symbolic machine learning approaches. At the same time, logical and relational learning problems are appearing everywhere. Advances in intelligent systems are enabling the generation of high-level symbolic and structured data in a wide variety of domains, including the semantic web, robotics, vision, social networks, and the life sciences, which in turn raises new challenges and opportunities for logical and relational learning. These developments have led to a new view on logical and relational learning and its role in machine learning and artificial intelligence. In this talk, I shall reflect on this view by identifying some of the lessons learned in logical and relational learning and formulating some challenges for future developments.", :doc-id "Logical and Relational Learning. 2008 Luc De Raedt"} #search_api.search_api.Paper{:id 3341018, :key "phd/de/Kersting2006", :title "An inductive logic programming approach to statistical relational learning.", :abstract nil, :author (#search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"}), :year 2006, :venue nil, :ncit 15, :string "An inductive logic programming approach to statistical relational learning.. ", :doc-id "An inductive logic programming approach to statistical relational learning. 2006 Kristian Kersting"} #search_api.search_api.Paper{:id 3008005, :key "conf/aied/Murray11", :title "Statistical Relational Learning in Student Modeling for Intelligent Tutoring Systems.", :abstract nil, :author (#search_api.search_api.Author{:id 1005468, :first-name nil, :last-name nil, :full-name "William R. Murray"}), :year 2011, :venue "AIED", :ncit 0, :string "Statistical Relational Learning in Student Modeling for Intelligent Tutoring Systems.. ", :doc-id "Statistical Relational Learning in Student Modeling for Intelligent Tutoring Systems. 2011 William R. Murray"} #search_api.search_api.Paper{:id 746711, :key "journals/aicom/Kersting06", :title "An inductive logic programming approach to statistical relational learning.", :abstract "It has been a great pleasure to be asked to write the preface for the book based on Kristian Kersting's thesis. There is no doubt in my mind that this is a remarkable and outstanding piece of work. In his thesis Kristian has made an assault on one of the hardest integration problems at the heart of Artificial Intelligence research. This involves taking three disparate major areas of research and attempting a fusion among them. The three areas are: Logic Programming, Uncertainty Reasoning and Machine Learning. Every one of these is a major sub-area of research with its own associated international research conferences. Having taken on such a Herculean task, Kristian has produced a series of widely published results which are now at the core of a newly emerging area: Probabilistic Inductive Logic Programming. The new area is closely tied to, though strictly subsumes, a new field known as “Statistical Relational Learning” which has in the last few years gained major prominence in the American Artificial Intelligence research community. Within his thesis Kristian makes several major contributions, many of which have already been published in refereed conference and journal papers. Firstly, Kristian introduces a series of definitions which circumscribe the new area formed by extending Inductive Logic Programming to the case in which clauses are annotated with probability values. This represents a new and powerful framework which supersedes a number of influential papers and research areas in Artificial Intelligence. Secondly, Kristian introduces Bayesian Logic Programs (BLPs). These represent an elegantly defined lifting of Judea Pearl's Bayesian networks to the logic programming level. Since Kristian's introduction of BLPs, a number of results indicate that BLPs generalise many previously defined representations, not the least of which are Bayesian networks, Logic Programs, Probabilistic Relation Models and Stochastic Logic Programs. Next Kristian investigates the approach of Learning from proofs. This is an interesting new learning framework which is the first to go beyond the two standard semantic frameworks of Inductive Logic Programming. Kristian then looks at the problem of upgrading HMMs to logical HMMs. Hidden Markov Models (HMMs) are one of the most widely used machine learning technologies in Statistical Linguistics and Bioinformatics, and allow the representation of probabilistic finite automata. Kristian has upgraded standard HMMs to allow relational descriptions to be included within the description of the automata. The three standard HMM estimation algorithms are also upgraded. He has demonstrated the power of such representations using biological predictive modelling problems, and shown performance increases over alternative approaches. Kristian next considers the issue of upgrading Fisher Kernels to Relational Fisher kernels. Fisher kernels have been widely used within statistics and more recently in support vector machines. Buildin...", :author (#search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"}), :year 2006, :venue "AI Commun.", :ncit 15, :string "An inductive logic programming approach to statistical relational learning.. It has been a great pleasure to be asked to write the preface for the book based on Kristian Kersting's thesis. There is no doubt in my mind that this is a remarkable and outstanding piece of work. In his thesis Kristian has made an assault on one of the hardest integration problems at the heart of Artificial Intelligence research. This involves taking three disparate major areas of research and attempting a fusion among them. The three areas are: Logic Programming, Uncertainty Reasoning and Machine Learning. Every one of these is a major sub-area of research with its own associated international research conferences. Having taken on such a Herculean task, Kristian has produced a series of widely published results which are now at the core of a newly emerging area: Probabilistic Inductive Logic Programming. The new area is closely tied to, though strictly subsumes, a new field known as “Statistical Relational Learning” which has in the last few years gained major prominence in the American Artificial Intelligence research community. Within his thesis Kristian makes several major contributions, many of which have already been published in refereed conference and journal papers. Firstly, Kristian introduces a series of definitions which circumscribe the new area formed by extending Inductive Logic Programming to the case in which clauses are annotated with probability values. This represents a new and powerful framework which supersedes a number of influential papers and research areas in Artificial Intelligence. Secondly, Kristian introduces Bayesian Logic Programs (BLPs). These represent an elegantly defined lifting of Judea Pearl's Bayesian networks to the logic programming level. Since Kristian's introduction of BLPs, a number of results indicate that BLPs generalise many previously defined representations, not the least of which are Bayesian networks, Logic Programs, Probabilistic Relation Models and Stochastic Logic Programs. Next Kristian investigates the approach of Learning from proofs. This is an interesting new learning framework which is the first to go beyond the two standard semantic frameworks of Inductive Logic Programming. Kristian then looks at the problem of upgrading HMMs to logical HMMs. Hidden Markov Models (HMMs) are one of the most widely used machine learning technologies in Statistical Linguistics and Bioinformatics, and allow the representation of probabilistic finite automata. Kristian has upgraded standard HMMs to allow relational descriptions to be included within the description of the automata. The three standard HMM estimation algorithms are also upgraded. He has demonstrated the power of such representations using biological predictive modelling problems, and shown performance increases over alternative approaches. Kristian next considers the issue of upgrading Fisher Kernels to Relational Fisher kernels. Fisher kernels have been widely used within statistics and more recently in support vector machines. Buildin...", :doc-id "An inductive logic programming approach to statistical relational learning. 2006 Kristian Kersting"} #search_api.search_api.Paper{:id 2812915, :key "conf/sdm/DelaneyFCWJ10", :title "The Application of Statistical Relational Learning to a Database of Criminal and Terrorist Activity.", :abstract nil, :author (#search_api.search_api.Author{:id 64107, :first-name nil, :last-name nil, :full-name "B. Delaney"} #search_api.search_api.Author{:id 676653, :first-name nil, :last-name nil, :full-name "Andrew S. Fast"} #search_api.search_api.Author{:id 395451, :first-name nil, :last-name nil, :full-name "W. M. Campbell"} #search_api.search_api.Author{:id 661360, :first-name nil, :last-name nil, :full-name "C. J. Weinstein"} #search_api.search_api.Author{:id 870164, :first-name nil, :last-name nil, :full-name "David D. Jensen"}), :year 2010, :venue "SDM", :ncit 1, :string "The Application of Statistical Relational Learning to a Database of Criminal and Terrorist Activity.. ", :doc-id "The Application of Statistical Relational Learning to a Database of Criminal and Terrorist Activity. 2010 B. Delaney, Andrew S. Fast, W. M. Campbell, C. J. Weinstein, David D. Jensen"} #search_api.search_api.Paper{:id 1236661, :key "conf/RelMiCS/Muller09", :title "Modalities, Relations, and Learning.", :abstract "While the popularity of statistical, probabilistic and exhaustive machine learning techniques still increases, relational and logic approaches are still a niche market in research. While the former approaches focus on predictive accuracy, the latter ones prove to be indispensable in knowledge discovery.In this paper we present a relational description of machine learning problems. We demonstrate how common ensemble learning methods as used in classifier learning can be reformulated in a relational setting. It is shown that multimodal logics and relational data analysis with rough sets are closely related. Finally, we give an interpretation of logic programs as approximations of hypotheses.It is demonstrated that at a certain level of abstraction all these methods unify into one and the same formalisation which nicely connects to multimodal operators. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :author (#search_api.search_api.Author{:id 1161107, :first-name nil, :last-name nil, :full-name "Martin Eric Müller"}), :year 2009, :venue "RelMiCS", :ncit 0, :string "Modalities, Relations, and Learning.. While the popularity of statistical, probabilistic and exhaustive machine learning techniques still increases, relational and logic approaches are still a niche market in research. While the former approaches focus on predictive accuracy, the latter ones prove to be indispensable in knowledge discovery.In this paper we present a relational description of machine learning problems. We demonstrate how common ensemble learning methods as used in classifier learning can be reformulated in a relational setting. It is shown that multimodal logics and relational data analysis with rough sets are closely related. Finally, we give an interpretation of logic programs as approximations of hypotheses.It is demonstrated that at a certain level of abstraction all these methods unify into one and the same formalisation which nicely connects to multimodal operators. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :doc-id "Modalities, Relations, and Learning. 2009 Martin Eric Müller"} #search_api.search_api.Paper{:id 1009537, :key "journals/ml/Hofmann01", :title "Unsupervised Learning by Probabilistic Latent Semantic Analysis.", :abstract "This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.", :author (#search_api.search_api.Author{:id 8394, :first-name nil, :last-name nil, :full-name "Thomas Hofmann"}), :year 2001, :venue "Machine Learning", :ncit 2463, :string "Unsupervised Learning by Probabilistic Latent Semantic Analysis.. This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.", :doc-id "Unsupervised Learning by Probabilistic Latent Semantic Analysis. 2001 Thomas Hofmann"} #search_api.search_api.Paper{:id 541902, :key "conf/pkdd/Raedt05", :title "Statistical Relational Learning: An Inductive Logic Programming Perspective.", :abstract nil, :author (#search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2005, :venue "PKDD", :ncit 3, :string "Statistical Relational Learning: An Inductive Logic Programming Perspective.. ", :doc-id "Statistical Relational Learning: An Inductive Logic Programming Perspective. 2005 Luc De Raedt"} #search_api.search_api.Paper{:id 3503096, :key "conf/emnlp/VerbekeAMFDR12", :title "A Statistical Relational Learning Approach to Identifying Evidence Based Medicine Categories.", :abstract nil, :author (#search_api.search_api.Author{:id 14326996, :first-name nil, :last-name nil, :full-name "Mathias Verbeke"} #search_api.search_api.Author{:id 1380620, :first-name nil, :last-name nil, :full-name "Vincent Van Asch"} #search_api.search_api.Author{:id 468210, :first-name nil, :last-name nil, :full-name "Roser Morante"} #search_api.search_api.Author{:id 34394, :first-name nil, :last-name nil, :full-name "Paolo Frasconi"} #search_api.search_api.Author{:id 1014420, :first-name nil, :last-name nil, :full-name "Walter Daelemans"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2012, :venue "EMNLP-CoNLL", :ncit 5, :string "A Statistical Relational Learning Approach to Identifying Evidence Based Medicine Categories.. ", :doc-id "A Statistical Relational Learning Approach to Identifying Evidence Based Medicine Categories. 2012 Mathias Verbeke, Vincent Van Asch, Roser Morante, Paolo Frasconi, Walter Daelemans, Luc De Raedt"} #search_api.search_api.Paper{:id 473123, :key "conf/kdd/NevilleJFH03", :title "Learning relational probability trees.", :abstract "Classification trees are widely used in the machine learning and data mining communities for modeling propositional data. Recent work has extended this basic paradigm to probability estimation trees. Traditional tree learning algorithms assume that instances in the training data are homogenous and independently distributed. Relational probability trees (RPTs) extend standard probability estimation trees to a relational setting in which data instances are heterogeneous and interdependent. Our algorithm for learning the structure and parameters of an RPT searches over a space of relational features that use aggregation functions (e.g. AVERAGE, MODE, COUNT) to dynamically propositionalize relational data and create binary splits within the RPT. Previous work has identified a number of statistical biases due to characteristics of relational data such as autocorrelation and degree disparity. The RPT algorithm uses a novel form of randomization test to adjust for these biases. On a variety of relational learning tasks, RPTs built using randomization tests are significantly smaller than other models and achieve equivalent, or better, performance.", :author (#search_api.search_api.Author{:id 150768, :first-name nil, :last-name nil, :full-name "Jennifer Neville"} #search_api.search_api.Author{:id 408381, :first-name nil, :last-name nil, :full-name "David Jensen"} #search_api.search_api.Author{:id 1434755, :first-name nil, :last-name nil, :full-name "Lisa Friedland"} #search_api.search_api.Author{:id 575750, :first-name nil, :last-name nil, :full-name "Michael Hay"}), :year 2003, :venue "KDD", :ncit 191, :string "Learning relational probability trees.. Classification trees are widely used in the machine learning and data mining communities for modeling propositional data. Recent work has extended this basic paradigm to probability estimation trees. Traditional tree learning algorithms assume that instances in the training data are homogenous and independently distributed. Relational probability trees (RPTs) extend standard probability estimation trees to a relational setting in which data instances are heterogeneous and interdependent. Our algorithm for learning the structure and parameters of an RPT searches over a space of relational features that use aggregation functions (e.g. AVERAGE, MODE, COUNT) to dynamically propositionalize relational data and create binary splits within the RPT. Previous work has identified a number of statistical biases due to characteristics of relational data such as autocorrelation and degree disparity. The RPT algorithm uses a novel form of randomization test to adjust for these biases. On a variety of relational learning tasks, RPTs built using randomization tests are significantly smaller than other models and achieve equivalent, or better, performance.", :doc-id "Learning relational probability trees. 2003 Jennifer Neville, David Jensen, Lisa Friedland, Michael Hay"} #search_api.search_api.Paper{:id 950858, :key "journals/jacm/Kearns98", :title "Efficient Noise-Tolerant Learning from Statistical Queries.", :abstract "In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :author (#search_api.search_api.Author{:id 1249307, :first-name nil, :last-name nil, :full-name "Michael J. Kearns"}), :year 1998, :venue "J. ACM", :ncit 456, :string "Efficient Noise-Tolerant Learning from Statistical Queries.. In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :doc-id "Efficient Noise-Tolerant Learning from Statistical Queries. 1998 Michael J. Kearns"} #search_api.search_api.Paper{:id 2784531, :key "conf/ecai/JainBB10", :title "Adaptive Markov Logic Networks: Learning Statistical Relational Models with Dynamic Parameters.", :abstract "Statistical relational models, such as Markov logic networks, seek to compactly describe properties of relational domains by representing general principles about objects belonging to particular classes. Models are intended to be independent of the set of objects to which these principles can be applied, and it is assumed that the principles will soundly generalize across arbitrary sets of objects. In this paper, we point out limitations of models that seek to represent the corresponding principles with a fixed set of parameters and discuss the conditions under which the soundness of fixed parameters is indeed questionable. We propose a novel representation formalism called adaptive Markov logic networks to allow more flexible representations of relational domains, which involve parameters that are dynamically adjusted to fit the properties of an instantiation by phrasing the model's parameters as functions over attributes of the instantiation at hand. We empirically demonstrate the value of our learning and representation system on a simple but well-motivated example domain. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :author (#search_api.search_api.Author{:id 639688, :first-name nil, :last-name nil, :full-name "Dominik Jain"} #search_api.search_api.Author{:id 1005331, :first-name nil, :last-name nil, :full-name "Andreas Barthels"} #search_api.search_api.Author{:id 361314, :first-name nil, :last-name nil, :full-name "Michael Beetz"}), :year 2010, :venue "ECAI", :ncit 8, :string "Adaptive Markov Logic Networks: Learning Statistical Relational Models with Dynamic Parameters.. Statistical relational models, such as Markov logic networks, seek to compactly describe properties of relational domains by representing general principles about objects belonging to particular classes. Models are intended to be independent of the set of objects to which these principles can be applied, and it is assumed that the principles will soundly generalize across arbitrary sets of objects. In this paper, we point out limitations of models that seek to represent the corresponding principles with a fixed set of parameters and discuss the conditions under which the soundness of fixed parameters is indeed questionable. We propose a novel representation formalism called adaptive Markov logic networks to allow more flexible representations of relational domains, which involve parameters that are dynamically adjusted to fit the properties of an instantiation by phrasing the model's parameters as functions over attributes of the instantiation at hand. We empirically demonstrate the value of our learning and representation system on a simple but well-motivated example domain. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player", :doc-id "Adaptive Markov Logic Networks: Learning Statistical Relational Models with Dynamic Parameters. 2010 Dominik Jain, Andreas Barthels, Michael Beetz"} #search_api.search_api.Paper{:id 395262, :key "conf/ilp/SlatteryC98", :title "Combining Statistical and Relational Methods for Learning in Hypertext Domains.", :abstract nil, :author (#search_api.search_api.Author{:id 174721, :first-name nil, :last-name nil, :full-name "Seán Slattery"} #search_api.search_api.Author{:id 305395, :first-name nil, :last-name nil, :full-name "Mark Craven"}), :year 1998, :venue "ILP", :ncit 102, :string "Combining Statistical and Relational Methods for Learning in Hypertext Domains.. ", :doc-id "Combining Statistical and Relational Methods for Learning in Hypertext Domains. 1998 Seán Slattery, Mark Craven"} #search_api.search_api.Paper{:id 91401, :key "conf/chi/PatelFLH08", :title "Investigating statistical machine learning as a tool for software development.", :abstract "As statistical machine learning algorithms and techniques continue to mature, many researchers and developers see statistical machine learning not only as a topic of expert study, but also as a tool for software development. Extensive prior work has studied software development, but little prior work has studied software developers applying statistical machine learning. This paper presents interviews of eleven researchers experienced in applying statistical machine learning algorithms and techniques to human-computer interaction problems, as well as a study of ten participants working during a five-hour study to apply statistical machine learning algorithms and techniques to a realistic problem. We distill three related categories of difficulties that arise in applying statistical machine learning as a tool for software development: (1) difficulty pursuing statistical machine learning as an iterative and exploratory process, (2) difficulty understanding relationships between data and the behavior of statistical machine learning algorithms, and (3) difficulty evaluating the performance of statistical machine learning algorithms and techniques in the context of applications. This paper provides important new insight into these difficulties and the need for development tools that better support the application of statistical machine learning.", :author (#search_api.search_api.Author{:id 939325, :first-name nil, :last-name nil, :full-name "Kayur Patel"} #search_api.search_api.Author{:id 1505625, :first-name nil, :last-name nil, :full-name "James Fogarty"} #search_api.search_api.Author{:id 82771, :first-name nil, :last-name nil, :full-name "James A. Landay"} #search_api.search_api.Author{:id 907542, :first-name nil, :last-name nil, :full-name "Beverly L. Harrison"}), :year 2008, :venue "CHI", :ncit 22, :string "Investigating statistical machine learning as a tool for software development.. As statistical machine learning algorithms and techniques continue to mature, many researchers and developers see statistical machine learning not only as a topic of expert study, but also as a tool for software development. Extensive prior work has studied software development, but little prior work has studied software developers applying statistical machine learning. This paper presents interviews of eleven researchers experienced in applying statistical machine learning algorithms and techniques to human-computer interaction problems, as well as a study of ten participants working during a five-hour study to apply statistical machine learning algorithms and techniques to a realistic problem. We distill three related categories of difficulties that arise in applying statistical machine learning as a tool for software development: (1) difficulty pursuing statistical machine learning as an iterative and exploratory process, (2) difficulty understanding relationships between data and the behavior of statistical machine learning algorithms, and (3) difficulty evaluating the performance of statistical machine learning algorithms and techniques in the context of applications. This paper provides important new insight into these difficulties and the need for development tools that better support the application of statistical machine learning.", :doc-id "Investigating statistical machine learning as a tool for software development. 2008 Kayur Patel, James Fogarty, James A. Landay, Beverly L. Harrison"} #search_api.search_api.Paper{:id 3241642, :key "conf/nips/BroechelerG10", :title "Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning.", :abstract nil, :author (#search_api.search_api.Author{:id 86345, :first-name nil, :last-name nil, :full-name "Matthias Broecheler"} #search_api.search_api.Author{:id 1449720, :first-name nil, :last-name nil, :full-name "Lise Getoor"}), :year 2010, :venue "NIPS", :ncit 3, :string "Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning.. ", :doc-id "Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning. 2010 Matthias Broecheler, Lise Getoor"} #search_api.search_api.Paper{:id 190192, :key "conf/esws/KieferBL08", :title "Adding Data Mining Support to SPARQL Via Statistical Relational Learning Methods.", :abstract "Exploiting the complex structure of relational data enables to build better models by taking into account the additional information provided by the links between objects. We extend this idea to the Semantic Web by introducing our novel SPARQL-ML approach to perform data mining for Semantic Web data. Our approach is based on traditional SPARQL and statistical relational learning methods, such as Relational Probability Trees and Relational Bayesian Classifiers. We analyze our approach thoroughly conducting three sets of experiments on synthetic as well as real-world data sets. Our analytical results show that our approach can be used for any Semantic Web data set to perform instance-based learning and classification. A comparison to kernel methods used in Support Vector Machines shows that our approach is superior in terms of classification accuracy.", :author (#search_api.search_api.Author{:id 1506617, :first-name nil, :last-name nil, :full-name "Christoph Kiefer"} #search_api.search_api.Author{:id 798825, :first-name nil, :last-name nil, :full-name "Abraham Bernstein"} #search_api.search_api.Author{:id 322034, :first-name nil, :last-name nil, :full-name "André Locher"}), :year 2008, :venue "ESWC", :ncit 26, :string "Adding Data Mining Support to SPARQL Via Statistical Relational Learning Methods.. Exploiting the complex structure of relational data enables to build better models by taking into account the additional information provided by the links between objects. We extend this idea to the Semantic Web by introducing our novel SPARQL-ML approach to perform data mining for Semantic Web data. Our approach is based on traditional SPARQL and statistical relational learning methods, such as Relational Probability Trees and Relational Bayesian Classifiers. We analyze our approach thoroughly conducting three sets of experiments on synthetic as well as real-world data sets. Our analytical results show that our approach can be used for any Semantic Web data set to perform instance-based learning and classification. A comparison to kernel methods used in Support Vector Machines shows that our approach is superior in terms of classification accuracy.", :doc-id "Adding Data Mining Support to SPARQL Via Statistical Relational Learning Methods. 2008 Christoph Kiefer, Abraham Bernstein, André Locher"} #search_api.search_api.Paper{:id 3402586, :key "series/faia/2005-148", :title "An Inductive Logic Programming Approach to Statistical Relational Learning", :abstract nil, :author (#search_api.search_api.Author{:id 177502, :first-name nil, :last-name nil, :full-name "Kristian Kersting"}), :year 2005, :venue "Frontiers in Artificial Intelligence and Applications", :ncit 15, :string "An Inductive Logic Programming Approach to Statistical Relational Learning. ", :doc-id "An Inductive Logic Programming Approach to Statistical Relational Learning 2005 Kristian Kersting"} #search_api.search_api.Paper{:id 622461, :key "conf/stoc/Kearns93", :title "Efficient noise-tolerant learning from statistical queries.", :abstract "In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :author (#search_api.search_api.Author{:id 1249307, :first-name nil, :last-name nil, :full-name "Michael J. Kearns"}), :year 1993, :venue "STOC", :ncit 437, :string "Efficient noise-tolerant learning from statistical queries.. In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of &ldquo;robust&rdquo; learning algorithms in the most general way, we formalize a new but related model of learning from statistical queries. Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a noise rate approaching the information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.", :doc-id "Efficient noise-tolerant learning from statistical queries. 1993 Michael J. Kearns"} #search_api.search_api.Paper{:id 2927016, :key "journals/sigkdd/Landwehr09", :title "Trading expressivity for efficiency in statistical relational learning: Ph.D. thesis abstract.", :abstract nil, :author (#search_api.search_api.Author{:id 127590, :first-name nil, :last-name nil, :full-name "Niels Landwehr"}), :year 2009, :venue "SIGKDD Explorations", :ncit 0, :string "Trading expressivity for efficiency in statistical relational learning: Ph.D. thesis abstract.. ", :doc-id "Trading expressivity for efficiency in statistical relational learning: Ph.D. thesis abstract. 2009 Niels Landwehr"} #search_api.search_api.Paper{:id 3503505, :key "conf/iaai/WeissNPMP12", :title "Statistical Relational Learning to Predict Primary Myocardial Infarction from Electronic Health Records.", :abstract nil, :author (#search_api.search_api.Author{:id 14327248, :first-name nil, :last-name nil, :full-name "Jeremy C. Weiss"} #search_api.search_api.Author{:id 883462, :first-name nil, :last-name nil, :full-name "Sriraam Natarajan"} #search_api.search_api.Author{:id 14259909, :first-name nil, :last-name nil, :full-name "Peggy L. Peissig"} #search_api.search_api.Author{:id 14259908, :first-name nil, :last-name nil, :full-name "Catherine A. McCarty"} #search_api.search_api.Author{:id 14312141, :first-name nil, :last-name nil, :full-name "David Page"}), :year 2012, :venue "IAAI", :ncit 0, :string "Statistical Relational Learning to Predict Primary Myocardial Infarction from Electronic Health Records.. ", :doc-id "Statistical Relational Learning to Predict Primary Myocardial Infarction from Electronic Health Records. 2012 Jeremy C. Weiss, Sriraam Natarajan, Peggy L. Peissig, Catherine A. McCarty, David Page"} #search_api.search_api.Paper{:id 1009754, :key "journals/ml/PerlichP06", :title "Distribution-based aggregation for relational learning with identifier attributes.", :abstract "Identifier attributes--very high-dimensional categorical attributes such as particular product ids or people's names--rarely are incorporated in statistical modeling. However, they can play an important role in relational modeling: it may be informative to have communicated with a particular set of people or to have purchased a particular set of products. A key limitation of existing relational modeling techniques is how they aggregate bags (multisets) of values from related entities. The aggregations used by existing methods are simple summaries of the distributions of features of related entities: e.g., MEAN, MODE, SUM, or COUNT. This paper's main contribution is the introduction of aggregation operators that capture more information about the value distributions, by storing meta-data about value distributions and referencing this meta-data when aggregating--for example by computing class-conditional distributional distances. Such aggregations are particularly important for aggregating values from high-dimensional categorical attributes, for which the simple aggregates provide little information. In the first half of the paper we provide general guidelines for designing aggregation operators, introduce the new aggregators in the context of the relational learning system ACORA (Automated Construction of Relational Attributes), and provide theoretical justification. We also conjecture special properties of identifier attributes, e.g., they proxy for unobserved attributes and for information deeper in the relationship network. In the second half of the paper we provide extensive empirical evidence that the distribution-based aggregators indeed do facilitate modeling with high-dimensional categorical attributes, and in support of the aforementioned conjectures.", :author (#search_api.search_api.Author{:id 445653, :first-name nil, :last-name nil, :full-name "Claudia Perlich"} #search_api.search_api.Author{:id 976323, :first-name nil, :last-name nil, :full-name "Foster J. Provost"}), :year 2006, :venue "Machine Learning", :ncit 54, :string "Distribution-based aggregation for relational learning with identifier attributes.. Identifier attributes--very high-dimensional categorical attributes such as particular product ids or people's names--rarely are incorporated in statistical modeling. However, they can play an important role in relational modeling: it may be informative to have communicated with a particular set of people or to have purchased a particular set of products. A key limitation of existing relational modeling techniques is how they aggregate bags (multisets) of values from related entities. The aggregations used by existing methods are simple summaries of the distributions of features of related entities: e.g., MEAN, MODE, SUM, or COUNT. This paper's main contribution is the introduction of aggregation operators that capture more information about the value distributions, by storing meta-data about value distributions and referencing this meta-data when aggregating--for example by computing class-conditional distributional distances. Such aggregations are particularly important for aggregating values from high-dimensional categorical attributes, for which the simple aggregates provide little information. In the first half of the paper we provide general guidelines for designing aggregation operators, introduce the new aggregators in the context of the relational learning system ACORA (Automated Construction of Relational Attributes), and provide theoretical justification. We also conjecture special properties of identifier attributes, e.g., they proxy for unobserved attributes and for information deeper in the relationship network. In the second half of the paper we provide extensive empirical evidence that the distribution-based aggregators indeed do facilitate modeling with high-dimensional categorical attributes, and in support of the aforementioned conjectures.", :doc-id "Distribution-based aggregation for relational learning with identifier attributes. 2006 Claudia Perlich, Foster J. Provost"} #search_api.search_api.Paper{:id 1033319, :key "journals/pami/WechslerDLC04", :title "Motion Estimation Using Statistical Learning Theory.", :abstract "Abstract--This paper describes a novel application of Statistical Learning Theory (SLT) to single motion estimation and tracking. The problem of motion estimation can be related to statistical model selection, where the goal is to select one (correct) motion model from several possible motion models, given finite noisy samples. SLT, also known as Vapnik-Chervonenkis (VC), theory provides analytic generalization bounds for model selection, which have been used successfully for practical model selection. This paper describes a successful application of an SLT-based model selection approach to the challenging problem of estimating optimal motion models from small data sets of image measurements (flow). We present results of experiments on both synthetic and real image sequences for motion interpolation and extrapolation; these results demonstrate the feasibility and strength of our approach. Our experimental results show that for motion estimation applications, SLT-based model selection compares favorably against alternative model selection methods, such as the Akaike's fpe, Schwartz' criterion (sc), Generalized Cross-Validation (gcv), and Shibata's Model Selector (sms). The paper also shows how to address the aperture problem using SLT-based model selection for penalized linear (ridge regression) formulation.", :author (#search_api.search_api.Author{:id 1499994, :first-name nil, :last-name nil, :full-name "Harry Wechsler"} #search_api.search_api.Author{:id 398169, :first-name nil, :last-name nil, :full-name "Zoran Duric"} #search_api.search_api.Author{:id 1493163, :first-name nil, :last-name nil, :full-name "Fayin Li"} #search_api.search_api.Author{:id 1104310, :first-name nil, :last-name nil, :full-name "Vladimir Cherkassky"}), :year 2004, :venue "IEEE Trans. Pattern Anal. Mach. Intell.", :ncit 50, :string "Motion Estimation Using Statistical Learning Theory.. Abstract--This paper describes a novel application of Statistical Learning Theory (SLT) to single motion estimation and tracking. The problem of motion estimation can be related to statistical model selection, where the goal is to select one (correct) motion model from several possible motion models, given finite noisy samples. SLT, also known as Vapnik-Chervonenkis (VC), theory provides analytic generalization bounds for model selection, which have been used successfully for practical model selection. This paper describes a successful application of an SLT-based model selection approach to the challenging problem of estimating optimal motion models from small data sets of image measurements (flow). We present results of experiments on both synthetic and real image sequences for motion interpolation and extrapolation; these results demonstrate the feasibility and strength of our approach. Our experimental results show that for motion estimation applications, SLT-based model selection compares favorably against alternative model selection methods, such as the Akaike's fpe, Schwartz' criterion (sc), Generalized Cross-Validation (gcv), and Shibata's Model Selector (sms). The paper also shows how to address the aperture problem using SLT-based model selection for penalized linear (ridge regression) formulation.", :doc-id "Motion Estimation Using Statistical Learning Theory. 2004 Harry Wechsler, Zoran Duric, Fayin Li, Vladimir Cherkassky"} #search_api.search_api.Paper{:id 3179782, :key "conf/ijcai/ZhangCJXF11", :title "Learning Inter-Related Statistical Query Translation Models for English-Chinese Bi-Directional CLIR.", :abstract nil, :author (#search_api.search_api.Author{:id 298970, :first-name nil, :last-name nil, :full-name "Yuejie Zhang"} #search_api.search_api.Author{:id 1549539, :first-name nil, :last-name nil, :full-name "Lei Cen"} #search_api.search_api.Author{:id 1444598, :first-name nil, :last-name nil, :full-name "Cheng Jin"} #search_api.search_api.Author{:id 323748, :first-name nil, :last-name nil, :full-name "Xiangyang Xue"} #search_api.search_api.Author{:id 55229, :first-name nil, :last-name nil, :full-name "Jianping Fan"}), :year 2011, :venue "IJCAI", :ncit 0, :string "Learning Inter-Related Statistical Query Translation Models for English-Chinese Bi-Directional CLIR.. ", :doc-id "Learning Inter-Related Statistical Query Translation Models for English-Chinese Bi-Directional CLIR. 2011 Yuejie Zhang, Lei Cen, Cheng Jin, Xiangyang Xue, Jianping Fan"} #search_api.search_api.Paper{:id 644121, :key "conf/vldb/ZhangHJLZ05", :title "Statistical Learning Techniques for Costing XML Queries.", :abstract "Developing cost models for query optimization is significantly harder for XML queries than for traditional relational queries. The reason is that XML query operators are much more complex than relational operators such as table scans and joins. In this paper, we propose a new approach, called COMET, to modeling the cost of XML operators; to our knowledge, COMET is the first method ever proposed for addressing the XML query costing problem. As in relational cost estimation, COMET exploits a set of system catalog statistics that summarizes the XML data; the set of \"simple path\" statistics that we propose is new, and is well suited to the XML setting. Unlike the traditional approach, COMET uses a new statistical learning technique called \"transform regression\" instead of detailed analytical models to predict the overall cost. Besides rendering the cost estimation problem tractable for XML queries, COMET has the further advantage of enabling the query optimizer to be self-tuning, automatically adapting to changes over time in the query workload and in the system environment. We demonstrate COMET's feasibility by developing a cost model for the recently proposed XNAV navigational operator. Empirical studies with synthetic, benchmark, and real-world data sets show that COMET can quickly obtain accurate cost estimates for a variety of XML queries and data sets.", :author (#search_api.search_api.Author{:id 39154, :first-name nil, :last-name nil, :full-name "Ning Zhang"} #search_api.search_api.Author{:id 832799, :first-name nil, :last-name nil, :full-name "Peter J. Haas"} #search_api.search_api.Author{:id 875466, :first-name nil, :last-name nil, :full-name "Vanja Josifovski"} #search_api.search_api.Author{:id 119924, :first-name nil, :last-name nil, :full-name "Guy M. Lohman"} #search_api.search_api.Author{:id 691076, :first-name nil, :last-name nil, :full-name "Chun Zhang"}), :year 2005, :venue "VLDB", :ncit 56, :string "Statistical Learning Techniques for Costing XML Queries.. Developing cost models for query optimization is significantly harder for XML queries than for traditional relational queries. The reason is that XML query operators are much more complex than relational operators such as table scans and joins. In this paper, we propose a new approach, called COMET, to modeling the cost of XML operators; to our knowledge, COMET is the first method ever proposed for addressing the XML query costing problem. As in relational cost estimation, COMET exploits a set of system catalog statistics that summarizes the XML data; the set of \"simple path\" statistics that we propose is new, and is well suited to the XML setting. Unlike the traditional approach, COMET uses a new statistical learning technique called \"transform regression\" instead of detailed analytical models to predict the overall cost. Besides rendering the cost estimation problem tractable for XML queries, COMET has the further advantage of enabling the query optimizer to be self-tuning, automatically adapting to changes over time in the query workload and in the system environment. We demonstrate COMET's feasibility by developing a cost model for the recently proposed XNAV navigational operator. Empirical studies with synthetic, benchmark, and real-world data sets show that COMET can quickly obtain accurate cost estimates for a variety of XML queries and data sets.", :doc-id "Statistical Learning Techniques for Costing XML Queries. 2005 Ning Zhang, Peter J. Haas, Vanja Josifovski, Guy M. Lohman, Chun Zhang"} #search_api.search_api.Paper{:id 391820, :key "conf/ijcai/Raedt07", :title "Statistical Relational Learning - A Logical Approach (Abstract of Invited Talk).", :abstract nil, :author (#search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2007, :venue "NeSy", :ncit 0, :string "Statistical Relational Learning - A Logical Approach (Abstract of Invited Talk).. ", :doc-id "Statistical Relational Learning - A Logical Approach (Abstract of Invited Talk). 2007 Luc De Raedt"} #search_api.search_api.Paper{:id 294586, :key "conf/icdar/CeciBM05", :title "Relational Learning techniques for Document Image Understanding: Comparing Statistical and Logical approaches.", :abstract "In this paper, we evaluate and systematically compare two different (multi-)relational learning methods based on a statistical approach and a logical approach for the task of document image understanding. For a fair comparison, both methods are tested on the same real world dataset consisting of multipage articles published in an international journal. An analysis of pros and cons of both approaches is reported.", :author (#search_api.search_api.Author{:id 1294148, :first-name nil, :last-name nil, :full-name "Michelangelo Ceci"} #search_api.search_api.Author{:id 1385113, :first-name nil, :last-name nil, :full-name "Margherita Berardi"} #search_api.search_api.Author{:id 282116, :first-name nil, :last-name nil, :full-name "Donato Malerba"}), :year 2005, :venue "ICDAR", :ncit 3, :string "Relational Learning techniques for Document Image Understanding: Comparing Statistical and Logical approaches.. In this paper, we evaluate and systematically compare two different (multi-)relational learning methods based on a statistical approach and a logical approach for the task of document image understanding. For a fair comparison, both methods are tested on the same real world dataset consisting of multipage articles published in an international journal. An analysis of pros and cons of both approaches is reported.", :doc-id "Relational Learning techniques for Document Image Understanding: Comparing Statistical and Logical approaches. 2005 Michelangelo Ceci, Margherita Berardi, Donato Malerba"} #search_api.search_api.Paper{:id 511029, :key "conf/naacl/KoehnOM03", :title "Statistical Phrase-Based Translation.", :abstract "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", :author (#search_api.search_api.Author{:id 1407757, :first-name nil, :last-name nil, :full-name "Philipp Koehn"} #search_api.search_api.Author{:id 1493982, :first-name nil, :last-name nil, :full-name "Franz Josef Och"} #search_api.search_api.Author{:id 911330, :first-name nil, :last-name nil, :full-name "Daniel Marcu"}), :year 2003, :venue "HLT-NAACL", :ncit 1866, :string "Statistical Phrase-Based Translation.. We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", :doc-id "Statistical Phrase-Based Translation. 2003 Philipp Koehn, Franz Josef Och, Daniel Marcu"} #search_api.search_api.Paper{:id 1032117, :key "journals/pami/KonishiYCZ03", :title "Statistical Edge Detection: Learning and Evaluating Edge Cues.", :abstract "We formulate edge detection as statistical inference. This statistical edge detection is data driven, unlike standard methods for edge detection which are model based. For any set of edge detection filters (implementing local edge cues), we use presegmented images to learn the probability distributions of filter responses conditioned on whether they are evaluated on or off an edge. Edge detection is formulated as a discrimination task specified by a likelihood ratio test on the filter responses. This approach emphasizes the necessity of modeling the image background (the off-edges). We represent the conditional probability distributions nonparametrically and illustrate them on two different data sets of 100 (Sowerby) and 50 (South Florida) images. Multiple edges cues, including chrominance and multiple-scale, are combined by using their joint distributions. Hence, this cue combination is optimal in the statistical sense. We evaluate the effectiveness of different visual cues using the Chernoff information and Receiver Operator Characteristic (ROC) curves. This shows that our approach gives quantitatively better results than the Canny edge detector when the image background contains significant clutter. In addition, it enables us to determine the effectiveness of different edge cues and gives quantitative measures for the advantages of multilevel processing, for the use of chrominance, and for the relative effectiveness of different detectors. Furthermore, we show that we can learn these conditional distributions on one data set and adapt them to the other with only slight degradation of performance without knowing the ground truth on the second data set. This shows that our results are not purely domain specific. We apply the same approach to the spatial grouping of edge cues and obtain analogies to nonmaximal suppression and hysteresis.", :author (#search_api.search_api.Author{:id 696497, :first-name nil, :last-name nil, :full-name "Scott Konishi"} #search_api.search_api.Author{:id 248255, :first-name nil, :last-name nil, :full-name "Alan L. Yuille"} #search_api.search_api.Author{:id 589169, :first-name nil, :last-name nil, :full-name "James M. Coughlan"} #search_api.search_api.Author{:id 589996, :first-name nil, :last-name nil, :full-name "Song Chun Zhu"}), :year 2003, :venue "IEEE Trans. Pattern Anal. Mach. Intell.", :ncit 187, :string "Statistical Edge Detection: Learning and Evaluating Edge Cues.. We formulate edge detection as statistical inference. This statistical edge detection is data driven, unlike standard methods for edge detection which are model based. For any set of edge detection filters (implementing local edge cues), we use presegmented images to learn the probability distributions of filter responses conditioned on whether they are evaluated on or off an edge. Edge detection is formulated as a discrimination task specified by a likelihood ratio test on the filter responses. This approach emphasizes the necessity of modeling the image background (the off-edges). We represent the conditional probability distributions nonparametrically and illustrate them on two different data sets of 100 (Sowerby) and 50 (South Florida) images. Multiple edges cues, including chrominance and multiple-scale, are combined by using their joint distributions. Hence, this cue combination is optimal in the statistical sense. We evaluate the effectiveness of different visual cues using the Chernoff information and Receiver Operator Characteristic (ROC) curves. This shows that our approach gives quantitatively better results than the Canny edge detector when the image background contains significant clutter. In addition, it enables us to determine the effectiveness of different edge cues and gives quantitative measures for the advantages of multilevel processing, for the use of chrominance, and for the relative effectiveness of different detectors. Furthermore, we show that we can learn these conditional distributions on one data set and adapt them to the other with only slight degradation of performance without knowing the ground truth on the second data set. This shows that our results are not purely domain specific. We apply the same approach to the spatial grouping of edge cues and obtain analogies to nonmaximal suppression and hysteresis.", :doc-id "Statistical Edge Detection: Learning and Evaluating Edge Cues. 2003 Scott Konishi, Alan L. Yuille, James M. Coughlan, Song Chun Zhu"} #search_api.search_api.Paper{:id 395451, :key "conf/ilp/SaittaV08", :title "A Comparison between Two Statistical Relational Models.", :abstract "Statistical Relational Learning has received much attention this last decade. In the ILP community, several models have emerged for modelling and learning uncertain knowledge, expressed in subset of first order logics. Nevertheless, no deep comparisons have been made among them and, given an application, determining which model must be chosen is difficult. In this paper, we compare two of them, namely Markov Logic Networks and Bayesian Programs, especially with respect to their representation ability and inference methods. The comparison shows that the two models are substantially different, from the point of view of the user, so that choosing one really means choosing a different philosophy to look at the problem. In order to make the comparison more concrete, we have used a running example, which shows most of the interesting points of the approaches, yet remaining exactly tractable.", :author (#search_api.search_api.Author{:id 176408, :first-name nil, :last-name nil, :full-name "Lorenza Saitta"} #search_api.search_api.Author{:id 99042, :first-name nil, :last-name nil, :full-name "Christel Vrain"}), :year 2008, :venue "ILP", :ncit 1, :string "A Comparison between Two Statistical Relational Models.. Statistical Relational Learning has received much attention this last decade. In the ILP community, several models have emerged for modelling and learning uncertain knowledge, expressed in subset of first order logics. Nevertheless, no deep comparisons have been made among them and, given an application, determining which model must be chosen is difficult. In this paper, we compare two of them, namely Markov Logic Networks and Bayesian Programs, especially with respect to their representation ability and inference methods. The comparison shows that the two models are substantially different, from the point of view of the user, so that choosing one really means choosing a different philosophy to look at the problem. In order to make the comparison more concrete, we have used a running example, which shows most of the interesting points of the approaches, yet remaining exactly tractable.", :doc-id "A Comparison between Two Statistical Relational Models. 2008 Lorenza Saitta, Christel Vrain"} #search_api.search_api.Paper{:id 2800113, :key "conf/ilp/MihalkovaR09", :title "Speeding Up Inference in Statistical Relational Learning by Clustering Similar Query Literals.", :abstract nil, :author (#search_api.search_api.Author{:id 421571, :first-name nil, :last-name nil, :full-name "Lilyana Mihalkova"} #search_api.search_api.Author{:id 709717, :first-name nil, :last-name nil, :full-name "Matthew Richardson"}), :year 2009, :venue "ILP", :ncit 10, :string "Speeding Up Inference in Statistical Relational Learning by Clustering Similar Query Literals.. ", :doc-id "Speeding Up Inference in Statistical Relational Learning by Clustering Similar Query Literals. 2009 Lilyana Mihalkova, Matthew Richardson"} #search_api.search_api.Paper{:id 985236, :key "journals/jmlr/PasseriniFR06", :title "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.", :abstract "We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :author (#search_api.search_api.Author{:id 1520074, :first-name nil, :last-name nil, :full-name "Andrea Passerini"} #search_api.search_api.Author{:id 34394, :first-name nil, :last-name nil, :full-name "Paolo Frasconi"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2006, :venue "Journal of Machine Learning Research", :ncit 31, :string "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.. We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :doc-id "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting. 2006 Andrea Passerini, Paolo Frasconi, Luc De Raedt"} #search_api.search_api.Paper{:id 949799, :key "journals/jacm/BlumKW03", :title "Noise-tolerant learning, the parity problem, and the statistical query model.", :abstract "We describe a slightly subexponential time algorithm for learning parity functions in the presence of random classification noise, a problem closely related to several cryptographic and coding problems. Our algorithm runs in polynomial time for the case of parity functions that depend on only the first O(log n log log n) bits of input, which provides the first known instance of an efficient noise-tolerant algorithm for a concept class that is not learnable in the Statistical Query model of Kearns [1998]. Thus, we demonstrate that the set of problems learnable in the statistical query model is a strict subset of those problems learnable in the presence of noise in the PAC model.In coding-theory terms, what we give is a poly(n)-time algorithm for decoding linear k &times; n codes in the presence of random noise for the case of k = c log n log log n for some c > 0. (The case of k = O(log n) is trivial since one can just individually check each of the 2k possible messages and choose the one that yields the closest codeword.)A natural extension of the statistical query model is to allow queries about statistical properties that involve t-tuples of examples, as opposed to just single examples. The second result of this article is to show that any class of functions learnable (strongly or weakly) with t-wise queries for t = O(log n) is also weakly learnable with standard unary queries. Hence, this natural extension to the statistical query model does not increase the set of weakly learnable functions.", :author (#search_api.search_api.Author{:id 79111, :first-name nil, :last-name nil, :full-name "Avrim Blum"} #search_api.search_api.Author{:id 281950, :first-name nil, :last-name nil, :full-name "Adam Kalai"} #search_api.search_api.Author{:id 248558, :first-name nil, :last-name nil, :full-name "Hal Wasserman"}), :year 2003, :venue "J. ACM", :ncit 232, :string "Noise-tolerant learning, the parity problem, and the statistical query model.. We describe a slightly subexponential time algorithm for learning parity functions in the presence of random classification noise, a problem closely related to several cryptographic and coding problems. Our algorithm runs in polynomial time for the case of parity functions that depend on only the first O(log n log log n) bits of input, which provides the first known instance of an efficient noise-tolerant algorithm for a concept class that is not learnable in the Statistical Query model of Kearns [1998]. Thus, we demonstrate that the set of problems learnable in the statistical query model is a strict subset of those problems learnable in the presence of noise in the PAC model.In coding-theory terms, what we give is a poly(n)-time algorithm for decoding linear k &times; n codes in the presence of random noise for the case of k = c log n log log n for some c > 0. (The case of k = O(log n) is trivial since one can just individually check each of the 2k possible messages and choose the one that yields the closest codeword.)A natural extension of the statistical query model is to allow queries about statistical properties that involve t-tuples of examples, as opposed to just single examples. The second result of this article is to show that any class of functions learnable (strongly or weakly) with t-wise queries for t = O(log n) is also weakly learnable with standard unary queries. Hence, this natural extension to the statistical query model does not increase the set of weakly learnable functions.", :doc-id "Noise-tolerant learning, the parity problem, and the statistical query model. 2003 Avrim Blum, Adam Kalai, Hal Wasserman"} #search_api.search_api.Paper{:id 675364, :key "conf/www/QinLZWXL08", :title "Learning to rank relational objects and its application to web search.", :abstract "Learning to rank is a new statistical learning technology on creating a ranking model for sorting objects. The technology has been successfully applied to web search, and is becoming one of the key machineries for building search engines. Existing approaches to learning to rank, however, did not consider the cases in which there exists relationship between the objects to be ranked, despite of the fact that such situations are very common in practice. For example, in web search, given a query certain relationships usually exist among the the retrieved documents, e.g., URL hierarchy, similarity, etc., and sometimes it is necessary to utilize the information in ranking of the documents. This paper addresses the issue and formulates it as a novel learning problem, referred to as, 'learning to rank relational objects'. In the new learning task, the ranking model is defined as a function of not only the contents (features) of objects but also the relations between objects. The paper further focuses on one setting of the learning problem in which the way of using relation information is predetermined. It formalizes the learning task as an optimization problem in the setting. The paper then proposes a new method to perform the optimization task, particularly an implementation based on SVM. Experimental results show that the proposed method outperforms the baseline methods for two ranking tasks (Pseudo Relevance Feedback and Topic Distillation) in web search, indicating that the proposed method can indeed make effective use of relation information and content information in ranking.", :author (#search_api.search_api.Author{:id 17439, :first-name nil, :last-name nil, :full-name "Tao Qin"} #search_api.search_api.Author{:id 1411681, :first-name nil, :last-name nil, :full-name "Tie-Yan Liu"} #search_api.search_api.Author{:id 981048, :first-name nil, :last-name nil, :full-name "Xu-Dong Zhang"} #search_api.search_api.Author{:id 566222, :first-name nil, :last-name nil, :full-name "De-Sheng Wang"} #search_api.search_api.Author{:id 1482892, :first-name nil, :last-name nil, :full-name "Wen-Ying Xiong"} #search_api.search_api.Author{:id 742456, :first-name nil, :last-name nil, :full-name "Hang Li"}), :year 2008, :venue "WWW", :ncit 62, :string "Learning to rank relational objects and its application to web search.. Learning to rank is a new statistical learning technology on creating a ranking model for sorting objects. The technology has been successfully applied to web search, and is becoming one of the key machineries for building search engines. Existing approaches to learning to rank, however, did not consider the cases in which there exists relationship between the objects to be ranked, despite of the fact that such situations are very common in practice. For example, in web search, given a query certain relationships usually exist among the the retrieved documents, e.g., URL hierarchy, similarity, etc., and sometimes it is necessary to utilize the information in ranking of the documents. This paper addresses the issue and formulates it as a novel learning problem, referred to as, 'learning to rank relational objects'. In the new learning task, the ranking model is defined as a function of not only the contents (features) of objects but also the relations between objects. The paper further focuses on one setting of the learning problem in which the way of using relation information is predetermined. It formalizes the learning task as an optimization problem in the setting. The paper then proposes a new method to perform the optimization task, particularly an implementation based on SVM. Experimental results show that the proposed method outperforms the baseline methods for two ranking tasks (Pseudo Relevance Feedback and Topic Distillation) in web search, indicating that the proposed method can indeed make effective use of relation information and content information in ranking.", :doc-id "Learning to rank relational objects and its application to web search. 2008 Tao Qin, Tie-Yan Liu, Xu-Dong Zhang, De-Sheng Wang, Wen-Ying Xiong, Hang Li"} #search_api.search_api.Paper{:id 303622, :key "conf/icdm/XiangN08", :title "Pseudolikelihood EM for Within-network Relational Learning.", :abstract "In this work, we study the problem of \\emph{within-network} relational learning and inference, where models are learned on a partially labeled relational dataset and then are applied to predict the classes of unlabeled instances in the same graph. We categorize recent work in statistical relational learning into three alternative approaches for this setting: disjoint learning with disjoint inference, disjoint learning with collective inference, and collective learning with collective inference. Models from each of these categories has been employed previously in different settings, but to our knowledge there has been no systematic comparison of models from all three categories. In this paper, we develop a novel pseudolikelihood EM method that facilitates more general \\emph{collective learning} and \\emph{collective inference} on partially labeled relational networks. We then compare this method to competing methods from the other categories on both synthetic and real-world data. We show that collective learning and inference with the pseudolikelihood EM approach achieves significantly higher accuracy than the other types of models when there are a moderate number of labeled examples in the data graph.", :author (#search_api.search_api.Author{:id 819343, :first-name nil, :last-name nil, :full-name "Rongjing Xiang"} #search_api.search_api.Author{:id 150768, :first-name nil, :last-name nil, :full-name "Jennifer Neville"}), :year 2008, :venue "ICDM", :ncit 9, :string "Pseudolikelihood EM for Within-network Relational Learning.. In this work, we study the problem of \\emph{within-network} relational learning and inference, where models are learned on a partially labeled relational dataset and then are applied to predict the classes of unlabeled instances in the same graph. We categorize recent work in statistical relational learning into three alternative approaches for this setting: disjoint learning with disjoint inference, disjoint learning with collective inference, and collective learning with collective inference. Models from each of these categories has been employed previously in different settings, but to our knowledge there has been no systematic comparison of models from all three categories. In this paper, we develop a novel pseudolikelihood EM method that facilitates more general \\emph{collective learning} and \\emph{collective inference} on partially labeled relational networks. We then compare this method to competing methods from the other categories on both synthetic and real-world data. We show that collective learning and inference with the pseudolikelihood EM approach achieves significantly higher accuracy than the other types of models when there are a moderate number of labeled examples in the data graph.", :doc-id "Pseudolikelihood EM for Within-network Relational Learning. 2008 Rongjing Xiang, Jennifer Neville"} #search_api.search_api.Paper{:id 621480, :key "conf/stoc/BlumKW00", :title "Noise-tolerant learning, the parity problem, and the statistical query model.", :abstract "We describe a slightly subexponential time algorithm for learning parity functions in the presence of random classification noise, a problem closely related to several cryptographic and coding problems. Our algorithm runs in polynomial time for the case of parity functions that depend on only the first O(log n log log n) bits of input, which provides the first known instance of an efficient noise-tolerant algorithm for a concept class that is not learnable in the Statistical Query model of Kearns [1998]. Thus, we demonstrate that the set of problems learnable in the statistical query model is a strict subset of those problems learnable in the presence of noise in the PAC model.In coding-theory terms, what we give is a poly(n)-time algorithm for decoding linear k &times; n codes in the presence of random noise for the case of k = c log n log log n for some c > 0. (The case of k = O(log n) is trivial since one can just individually check each of the 2k possible messages and choose the one that yields the closest codeword.)A natural extension of the statistical query model is to allow queries about statistical properties that involve t-tuples of examples, as opposed to just single examples. The second result of this article is to show that any class of functions learnable (strongly or weakly) with t-wise queries for t = O(log n) is also weakly learnable with standard unary queries. Hence, this natural extension to the statistical query model does not increase the set of weakly learnable functions.", :author (#search_api.search_api.Author{:id 79111, :first-name nil, :last-name nil, :full-name "Avrim Blum"} #search_api.search_api.Author{:id 281950, :first-name nil, :last-name nil, :full-name "Adam Kalai"} #search_api.search_api.Author{:id 248558, :first-name nil, :last-name nil, :full-name "Hal Wasserman"}), :year 2000, :venue "STOC", :ncit 232, :string "Noise-tolerant learning, the parity problem, and the statistical query model.. We describe a slightly subexponential time algorithm for learning parity functions in the presence of random classification noise, a problem closely related to several cryptographic and coding problems. Our algorithm runs in polynomial time for the case of parity functions that depend on only the first O(log n log log n) bits of input, which provides the first known instance of an efficient noise-tolerant algorithm for a concept class that is not learnable in the Statistical Query model of Kearns [1998]. Thus, we demonstrate that the set of problems learnable in the statistical query model is a strict subset of those problems learnable in the presence of noise in the PAC model.In coding-theory terms, what we give is a poly(n)-time algorithm for decoding linear k &times; n codes in the presence of random noise for the case of k = c log n log log n for some c > 0. (The case of k = O(log n) is trivial since one can just individually check each of the 2k possible messages and choose the one that yields the closest codeword.)A natural extension of the statistical query model is to allow queries about statistical properties that involve t-tuples of examples, as opposed to just single examples. The second result of this article is to show that any class of functions learnable (strongly or weakly) with t-wise queries for t = O(log n) is also weakly learnable with standard unary queries. Hence, this natural extension to the statistical query model does not increase the set of weakly learnable functions.", :doc-id "Noise-tolerant learning, the parity problem, and the statistical query model. 2000 Avrim Blum, Adam Kalai, Hal Wasserman"} #search_api.search_api.Paper{:id 2805360, :key "conf/kdd/2009srb", :title "Proceedings of the ACM SIGKDD Workshop on Statistical and Relational Learning in Bioinformatics, Paris, France, June 28, 2009", :abstract nil, :author (#search_api.search_api.Author{:id 981793, :first-name nil, :last-name nil, :full-name "Christophe Costa Florêncio"} #search_api.search_api.Author{:id 892400, :first-name nil, :last-name nil, :full-name "Fabrizio Costa"} #search_api.search_api.Author{:id 1015109, :first-name nil, :last-name nil, :full-name "Jan Ramon"} #search_api.search_api.Author{:id 19675, :first-name nil, :last-name nil, :full-name "Joost N. Kok"}), :year 2009, :venue "KDD Workshop on Statistical and Relational Learning in Bioinformatics", :ncit 0, :string "Proceedings of the ACM SIGKDD Workshop on Statistical and Relational Learning in Bioinformatics, Paris, France, June 28, 2009. ", :doc-id "Proceedings of the ACM SIGKDD Workshop on Statistical and Relational Learning in Bioinformatics, Paris, France, June 28, 2009 2009 Christophe Costa Florêncio, Fabrizio Costa, Jan Ramon, Joost N. Kok"} #search_api.search_api.Paper{:id 1319504, :key "conf/icdm/NevilleGE09", :title "Evaluating Statistical Tests for Within-Network Classifiers of Relational Data.", :abstract "Recently a number of modeling techniques have been developed for data mining and machine learning in relational and network domains where the instances are not independent and identically distributed (i.i.d.). These methods specifically exploit the statistical dependencies among instances in order to improve classification accuracy. However, there has been little focus on how these same dependencies affect our ability to draw accurate conclusions about the performance of the models. More specifically, the complex link structure and attribute dependencies in network data violate the assumptions of many conventional statistical tests and make it difficult to use these tests to assess the models in an unbiased manner. In this work, we examine the task of within-network classification and the question of whether two algorithms will learn models which will result in significantly different levels of performance. We show that the commonly-used form of evaluation (paired t-test on overlapping network samples) can result in an unacceptable level of Type I error. Furthermore we show that Type I error increases as (1) the correlation among instances increases and (2) the size of the evaluation set increases (i.e., the proportion of labeled nodes in the network decreases). We propose a method for network cross-validation that combined with paired t-tests produces more acceptable levels of Type I error while still providing reasonable levels of statistical power (i.e., Type II error).", :author (#search_api.search_api.Author{:id 150768, :first-name nil, :last-name nil, :full-name "Jennifer Neville"} #search_api.search_api.Author{:id 335801, :first-name nil, :last-name nil, :full-name "Brian Gallagher"} #search_api.search_api.Author{:id 806969, :first-name nil, :last-name nil, :full-name "Tina Eliassi-Rad"}), :year 2009, :venue "ICDM", :ncit 12, :string "Evaluating Statistical Tests for Within-Network Classifiers of Relational Data.. Recently a number of modeling techniques have been developed for data mining and machine learning in relational and network domains where the instances are not independent and identically distributed (i.i.d.). These methods specifically exploit the statistical dependencies among instances in order to improve classification accuracy. However, there has been little focus on how these same dependencies affect our ability to draw accurate conclusions about the performance of the models. More specifically, the complex link structure and attribute dependencies in network data violate the assumptions of many conventional statistical tests and make it difficult to use these tests to assess the models in an unbiased manner. In this work, we examine the task of within-network classification and the question of whether two algorithms will learn models which will result in significantly different levels of performance. We show that the commonly-used form of evaluation (paired t-test on overlapping network samples) can result in an unacceptable level of Type I error. Furthermore we show that Type I error increases as (1) the correlation among instances increases and (2) the size of the evaluation set increases (i.e., the proportion of labeled nodes in the network decreases). We propose a method for network cross-validation that combined with paired t-tests produces more acceptable levels of Type I error while still providing reasonable levels of statistical power (i.e., Type II error).", :doc-id "Evaluating Statistical Tests for Within-Network Classifiers of Relational Data. 2009 Jennifer Neville, Brian Gallagher, Tina Eliassi-Rad"} #search_api.search_api.Paper{:id 937563, :key "journals/ir/Yang99", :title "An Evaluation of Statistical Approaches to Text Categorization.", :abstract "This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance&semi; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.", :author (#search_api.search_api.Author{:id 1066064, :first-name nil, :last-name nil, :full-name "Yiming Yang"}), :year 1999, :venue "Inf. Retr.", :ncit 1871, :string "An Evaluation of Statistical Approaches to Text Categorization.. This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance&semi; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.", :doc-id "An Evaluation of Statistical Approaches to Text Categorization. 1999 Yiming Yang"} #search_api.search_api.Paper{:id 473321, :key "conf/kdd/SuchanekIW06", :title "Combining linguistic and statistical analysis to extract relations from web documents.", :abstract "The World Wide Web provides a nearly endless source of knowledge, which is mostly given in natural language. A first step towards exploiting this data automatically could be to extract pairs of a given semantic relation from text documents - for example all pairs of a person and her birthdate. One strategy for this task is to find text patterns that express the semantic relation, to generalize these patterns, and to apply them to a corpus to find new pairs. In this paper, we show that this approach profits significantly when deep linguistic structures are used instead of surface text patterns. We demonstrate how linguistic structures can be represented for machine learning, and we provide a theoretical analysis of the pattern matching approach. We show the benefits of our approach by extensive experiments with our prototype system LEILA.", :author (#search_api.search_api.Author{:id 793922, :first-name nil, :last-name nil, :full-name "Fabian M. Suchanek"} #search_api.search_api.Author{:id 528047, :first-name nil, :last-name nil, :full-name "Georgiana Ifrim"} #search_api.search_api.Author{:id 1015097, :first-name nil, :last-name nil, :full-name "Gerhard Weikum"}), :year 2006, :venue "KDD", :ncit 97, :string "Combining linguistic and statistical analysis to extract relations from web documents.. The World Wide Web provides a nearly endless source of knowledge, which is mostly given in natural language. A first step towards exploiting this data automatically could be to extract pairs of a given semantic relation from text documents - for example all pairs of a person and her birthdate. One strategy for this task is to find text patterns that express the semantic relation, to generalize these patterns, and to apply them to a corpus to find new pairs. In this paper, we show that this approach profits significantly when deep linguistic structures are used instead of surface text patterns. We demonstrate how linguistic structures can be represented for machine learning, and we provide a theoretical analysis of the pattern matching approach. We show the benefits of our approach by extensive experiments with our prototype system LEILA.", :doc-id "Combining linguistic and statistical analysis to extract relations from web documents. 2006 Fabian M. Suchanek, Georgiana Ifrim, Gerhard Weikum"} #search_api.search_api.Paper{:id 33706, :key "conf/aiia/CeciBM05", :title "Relational Learning: Statistical Approach Versus Logical Approach in Document Image Understanding.", :abstract nil, :author (#search_api.search_api.Author{:id 1294148, :first-name nil, :last-name nil, :full-name "Michelangelo Ceci"} #search_api.search_api.Author{:id 1385113, :first-name nil, :last-name nil, :full-name "Margherita Berardi"} #search_api.search_api.Author{:id 282116, :first-name nil, :last-name nil, :full-name "Donato Malerba"}), :year 2005, :venue "AI*IA", :ncit 0, :string "Relational Learning: Statistical Approach Versus Logical Approach in Document Image Understanding.. ", :doc-id "Relational Learning: Statistical Approach Versus Logical Approach in Document Image Understanding. 2005 Michelangelo Ceci, Margherita Berardi, Donato Malerba"} #search_api.search_api.Paper{:id 1009299, :key "journals/ml/BeefermanBL99", :title "Statistical Models for Text Segmentation.", :abstract "This paper introduces a new statistical approach to automatically partitioning text into coherent segments. The approach is based on a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text. The models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may be domain-specific, that tend to be used near segment boundaries. Assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, Wall Street Journal news articles and television broadcast news story transcripts. Quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. This metric is used to make a quantitative assessment of the relative contributions of the different feature types, as well as a comparison with decision trees and previously proposed text segmentation algorithms.", :author (#search_api.search_api.Author{:id 1914, :first-name nil, :last-name nil, :full-name "Doug Beeferman"} #search_api.search_api.Author{:id 109818, :first-name nil, :last-name nil, :full-name "Adam L. Berger"} #search_api.search_api.Author{:id 1463112, :first-name nil, :last-name nil, :full-name "John D. Lafferty"}), :year 1999, :venue "Machine Learning", :ncit 488, :string "Statistical Models for Text Segmentation.. This paper introduces a new statistical approach to automatically partitioning text into coherent segments. The approach is based on a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text. The models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may be domain-specific, that tend to be used near segment boundaries. Assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, Wall Street Journal news articles and television broadcast news story transcripts. Quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. This metric is used to make a quantitative assessment of the relative contributions of the different feature types, as well as a comparison with decision trees and previously proposed text segmentation algorithms.", :doc-id "Statistical Models for Text Segmentation. 1999 Doug Beeferman, Adam L. Berger, John D. Lafferty"} #search_api.search_api.Paper{:id 1017236, :key "journals/mta/HanLL08", :title "Semantic image classification using statistical local spatial relations model.", :abstract "In this paper, a statistical model called statistical local spatial relations (SLSR) is presented as a novel technique of a learning model with spatial and statistical information for semantic image classification. The model is inspired by probabilistic Latent Semantic Analysis (PLSA) for text mining. In text analysis, PLSA is used to discover topics in a corpus using the bag-of-word document representation. In SLSR, we treat image categories as topics, therefore an image containing instances of multiple categories can be modeled as a mixture of topics. More significantly, SLSR introduces spatial relation information as a factor which is not present in PLSA. SLSR has rotation, scale, translation and affine invariant properties and can solve partial occlusion problems. Using the Dirichlet process and variational Expectation-Maximization learning algorithm, SLSR is developed as an implementation of an image classification algorithm. SLSR uses an unsupervised process which can capture both spatial relations and statistical information simultaneously. The experiments are demonstrated on some standard data sets and show that the SLSR model is a promising model for semantic image classification problems.", :author (#search_api.search_api.Author{:id 51919, :first-name nil, :last-name nil, :full-name "Dongfeng Han"} #search_api.search_api.Author{:id 589651, :first-name nil, :last-name nil, :full-name "Wenhui Li"} #search_api.search_api.Author{:id 1381040, :first-name nil, :last-name nil, :full-name "Zongcheng Li"}), :year 2008, :venue "Multimedia Tools Appl.", :ncit 11, :string "Semantic image classification using statistical local spatial relations model.. In this paper, a statistical model called statistical local spatial relations (SLSR) is presented as a novel technique of a learning model with spatial and statistical information for semantic image classification. The model is inspired by probabilistic Latent Semantic Analysis (PLSA) for text mining. In text analysis, PLSA is used to discover topics in a corpus using the bag-of-word document representation. In SLSR, we treat image categories as topics, therefore an image containing instances of multiple categories can be modeled as a mixture of topics. More significantly, SLSR introduces spatial relation information as a factor which is not present in PLSA. SLSR has rotation, scale, translation and affine invariant properties and can solve partial occlusion problems. Using the Dirichlet process and variational Expectation-Maximization learning algorithm, SLSR is developed as an implementation of an image classification algorithm. SLSR uses an unsupervised process which can capture both spatial relations and statistical information simultaneously. The experiments are demonstrated on some standard data sets and show that the SLSR model is a promising model for semantic image classification problems.", :doc-id "Semantic image classification using statistical local spatial relations model. 2008 Dongfeng Han, Wenhui Li, Zongcheng Li"} #search_api.search_api.Paper{:id 1028237, :key "journals/npl/LuoUN99", :title "Unsupervised Learning of Higher-Order Statistics.", :abstract "This paper deals with the adaptive extraction of higher-order statistics of related signals. We will show how to use higher-order neural networks to adaptively extract the higher-order cumulant matrices and tensors with an invariant weight norm. This proposed scheme can serve as an alternative tool in many application fields with higher-order statistics.", :author (#search_api.search_api.Author{:id 658006, :first-name nil, :last-name nil, :full-name "Fa-Long Luo"} #search_api.search_api.Author{:id 416876, :first-name nil, :last-name nil, :full-name "Rolf Unbehauen"} #search_api.search_api.Author{:id 1370072, :first-name nil, :last-name nil, :full-name "Tertulien Ndjountche"}), :year 1999, :venue "Neural Processing Letters", :ncit 0, :string "Unsupervised Learning of Higher-Order Statistics.. This paper deals with the adaptive extraction of higher-order statistics of related signals. We will show how to use higher-order neural networks to adaptively extract the higher-order cumulant matrices and tensors with an invariant weight norm. This proposed scheme can serve as an alternative tool in many application fields with higher-order statistics.", :doc-id "Unsupervised Learning of Higher-Order Statistics. 1999 Fa-Long Luo, Rolf Unbehauen, Tertulien Ndjountche"} #search_api.search_api.Paper{:id 336067, :key "conf/icml/KokD07", :title "Statistical predicate invention.", :abstract "We propose statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models and predicate invention in ILP. We propose an initial model for SPI based on second-order Markov logic, in which predicates as well as arguments can be variables, and the domain of discourse is not fully known in advance. Our approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with (e.g., it clusters relations by the clusters of the objects they relate). Since different clusterings are better for predicting different subsets of the atoms, we allow multiple cross-cutting clusterings. We show that this approach outperforms Markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets.", :author (#search_api.search_api.Author{:id 801078, :first-name nil, :last-name nil, :full-name "Stanley Kok"} #search_api.search_api.Author{:id 668085, :first-name nil, :last-name nil, :full-name "Pedro Domingos"}), :year 2007, :venue "ICML", :ncit 59, :string "Statistical predicate invention.. We propose statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models and predicate invention in ILP. We propose an initial model for SPI based on second-order Markov logic, in which predicates as well as arguments can be variables, and the domain of discourse is not fully known in advance. Our approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with (e.g., it clusters relations by the clusters of the objects they relate). Since different clusterings are better for predicting different subsets of the atoms, we allow multiple cross-cutting clusterings. We show that this approach outperforms Markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets.", :doc-id "Statistical predicate invention. 2007 Stanley Kok, Pedro Domingos"} #search_api.search_api.Paper{:id 952368, :key "journals/jair/DaumeM06", :title "Domain Adaptation for Statistical Classifiers.", :abstract "The most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution. Unfortunately, in many applications, the \"in-domain\" test data is drawn from a distribution that is related, but not identical, to the \"out-of-domain\" distribution of the training data. We consider the common case in which labeled out-of-domain data is plentiful, but labeled in-domain data is scarce. We introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts. We present efficient inference algorithms for this special case based on the technique of conditional expectation maximization. Our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain.", :author (#search_api.search_api.Author{:id 820633, :first-name nil, :last-name nil, :full-name "Hal Daumé III"} #search_api.search_api.Author{:id 911330, :first-name nil, :last-name nil, :full-name "Daniel Marcu"}), :year 2006, :venue "J. Artif. Intell. Res. (JAIR)", :ncit 250, :string "Domain Adaptation for Statistical Classifiers.. The most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution. Unfortunately, in many applications, the \"in-domain\" test data is drawn from a distribution that is related, but not identical, to the \"out-of-domain\" distribution of the training data. We consider the common case in which labeled out-of-domain data is plentiful, but labeled in-domain data is scarce. We introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts. We present efficient inference algorithms for this special case based on the technique of conditional expectation maximization. Our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain.", :doc-id "Domain Adaptation for Statistical Classifiers. 2006 Hal Daumé III, Daniel Marcu"} #search_api.search_api.Paper{:id 797516, :key "journals/coling/OchN04", :title "The Alignment Template Approach to Statistical Machine Translation.", :abstract "A phrase-based statistical machine translation approach &mdash; the alignment template approach &mdash; is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source&ndash;channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German&ndash;English speech VERBMOBIL task, we analyze the effect of various system components. On the French&ndash;English Canadian HANSARDS task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese&ndash;English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.", :author (#search_api.search_api.Author{:id 1493982, :first-name nil, :last-name nil, :full-name "Franz Josef Och"} #search_api.search_api.Author{:id 526052, :first-name nil, :last-name nil, :full-name "Hermann Ney"}), :year 2004, :venue "Computational Linguistics", :ncit 644, :string "The Alignment Template Approach to Statistical Machine Translation.. A phrase-based statistical machine translation approach &mdash; the alignment template approach &mdash; is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source&ndash;channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German&ndash;English speech VERBMOBIL task, we analyze the effect of various system components. On the French&ndash;English Canadian HANSARDS task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese&ndash;English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.", :doc-id "The Alignment Template Approach to Statistical Machine Translation. 2004 Franz Josef Och, Hermann Ney"} #search_api.search_api.Paper{:id 893160, :key "journals/ida/CucchiaraMPR01", :title "An application of machine learning and statistics to defect detection.", :abstract "We present an application of machine learning and statistics to the problem of distinguishing between defective and non-defective industrial workpieces, where the defect takes the form of a long and thin crack on the surface of the piece. From the images of pieces a number of features are extracted by using the Hough transform and the Correlated Hough transform. Two datasets are considered, one containing only features related to the Hough transform and the other containing also features related to the Correlated Hough transform. On these datasets we have compared six different learning algorithms: an attribute-value learner, C4.5, a backpropagation neural network, NeuralWorks Predict, a k-nearest neighbour algorithm, and three statistical techniques, linear, logistic and quadratic discriminant. The experiments show that C4.5 performs best for both feature sets and gives an average accuracy of 93.3% for the first dataset and 95.9% for the second dataset.", :author (#search_api.search_api.Author{:id 394534, :first-name nil, :last-name nil, :full-name "Rita Cucchiara"} #search_api.search_api.Author{:id 1065974, :first-name nil, :last-name nil, :full-name "Paola Mello"} #search_api.search_api.Author{:id 893047, :first-name nil, :last-name nil, :full-name "Massimo Piccardi"} #search_api.search_api.Author{:id 1316705, :first-name nil, :last-name nil, :full-name "Fabrizio Riguzzi"}), :year 2001, :venue "Intell. Data Anal.", :ncit 3, :string "An application of machine learning and statistics to defect detection.. We present an application of machine learning and statistics to the problem of distinguishing between defective and non-defective industrial workpieces, where the defect takes the form of a long and thin crack on the surface of the piece. From the images of pieces a number of features are extracted by using the Hough transform and the Correlated Hough transform. Two datasets are considered, one containing only features related to the Hough transform and the other containing also features related to the Correlated Hough transform. On these datasets we have compared six different learning algorithms: an attribute-value learner, C4.5, a backpropagation neural network, NeuralWorks Predict, a k-nearest neighbour algorithm, and three statistical techniques, linear, logistic and quadratic discriminant. The experiments show that C4.5 performs best for both feature sets and gives an average accuracy of 93.3% for the first dataset and 95.9% for the second dataset.", :doc-id "An application of machine learning and statistics to defect detection. 2001 Rita Cucchiara, Paola Mello, Massimo Piccardi, Fabrizio Riguzzi"} #search_api.search_api.Paper{:id 1288331, :key "journals/sadm/SundararaghavanZ09", :title "A statistical learning approach for the design of polycrystalline materials.", :abstract "Important physical properties such as yield strength, elastic modulus, and thermal conductivity depend on the material microstructure. Realization of optimal microstructures is important for hardware components in aerospace applications where there is a need to optimize material properties for improved performance. Microstructures can be tailored through controlled deformation or heat treatment. However, identification of the optimal processing path is a non-trivial (and non-unique) problem. Data-mining techniques are eminently suitable for process design since optimal processing paths can be selected based on available information from a large database-relating processes, properties, and microstructures. In this paper, the problem of designing processing stages that lead to a desired microstructure or material property is addressed by mining over a database of microstructural signatures. A hierarchical X-means classifier is designed to match crystallographic orientation features to a class of microstructural signatures within a database. Instead of the conventional distortion minimization algorithm of k-means, X-means maximizes a Bayesian information measure for calculating cluster centers which allows automatic detection of number of classes. Using the microstructural database, an adaptive data-compression technique based on proper orthogonal decomposition (POD) has been designed to accelerate materials design. In this technique, reduced modes selected adaptively from the database are used to speed up auxiliary microstructure optimization algorithms built over the database. Copyright &copy; 2009 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 1: 000-000, 2009", :author (#search_api.search_api.Author{:id 1415860, :first-name nil, :last-name nil, :full-name "Veera Sundararaghavan"} #search_api.search_api.Author{:id 585807, :first-name nil, :last-name nil, :full-name "Nicholas Zabaras"}), :year 2009, :venue "Statistical Analysis and Data Mining", :ncit 3, :string "A statistical learning approach for the design of polycrystalline materials.. Important physical properties such as yield strength, elastic modulus, and thermal conductivity depend on the material microstructure. Realization of optimal microstructures is important for hardware components in aerospace applications where there is a need to optimize material properties for improved performance. Microstructures can be tailored through controlled deformation or heat treatment. However, identification of the optimal processing path is a non-trivial (and non-unique) problem. Data-mining techniques are eminently suitable for process design since optimal processing paths can be selected based on available information from a large database-relating processes, properties, and microstructures. In this paper, the problem of designing processing stages that lead to a desired microstructure or material property is addressed by mining over a database of microstructural signatures. A hierarchical X-means classifier is designed to match crystallographic orientation features to a class of microstructural signatures within a database. Instead of the conventional distortion minimization algorithm of k-means, X-means maximizes a Bayesian information measure for calculating cluster centers which allows automatic detection of number of classes. Using the microstructural database, an adaptive data-compression technique based on proper orthogonal decomposition (POD) has been designed to accelerate materials design. In this technique, reduced modes selected adaptively from the database are used to speed up auxiliary microstructure optimization algorithms built over the database. Copyright &copy; 2009 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 1: 000-000, 2009", :doc-id "A statistical learning approach for the design of polycrystalline materials. 2009 Veera Sundararaghavan, Nicholas Zabaras"} #search_api.search_api.Paper{:id 1250938, :key "conf/ismis/BibaFE09", :title "Efficient MAP Inference for Statistical Relational Models through Hybrid Metaheuristics.", :abstract "Statistical Relational Models are state-of-the-art representation formalisms at the intersection of logical and statistical machine learning. One of the most promising models is Markov Logic (ML) which combines Markov networks (MNs) and first-order logic by attaching weights to first-order formulas and using these as templates for features of MNs. MAP inference in ML is the task of finding the most likely state of a set of output variables given the state of the input variables and this problem is NP-hard. In this paper we present an algorithm for this inference task based on the Iterated Local Search (ILS) and Robust Tabu Search (RoTS) metaheuristics. The algorithm performs a biased sampling of the set of local optima by using RoTS as a local search procedure and repetitively jumping in the search space through a perturbation operator, focusing the search not on the full space of solutions but on a smaller subspace defined by the solutions that are locally optimal for the optimization engine. We show through extensive experiments in real-world domains that it improves over the state-of-the-art algorithm in terms of solution quality and inference time.", :author (#search_api.search_api.Author{:id 1482903, :first-name nil, :last-name nil, :full-name "Marenglen Biba"} #search_api.search_api.Author{:id 1126564, :first-name nil, :last-name nil, :full-name "Stefano Ferilli"} #search_api.search_api.Author{:id 769268, :first-name nil, :last-name nil, :full-name "Floriana Esposito"}), :year 2009, :venue "ISMIS", :ncit 0, :string "Efficient MAP Inference for Statistical Relational Models through Hybrid Metaheuristics.. Statistical Relational Models are state-of-the-art representation formalisms at the intersection of logical and statistical machine learning. One of the most promising models is Markov Logic (ML) which combines Markov networks (MNs) and first-order logic by attaching weights to first-order formulas and using these as templates for features of MNs. MAP inference in ML is the task of finding the most likely state of a set of output variables given the state of the input variables and this problem is NP-hard. In this paper we present an algorithm for this inference task based on the Iterated Local Search (ILS) and Robust Tabu Search (RoTS) metaheuristics. The algorithm performs a biased sampling of the set of local optima by using RoTS as a local search procedure and repetitively jumping in the search space through a perturbation operator, focusing the search not on the full space of solutions but on a smaller subspace defined by the solutions that are locally optimal for the optimization engine. We show through extensive experiments in real-world domains that it improves over the state-of-the-art algorithm in terms of solution quality and inference time.", :doc-id "Efficient MAP Inference for Statistical Relational Models through Hybrid Metaheuristics. 2009 Marenglen Biba, Stefano Ferilli, Floriana Esposito"} #search_api.search_api.Paper{:id 3443339, :key "conf/aaai/ChechetkaDP10", :title "Relational Learning for Collective Classification of Entities in Images.", :abstract nil, :author (#search_api.search_api.Author{:id 107498, :first-name nil, :last-name nil, :full-name "Anton Chechetka"} #search_api.search_api.Author{:id 1278209, :first-name nil, :last-name nil, :full-name "Denver Dash"} #search_api.search_api.Author{:id 285308, :first-name nil, :last-name nil, :full-name "Matthai Philipose"}), :year 2010, :venue "Statistical Relational Artificial Intelligence", :ncit 5, :string "Relational Learning for Collective Classification of Entities in Images.. ", :doc-id "Relational Learning for Collective Classification of Entities in Images. 2010 Anton Chechetka, Denver Dash, Matthai Philipose"} #search_api.search_api.Paper{:id 1323591, :key "conf/icc/TiwanaSA09", :title "Statistical Learning for Automated RRM: Application to eUTRAN Mobility.", :abstract "Self Organizing Network (SON) functionalities are currently developed to improve network performance and management tasks. SON functionalities require efficient utilization of data extracted from the network. In this context, the paper has two objectives. First it is shown that one can use simple statistical learning techniques such as regression to extract a model from data. The model comprises closed form expressions that approximate the functional relations between measured Key Performance Indicators (KPIs) and Radio Resource Management (RRM) parameters. Second, it is shown how the model can be integrated in a monitoring process and be used to devise an efficient auto-tuning algorithm. To this end, two case studies of handover monitoring and handover auto-tuning in a LTE network are described and illustrate the application of the proposed approach.", :author (#search_api.search_api.Author{:id -1, :first-name nil, :last-name nil, :full-name "Moazzam Islam Tiwana"} #search_api.search_api.Author{:id 1094720, :first-name nil, :last-name nil, :full-name "Berna Sayraç"} #search_api.search_api.Author{:id 223882, :first-name nil, :last-name nil, :full-name "Zwi Altman"}), :year 2009, :venue "ICC", :ncit 6, :string "Statistical Learning for Automated RRM: Application to eUTRAN Mobility.. Self Organizing Network (SON) functionalities are currently developed to improve network performance and management tasks. SON functionalities require efficient utilization of data extracted from the network. In this context, the paper has two objectives. First it is shown that one can use simple statistical learning techniques such as regression to extract a model from data. The model comprises closed form expressions that approximate the functional relations between measured Key Performance Indicators (KPIs) and Radio Resource Management (RRM) parameters. Second, it is shown how the model can be integrated in a monitoring process and be used to devise an efficient auto-tuning algorithm. To this end, two case studies of handover monitoring and handover auto-tuning in a LTE network are described and illustrate the application of the proposed approach.", :doc-id "Statistical Learning for Automated RRM: Application to eUTRAN Mobility. 2009 Moazzam Islam Tiwana, Berna Sayraç, Zwi Altman"} #search_api.search_api.Paper{:id 1279856, :key "journals/neco/AmariM93", :title "Statistical Theory of Learning Curves under Entropic Loss Criterion.", :abstract "The present paper elucidates a universal property of learning curves, which shows how the generalization error, training error, and the complexity of the underlying stochastic machine are related and how the behavior of a stochastic machine is improved as the number of training examples increases. The error is measured by the entropic loss. It is proved that the generalization error converges to H0, the entropy of the conditional distribution of the true machine, as H0 + m*/(2t), while the training error converges as H0-m*/(2t), where t is the number of examples and m* shows the complexity of the network. When the model is faithful, implying that the true machine is in the model, m* is reduced to m, the number of modifiable parameters. This is a universal law because it holds for any regular machine irrespective of its structure under the maximum likelihood estimator. Similar relations are obtained for the Bayes and Gibbs learning algorithms. These learning curves show the relation among the accuracy of learning, the complexity of a model, and the number of training examples.", :author (#search_api.search_api.Author{:id 99365, :first-name nil, :last-name nil, :full-name "Shun-ichi Amari"} #search_api.search_api.Author{:id 918942, :first-name nil, :last-name nil, :full-name "Noboru Murata"}), :year 1993, :venue "Neural Computation", :ncit 143, :string "Statistical Theory of Learning Curves under Entropic Loss Criterion.. The present paper elucidates a universal property of learning curves, which shows how the generalization error, training error, and the complexity of the underlying stochastic machine are related and how the behavior of a stochastic machine is improved as the number of training examples increases. The error is measured by the entropic loss. It is proved that the generalization error converges to H0, the entropy of the conditional distribution of the true machine, as H0 + m*/(2t), while the training error converges as H0-m*/(2t), where t is the number of examples and m* shows the complexity of the network. When the model is faithful, implying that the true machine is in the model, m* is reduced to m, the number of modifiable parameters. This is a universal law because it holds for any regular machine irrespective of its structure under the maximum likelihood estimator. Similar relations are obtained for the Bayes and Gibbs learning algorithms. These learning curves show the relation among the accuracy of learning, the complexity of a model, and the number of training examples.", :doc-id "Statistical Theory of Learning Curves under Entropic Loss Criterion. 1993 Shun-ichi Amari, Noboru Murata"} #search_api.search_api.Paper{:id 2881512, :key "journals/jocn/Turk-BrowneSCJ09", :title "Neural Evidence of Statistical Learning: Efficient Detection of Visual Regularities Without Awareness.", :abstract "Our environment contains regularities distributed in space and time that can be detected by way of statistical learning. This unsupervised learning occurs without intent or awareness, but little is known about how it relates to other types of learning, how it affects perceptual processing, and how quickly it can occur. Here we use fMRI during statistical learning to explore these questions. Participants viewed statistically structured versus unstructured sequences of shapes while performing a task unrelated to the structure. Robust neural responses to statistical structure were observed, and these responses were notable in four ways: First, responses to structure were observed in the striatum and medial temporal lobe, suggesting that statistical learning may be related to other forms of associative learning and relational memory. Second, statistical regularities yielded greater activation in category-specific visual regions (object-selective lateral occipital cortex and word-selective ventral occipito-temporal cortex), demonstrating that these regions are sensitive to information distributed in time. Third, evidence of learning emerged early during familiarization, showing that statistical learning can operate very quickly and with little exposure. Finally, neural signatures of learning were dissociable from subsequent explicit familiarity, suggesting that learning can occur in the absence of awareness. Overall, our findings help elucidate the underlying nature of statistical learning.", :author (#search_api.search_api.Author{:id 795446, :first-name nil, :last-name nil, :full-name "Nicholas B. Turk-Browne"} #search_api.search_api.Author{:id 1336480, :first-name nil, :last-name nil, :full-name "Brian J. Scholl"} #search_api.search_api.Author{:id 1113401, :first-name nil, :last-name nil, :full-name "Marvin M. Chun"} #search_api.search_api.Author{:id 778152, :first-name nil, :last-name nil, :full-name "Marcia K. Johnson"}), :year 2009, :venue "J. Cognitive Neuroscience", :ncit 25, :string "Neural Evidence of Statistical Learning: Efficient Detection of Visual Regularities Without Awareness.. Our environment contains regularities distributed in space and time that can be detected by way of statistical learning. This unsupervised learning occurs without intent or awareness, but little is known about how it relates to other types of learning, how it affects perceptual processing, and how quickly it can occur. Here we use fMRI during statistical learning to explore these questions. Participants viewed statistically structured versus unstructured sequences of shapes while performing a task unrelated to the structure. Robust neural responses to statistical structure were observed, and these responses were notable in four ways: First, responses to structure were observed in the striatum and medial temporal lobe, suggesting that statistical learning may be related to other forms of associative learning and relational memory. Second, statistical regularities yielded greater activation in category-specific visual regions (object-selective lateral occipital cortex and word-selective ventral occipito-temporal cortex), demonstrating that these regions are sensitive to information distributed in time. Third, evidence of learning emerged early during familiarization, showing that statistical learning can operate very quickly and with little exposure. Finally, neural signatures of learning were dissociable from subsequent explicit familiarity, suggesting that learning can occur in the absence of awareness. Overall, our findings help elucidate the underlying nature of statistical learning.", :doc-id "Neural Evidence of Statistical Learning: Efficient Detection of Visual Regularities Without Awareness. 2009 Nicholas B. Turk-Browne, Brian J. Scholl, Marvin M. Chun, Marcia K. Johnson"} #search_api.search_api.Paper{:id 138052, :key "conf/dagstuhl/PasseriniFR05", :title "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.", :abstract "We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :author (#search_api.search_api.Author{:id 1520074, :first-name nil, :last-name nil, :full-name "Andrea Passerini"} #search_api.search_api.Author{:id 34394, :first-name nil, :last-name nil, :full-name "Paolo Frasconi"} #search_api.search_api.Author{:id 487936, :first-name nil, :last-name nil, :full-name "Luc De Raedt"}), :year 2005, :venue "Probabilistic, Logical and Relational Learning", :ncit 31, :string "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting.. We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.", :doc-id "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting. 2005 Andrea Passerini, Paolo Frasconi, Luc De Raedt"} #search_api.search_api.Paper{:id 1026778, :key "journals/nn/Linsker05", :title "Improved local learning rule for information maximization and related applications.", :abstract "For a neural network comprising feedforward and lateral connections, a local learning rule is proposed that causes the lateral connections to learn directly the inverse of a covariance matrix. In contrast to earlier work, the rule involves just one processing pass through the lateral connections for each input presentation, and consists of a simple anti-Hebbian term. This provides an effective and simple method for online network learning algorithms that implement optimization principles, drawn from statistics or from information or control theory, for which a running estimate of the covariance matrix inverse is useful. An application to infomax learning (mutual information maximization) in the presence of input and output noise is used to illustrate the method.", :author (#search_api.search_api.Author{:id 321062, :first-name nil, :last-name nil, :full-name "Ralph Linsker"}), :year 2005, :venue "Neural Networks", :ncit 19, :string "Improved local learning rule for information maximization and related applications.. For a neural network comprising feedforward and lateral connections, a local learning rule is proposed that causes the lateral connections to learn directly the inverse of a covariance matrix. In contrast to earlier work, the rule involves just one processing pass through the lateral connections for each input presentation, and consists of a simple anti-Hebbian term. This provides an effective and simple method for online network learning algorithms that implement optimization principles, drawn from statistics or from information or control theory, for which a running estimate of the covariance matrix inverse is useful. An application to infomax learning (mutual information maximization) in the presence of input and output noise is used to illustrate the method.", :doc-id "Improved local learning rule for information maximization and related applications. 2005 Ralph Linsker"} #search_api.search_api.Paper{:id 1009218, :key "journals/ml/Boulle04", :title "Khiops: A Statistical Discretization Method of Continuous Attributes.", :abstract "In supervised machine learning, some algorithms are restricted to discrete data and have to discretize continuous attributes. Many discretization methods, based on statistical criteria, information content, or other specialized criteria, have been studied in the past. In this paper, we propose the discretization method Khiops,1 based on the chi-square statistic. In contrast with related methods ChiMerge and ChiSplit, this method optimizes the chi-square criterion in a global manner on the whole discretization domain and does not require any stopping criterion. A theoretical study followed by experiments demonstrates the robustness and the good predictive performance of the method.", :author (#search_api.search_api.Author{:id 1526635, :first-name nil, :last-name nil, :full-name "Marc Boullé"}), :year 2004, :venue "Machine Learning", :ncit 0, :string "Khiops: A Statistical Discretization Method of Continuous Attributes.. In supervised machine learning, some algorithms are restricted to discrete data and have to discretize continuous attributes. Many discretization methods, based on statistical criteria, information content, or other specialized criteria, have been studied in the past. In this paper, we propose the discretization method Khiops,1 based on the chi-square statistic. In contrast with related methods ChiMerge and ChiSplit, this method optimizes the chi-square criterion in a global manner on the whole discretization domain and does not require any stopping criterion. A theoretical study followed by experiments demonstrates the robustness and the good predictive performance of the method.", :doc-id "Khiops: A Statistical Discretization Method of Continuous Attributes. 2004 Marc Boullé"} #search_api.search_api.Paper{:id 498833, :key "conf/miccai/UnalNSF08", :title "Customized Design of Hearing Aids Using Statistical Shape Learning.", :abstract "3D shape modeling is a crucial component of rapid prototyping systems that customize shapes of implants and prosthetic devices to a patient's anatomy. In this paper, we present a solution to the problem of customized 3D shape modeling using a statistical shape analysis framework. We design a novel method to learn the relationship between two classes of shapes, which are related by certain operations or transformation. The two associated shape classes are represented in a lower dimensional manifold, and the reduced set of parameters obtained in this subspace is utilized in an estimation, which is exemplified by a multivariate regression in this paper. We demonstrate our method with a felicitous application to estimation of customized hearing aid devices.", :author (#search_api.search_api.Author{:id 987788, :first-name nil, :last-name nil, :full-name "Gozde B. Unal"} #search_api.search_api.Author{:id 1388067, :first-name nil, :last-name nil, :full-name "Delphine Nain"} #search_api.search_api.Author{:id 174234, :first-name nil, :last-name nil, :full-name "Gregory G. Slabaugh"} #search_api.search_api.Author{:id 303194, :first-name nil, :last-name nil, :full-name "Tong Fang"}), :year 2008, :venue "MICCAI (1)", :ncit 6, :string "Customized Design of Hearing Aids Using Statistical Shape Learning.. 3D shape modeling is a crucial component of rapid prototyping systems that customize shapes of implants and prosthetic devices to a patient's anatomy. In this paper, we present a solution to the problem of customized 3D shape modeling using a statistical shape analysis framework. We design a novel method to learn the relationship between two classes of shapes, which are related by certain operations or transformation. The two associated shape classes are represented in a lower dimensional manifold, and the reduced set of parameters obtained in this subspace is utilized in an estimation, which is exemplified by a multivariate regression in this paper. We demonstrate our method with a felicitous application to estimation of customized hearing aid devices.", :doc-id "Customized Design of Hearing Aids Using Statistical Shape Learning. 2008 Gozde B. Unal, Delphine Nain, Gregory G. Slabaugh, Tong Fang"} #search_api.search_api.Paper{:id 473095, :key "conf/kdd/Moore06", :title "New cached-sufficient statistics algorithms for quickly answering statistical questions.", :abstract "This talk is about recent work on new ways to exploit preprocessed views of data tables for tractably solving big statistical queries. We'll describe deployments of these new algorithms in the realms of detecting killer asteroids and unnatural disease outbreaks.In recent years, several groups have looked at methods for pre-storing general sufficient statistics of the data in spatial data structures such as kd-trees and ball-trees so that both frequentist and Bayesian statistical operations become fast for large datasets. In this talk we will look at two other classes of optimization required in important statistical queries.The first involves iterating over all spatial regions (big and small). The second involves detection of tracks from noisy intermittent observations separated far apart in time and space. We will also discuss the implications that have arisen from making these operations tractable. We will focus particularly onDetecting all asteroids in the solar system larger than Pittsburgh's Cathedral of Learning (data to be collected over 2006-2010). Early detection of emerging diseases based on national monitoring of health-related transactions..", :author (#search_api.search_api.Author{:id 685289, :first-name nil, :last-name nil, :full-name "Andrew Moore"}), :year 2006, :venue "KDD", :ncit 0, :string "New cached-sufficient statistics algorithms for quickly answering statistical questions.. This talk is about recent work on new ways to exploit preprocessed views of data tables for tractably solving big statistical queries. We'll describe deployments of these new algorithms in the realms of detecting killer asteroids and unnatural disease outbreaks.In recent years, several groups have looked at methods for pre-storing general sufficient statistics of the data in spatial data structures such as kd-trees and ball-trees so that both frequentist and Bayesian statistical operations become fast for large datasets. In this talk we will look at two other classes of optimization required in important statistical queries.The first involves iterating over all spatial regions (big and small). The second involves detection of tracks from noisy intermittent observations separated far apart in time and space. We will also discuss the implications that have arisen from making these operations tractable. We will focus particularly onDetecting all asteroids in the solar system larger than Pittsburgh's Cathedral of Learning (data to be collected over 2006-2010). Early detection of emerging diseases based on national monitoring of health-related transactions..", :doc-id "New cached-sufficient statistics algorithms for quickly answering statistical questions. 2006 Andrew Moore"} #search_api.search_api.Paper{:id 1010126, :key "journals/ml/DietterichDGMT08", :title "Structured machine learning: the next ten years.", :abstract "The field of inductive logic programming (ILP) has made steady progress, since the first ILP workshop in 1991, based on a balance of developments in theory, implementations and applications. More recently there has been an increased emphasis on Probabilistic ILP and the related fields of Statistical Relational Learning (SRL) and Structured Prediction. The goal of the current paper is to consider these emerging trends and chart out the strategic directions and open problems for the broader area of structured machine learning for the next 10 years.", :author (#search_api.search_api.Author{:id 737377, :first-name nil, :last-name nil, :full-name "Thomas G. Dietterich"} #search_api.search_api.Author{:id 668085, :first-name nil, :last-name nil, :full-name "Pedro Domingos"} #search_api.search_api.Author{:id 1449720, :first-name nil, :last-name nil, :full-name "Lise Getoor"} #search_api.search_api.Author{:id 120131, :first-name nil, :last-name nil, :full-name "Stephen Muggleton"} #search_api.search_api.Author{:id 435675, :first-name nil, :last-name nil, :full-name "Prasad Tadepalli"}), :year 2008, :venue "Machine Learning", :ncit 42, :string "Structured machine learning: the next ten years.. The field of inductive logic programming (ILP) has made steady progress, since the first ILP workshop in 1991, based on a balance of developments in theory, implementations and applications. More recently there has been an increased emphasis on Probabilistic ILP and the related fields of Statistical Relational Learning (SRL) and Structured Prediction. The goal of the current paper is to consider these emerging trends and chart out the strategic directions and open problems for the broader area of structured machine learning for the next 10 years.", :doc-id "Structured machine learning: the next ten years. 2008 Thomas G. Dietterich, Pedro Domingos, Lise Getoor, Stephen Muggleton, Prasad Tadepalli"} #search_api.search_api.Paper{:id 1280035, :key "journals/nn/Watanabe10", :title "Equations of states in singular statistical estimation.", :abstract "Learning machines that have hierarchical structures or hidden variables are singular statistical models because they are nonidentifiable and their Fisher information matrices are singular. In singular statistical models, neither does the Bayes a posteriori distribution converge to the normal distribution nor does the maximum likelihood estimator satisfy asymptotic normality. This is the main reason that it has been difficult to predict their generalization performance from trained states. In this paper, we study four errors, (1) the Bayes generalization error, (2) the Bayes training error, (3) the Gibbs generalization error, and (4) the Gibbs training error, and prove that there are universal mathematical relations among these errors. The formulas proved in this paper are equations of states in statistical estimation because they hold for any true distribution, any parametric model, and any a priori distribution. Also we show that the Bayes and Gibbs generalization errors can be estimated by Bayes and Gibbs training errors, and we propose widely applicable information criteria that can be applied to both regular and singular statistical models.", :author (#search_api.search_api.Author{:id 976510, :first-name nil, :last-name nil, :full-name "Sumio Watanabe"}), :year 2010, :venue "Neural Networks", :ncit 13, :string "Equations of states in singular statistical estimation.. Learning machines that have hierarchical structures or hidden variables are singular statistical models because they are nonidentifiable and their Fisher information matrices are singular. In singular statistical models, neither does the Bayes a posteriori distribution converge to the normal distribution nor does the maximum likelihood estimator satisfy asymptotic normality. This is the main reason that it has been difficult to predict their generalization performance from trained states. In this paper, we study four errors, (1) the Bayes generalization error, (2) the Bayes training error, (3) the Gibbs generalization error, and (4) the Gibbs training error, and prove that there are universal mathematical relations among these errors. The formulas proved in this paper are equations of states in statistical estimation because they hold for any true distribution, any parametric model, and any a priori distribution. Also we show that the Bayes and Gibbs generalization errors can be estimated by Bayes and Gibbs training errors, and we propose widely applicable information criteria that can be applied to both regular and singular statistical models.", :doc-id "Equations of states in singular statistical estimation. 2010 Sumio Watanabe"} #search_api.search_api.Paper{:id 1185901, :key "journals/jocn/MuellerBF08", :title "The Role of Pause Cues in Language Learning: The Emergence of Event-related Potentials Related to Sequence Processing.", :abstract "Humans can derive sequential dependencies from unfamiliar artificial speech within several of minutes of exposure. However, there is an ongoing debate about the nature of the underlying learning processes. In a widely discussed study Pea et al. [Pea, M., Bonatti, L. L., Nespor, M., & Mehler, J. Signal-driven computations in speech processing. Science, 298, 604607, 2002] argued for the importance of subtle acoustic cues in the signal, such as pauses, in order to switch between two computational mechanisms, which are conceptualized as rule-based versus statistical. The present study was aimed to approach this problem by recording event-related potentials in response to correct and incorrect phrases consisting of bisyllabics after short exposure to either rule-based or random artificial speech streams. Rule-based streams contained dependencies of the form AXC, whereby A elements reliably predicted the C elements and X elements were variable. Participants were exposed to four input and test phases. Two of the input streams were rule-based and contained either only probabilistic information related to the distribution of the AXC stimuli or an additional acoustic cue indicating the boundaries of relevant units. The other two streams were random variations of the rule-based streams. During the test phase in the condition with pause cues, an early negativity and a later positivity emerged for correct and incorrect items in comparison to their acoustically identical counterparts, which were presented after the random control condition. In the noncued condition, only negativities were seen. The timing and the scalp distribution of the negativities were different for correct and incorrect sequences in both the cued and the noncued conditions. The results are interpreted in support of a view of grammatical learning in which both distributional and acoustic cues may contribute to different aspects of syntactic learning.", :author (#search_api.search_api.Author{:id 983626, :first-name nil, :last-name nil, :full-name "Jutta L. Mueller"} #search_api.search_api.Author{:id -1, :first-name nil, :last-name nil, :full-name "Jörg Bahlmann"} #search_api.search_api.Author{:id 258614, :first-name nil, :last-name nil, :full-name "Angela D. Friederici"}), :year 2008, :venue "J. Cognitive Neuroscience", :ncit 16, :string "The Role of Pause Cues in Language Learning: The Emergence of Event-related Potentials Related to Sequence Processing.. Humans can derive sequential dependencies from unfamiliar artificial speech within several of minutes of exposure. However, there is an ongoing debate about the nature of the underlying learning processes. In a widely discussed study Pea et al. [Pea, M., Bonatti, L. L., Nespor, M., & Mehler, J. Signal-driven computations in speech processing. Science, 298, 604607, 2002] argued for the importance of subtle acoustic cues in the signal, such as pauses, in order to switch between two computational mechanisms, which are conceptualized as rule-based versus statistical. The present study was aimed to approach this problem by recording event-related potentials in response to correct and incorrect phrases consisting of bisyllabics after short exposure to either rule-based or random artificial speech streams. Rule-based streams contained dependencies of the form AXC, whereby A elements reliably predicted the C elements and X elements were variable. Participants were exposed to four input and test phases. Two of the input streams were rule-based and contained either only probabilistic information related to the distribution of the AXC stimuli or an additional acoustic cue indicating the boundaries of relevant units. The other two streams were random variations of the rule-based streams. During the test phase in the condition with pause cues, an early negativity and a later positivity emerged for correct and incorrect items in comparison to their acoustically identical counterparts, which were presented after the random control condition. In the noncued condition, only negativities were seen. The timing and the scalp distribution of the negativities were different for correct and incorrect sequences in both the cued and the noncued conditions. The results are interpreted in support of a view of grammatical learning in which both distributional and acoustic cues may contribute to different aspects of syntactic learning.", :doc-id "The Role of Pause Cues in Language Learning: The Emergence of Event-related Potentials Related to Sequence Processing. 2008 Jutta L. Mueller, Jörg Bahlmann, Angela D. Friederici"} #search_api.search_api.Paper{:id 952340, :key "journals/jair/Jaeger05", :title "Ignorability in Statistical and Probabilistic Inference.", :abstract "When dealing with incomplete data in statistical learning, or incomplete observations in probabilistic inference, one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened. Since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive, one asks for conditions under which it can be safely ignored. Such conditions are given by the missing at random (mar) and coarsened at random (car) assumptions. In this paper we provide an in-depth analysis of several questions relating to mar/car assumptions. Main purpose of our study is to provide criteria by which one may evaluate whether a car assumption is reasonable for a particular data collecting or observational process. This question is complicated by the fact that several distinct versions of mar/car assumptions exist. We therefore first provide an overview over these different versions, in which we highlight the distinction between distributional and coarsening variable induced versions. We show that distributional versions are less restrictive and sufficient for most applications. We then address from two different perspectives the question of when the mar/car assumption is warranted. First we provide a \"static\" analysis that characterizes the admissibility of the car assumption in terms of the support structure of the joint probability distribution of complete data and incomplete observations. Here we obtain an equivalence characterization that improves and extends a recent result by Grünwald and Halpern. We then turn to a \"procedural\" analysis that characterizes the admissibility of the car assumption in terms of procedural models for the actual data (or observation) generating process. The main result of this analysis is that the stronger coarsened completely at random (ccar) condition is arguably the most reasonable assumption, as it alone corresponds to data coarsening procedures that satisfy a natural robustness property.", :author (#search_api.search_api.Author{:id 252997, :first-name nil, :last-name nil, :full-name "Manfred Jaeger"}), :year 2005, :venue "J. Artif. Intell. Res. (JAIR)", :ncit 7, :string "Ignorability in Statistical and Probabilistic Inference.. When dealing with incomplete data in statistical learning, or incomplete observations in probabilistic inference, one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened. Since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive, one asks for conditions under which it can be safely ignored. Such conditions are given by the missing at random (mar) and coarsened at random (car) assumptions. In this paper we provide an in-depth analysis of several questions relating to mar/car assumptions. Main purpose of our study is to provide criteria by which one may evaluate whether a car assumption is reasonable for a particular data collecting or observational process. This question is complicated by the fact that several distinct versions of mar/car assumptions exist. We therefore first provide an overview over these different versions, in which we highlight the distinction between distributional and coarsening variable induced versions. We show that distributional versions are less restrictive and sufficient for most applications. We then address from two different perspectives the question of when the mar/car assumption is warranted. First we provide a \"static\" analysis that characterizes the admissibility of the car assumption in terms of the support structure of the joint probability distribution of complete data and incomplete observations. Here we obtain an equivalence characterization that improves and extends a recent result by Grünwald and Halpern. We then turn to a \"procedural\" analysis that characterizes the admissibility of the car assumption in terms of procedural models for the actual data (or observation) generating process. The main result of this analysis is that the stronger coarsened completely at random (ccar) condition is arguably the most reasonable assumption, as it alone corresponds to data coarsening procedures that satisfy a natural robustness property.", :doc-id "Ignorability in Statistical and Probabilistic Inference. 2005 Manfred Jaeger"} #search_api.search_api.Paper{:id 1009230, :key "journals/ml/BrazdilSC03", :title "Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results.", :abstract "We present a meta-learning method to support selection of candidate learning algorithms. It uses a k-Nearest Neighbor algorithm to identify the datasets that are most similar to the one at hand. The distance between datasets is assessed using a relatively small set of data characteristics, which was selected to represent properties that affect algorithm performance. The performance of the candidate algorithms on those datasets is used to generate a recommendation to the user in the form of a ranking. The performance is assessed using a multicriteria evaluation measure that takes not only accuracy, but also time into account. As it is not common in Machine Learning to work with rankings, we had to identify and adapt existing statistical techniques to devise an appropriate evaluation methodology. Using that methodology, we show that the meta-learning method presented leads to significantly better rankings than the baseline ranking method. The evaluation methodology is general and can be adapted to other ranking problems. Although here we have concentrated on ranking classification algorithms, the meta-learning framework presented can provide assistance in the selection of combinations of methods or more complex problem solving strategies.", :author (#search_api.search_api.Author{:id 1157783, :first-name nil, :last-name nil, :full-name "Pavel Brazdil"} #search_api.search_api.Author{:id 791367, :first-name nil, :last-name nil, :full-name "Carlos Soares"} #search_api.search_api.Author{:id 212848, :first-name nil, :last-name nil, :full-name "Joaquim Pinto da Costa"}), :year 2003, :venue "Machine Learning", :ncit 200, :string "Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results.. We present a meta-learning method to support selection of candidate learning algorithms. It uses a k-Nearest Neighbor algorithm to identify the datasets that are most similar to the one at hand. The distance between datasets is assessed using a relatively small set of data characteristics, which was selected to represent properties that affect algorithm performance. The performance of the candidate algorithms on those datasets is used to generate a recommendation to the user in the form of a ranking. The performance is assessed using a multicriteria evaluation measure that takes not only accuracy, but also time into account. As it is not common in Machine Learning to work with rankings, we had to identify and adapt existing statistical techniques to devise an appropriate evaluation methodology. Using that methodology, we show that the meta-learning method presented leads to significantly better rankings than the baseline ranking method. The evaluation methodology is general and can be adapted to other ranking problems. Although here we have concentrated on ranking classification algorithms, the meta-learning framework presented can provide assistance in the selection of combinations of methods or more complex problem solving strategies.", :doc-id "Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results. 2003 Pavel Brazdil, Carlos Soares, Joaquim Pinto da Costa"} #search_api.search_api.Paper{:id 271964, :key "conf/icann/KopeczM97", :title "Relative Time Scales in the Self-Organization of Pattern Classification: From One-Shot to Statistical Learning.", :abstract nil, :author (#search_api.search_api.Author{:id 840288, :first-name nil, :last-name nil, :full-name "Klaus Kopecz"} #search_api.search_api.Author{:id 379366, :first-name nil, :last-name nil, :full-name "Karim Mohraz"}), :year 1997, :venue "ICANN", :ncit 0, :string "Relative Time Scales in the Self-Organization of Pattern Classification: From One-Shot to Statistical Learning.. ", :doc-id "Relative Time Scales in the Self-Organization of Pattern Classification: From One-Shot to Statistical Learning. 1997 Klaus Kopecz, Karim Mohraz"} #search_api.search_api.Paper{:id 20109, :key "conf/acl/Chiang05", :title "A Hierarchical Phrase-Based Model for Statistical Machine Translation.", :abstract "We present a statistical phrase-based translation model that uses hierarchical phrases---phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.", :author (#search_api.search_api.Author{:id 415922, :first-name nil, :last-name nil, :full-name "David Chiang"}), :year 2005, :venue "ACL", :ncit 739, :string "A Hierarchical Phrase-Based Model for Statistical Machine Translation.. We present a statistical phrase-based translation model that uses hierarchical phrases---phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.", :doc-id "A Hierarchical Phrase-Based Model for Statistical Machine Translation. 2005 David Chiang"})}, :query "statistical relational learning"}